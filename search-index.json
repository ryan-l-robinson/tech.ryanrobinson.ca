<!-- Enforce UTF-8 encoding -->
<!DOCTYPE html>
<html lang="en-CA">
	<head>
	  <meta charset="UTF-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
		<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#2E2C2C">
		<meta name="geo.region" content="CA-ON">
		<meta name="author" content="Ryan Robinson">
		<meta name="description" content="Technologist writing about web development, Microsoft 365, and more.">
		<meta name="fediverse:creator" content="@Ryan@mstdn.ca" />
		<meta http-equiv="X-Content-Type-Options" content="nosniff" />

		<meta property="og:title" content="Ryan Robinson Technology">
		<meta property="og:description" content="Technologist writing about web development, Microsoft 365, and more.">
		<meta property="og:url" content="https://tech.ryanrobinson.ca//search-index.json">
		<meta property="og:image" content="https://tech.ryanrobinson.ca//img/Ryan.png">
		<meta property="og:type" content="website">

		<meta name="twitter:card" content="summary_large_image">
		<meta name="twitter:title" content="Ryan Robinson Technology">
		<meta name="twitter:description" content="Technologist writing about web development, Microsoft 365, and more.">
		<meta name="twitter:image" content="https://tech.ryanrobinson.ca//img/Ryan.png">

		<title> | Ryan Robinson Technology</title>

		<link rel="canonical" href="https://tech.ryanrobinson.ca//search-index.json">
  	<link rel="icon" href="/img/favicon.ico" type="image/x-icon">
		<link rel="alternate" href="/feed/feed.xml" type="application/atom+xml" title="Ryan Robinson Technology">
		<link rel="stylesheet" href="/dist/ZG8TgAHLWn.css">
		<link rel="preconnect" href="https://fonts.googleapis.com">
		<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
		<link href="https://fonts.googleapis.com/css2?family=Atkinson+Hyperlegible:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel="stylesheet">
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.1/css/all.min.css?v=1">
	</head>

	<body>
		<a href="#main" class="visually-hidden">Skip to main content</a>
		<div class="wrapper">
			<header>
				<nav class="container header-nav" aria-labelledby="landing-page-header">
					<h2 class="visually-hidden" id="landing-page-header">External Navigation Links</h2>
					<a href="https://ryanrobinson.ca" class="icon-before home-page">Ryan's Home Page</a>
				</nav>
				<div class="container title-search">
					<p class="site-title"><a href="/" class="icon-before tech-blog">Ryan Robinson Technology</a></p>
					<div class="search">
						<form id="search-form" role="search">
							<label for="search-input" class="visually-hidden">Search posts</label>
							<input type="search" id="search-input" placeholder="Search posts..." aria-label="Search posts">
							<button type="submit" class="visually-hidden">Search</button>
						</form>
						<div id="search-results" class="search-results" aria-live="polite"></div>
					</div>
				</div>
			</header>
			<main id="main">
				<div class="main-content">
					<h1></h1>
					[{
      "url": &quot;/2021/viva-topics-configuration/&quot;,
      "title": &quot;Microsoft Viva Topics: Configuration&quot;,
      "content": &quot;Microsoft recently launched their new Viva platform. Nothing about the announcement was radically new – everything had at least been previewed – but the new Viva branding helps tie them together with the shared goal of improving the employee experience. The most interesting component for me is one piece that did launch that day: Topics. In this post and the next I’ll break down my first impressions from some simple tests. Configuring The announcement blogs about Viva Topics did not make it clear how to get started. It also isn’t obvious; unlike some of the larger workloads in Microsoft 365, there is no new Viva Admin Centre. Fortunately it is clear in documentation elsewhere where to get started in the main Admin Centre. Once you find the process, it’s straightforward with some useful options, such as: SharePoint sites: you can specify to look for topics on all sites, all sites except those specified, only those specified, or not at all. In most cases, all sites will make the most sense. Don’t worry about permissions as it already utilizes the Microsoft Graph to make sure that nobody sees content they shouldn’t. Exclude topics: if you know some topics that you don’t want in advance, you can upload a csv file with those. You’ll always have manual control over topics, though, before they get published. If you just let it find everything it can and then remove them from there, that’s also fine. Who can see topics: you can allow the topics functionality for all users, only selected users, or nobody. This feels like a redundant setting to me as there is also an extra license needed to see topics. Who can manage topics: you can allow editing topics as well as adding new topics to different permission groups. You can leave this open for everybody, but in this case most larger organizations will want to restrict it to a handful of dedicated admins determined by a security group. Topic centre: name and pick the URL for your new topic centre, a special SharePoint site. You may want this to be generic like “Topic Centre” or you may want something more specific if you are going to use it for a more precise purpose like a client listing. When you’ve picked all the desired settings, confirm that you’re ready and the features will be enabled. Note that it took an hour or two after completing this setup before the topic centre was ready for my use. That makes sense, but it doesn’t warn about that delay; if you are confused why it doesn’t seem to be working right away, you likely just need to wait. The Topic Centre The Topic Centre which is created after the setup process above looks like a typical SharePoint site, but with some specific page types. The homepage shows topics related to you. The Manage Topics page is the core feature for administering topics on an ongoing basis. This page shows all of your organization’s topics, broken down by the stage of approval: Suggested: topics found by the automatic search for you to review. The question to move from this stage is whether you do want that to appear as a topic across your organization. If not, you can remove it. If yes, you can confirm it. Confirmed: topics which have been confirmed as a real topic of interest for your organization. For items here, take the time to view the page and make some manual edits to the topic before publishing. Published: topics that are active within your organization and can be seen by typical users with the appropriate license and settings. Removed: topics that have been removed from your organization. This is the hub for your Topics admins to keep things organized. I found it clean and intuitive to use. I imagine in large organizations that there will need to be somebody regularly checking in on the generated topics and keeping things organized, but the tool to do that is straightforward enough that the technology won’t get in the way. Overall, it’s off to a great start. In the next post, I’ll talk about what this looks like for typical users seeing the topics within their workflows.&quot;
    },{
      "url": &quot;/2021/ms-101-prep-how-im-studying/&quot;,
      "title": &quot;MS-101 Prep: How I&#39;m Studying&quot;,
      "content": &quot;Over the last few months, I’ve been working on preparing for the MS-101 exam. This exam covers a few topics around enterprise device management and security. I’ve mostly been studying with a few methods: The official practice test from MeasureUp Reading the learning paths from Microsoft Reading a prep booklet from Nate Chamberlain Trying as much as I can in my personal tenant with license trials, or a dev tenant, but some things can be hard to test out without significant amounts of real data and users I’m adding another piece: writing about it. One of the best ways to learn something is to try to explain to somebody else in your own words. So with that, I’m starting a new series on what I’m learning as I study for the MS-101.&quot;
    },{
      "url": &quot;/2021/viva-topics-experience/&quot;,
      "title": &quot;Microsoft Viva Topics: User Experience&quot;,
      "content": &quot;In my previous post, I walked through my thoughts on the configuration process for the new Microsoft Viva Topics functionality within Microsoft 365. In this post I’ll dive into what the experience is like for users once it is configured. Creating / Editing a Topic You can edit a topic page similar to editing any other SharePoint page, but there is a lot less flexibility. It has a predefined set of features and you cannot add other web parts. Title Alternate names: this allows for the topic being surfaced for alternate names, not just the core title. A suggestion here is to add any acronyms for the topic that you’re likely to use in normal conversation. Short description: a bit of text to summarize the topic. Pinned people: identify users in the organization who are relevant to the topic, with a title for how they are related, such as “topic expert.” Pinned files and pages: if there are files and pages on other SharePoint sites that are particularly important to this topic, you can pin them here for easy access. Related sites: similar idea, but for SharePoint sites. Related topics: links to other topics, visualized in a way to help you see how all your topics connect to each other. Surfacing Topics Currently the surfacing of topics for users is limited to only SharePoint pages. When you hover over the linked topic title, you see a hover card very similar to what you see when a person is referenced. This should expand over time to also show up in other Microsoft 365 resources, such as in emails and Teams conversations. It’s moderately useful now, but once it also starts showing up in this more organic contexts for conversation, that will be a huge additional value. Use Cases A few scenarios come to mind where this topic functionality could be great. If you are a client-oriented business, you could use a topic as a landing page for each client. Alternate names could be acronyms or shortened versions of the client’s name. Pinned people could be an account manager or project manager who often works with the client. Related files may be the most important files for the client, such as a contract, and related sites may be other SharePoint sites in your system that contain data like a project’s files. A product or service oriented business could be similar, with a topic card for your major products. Pinned people would be the experts on that product. Internal documentation would also work as topics. The detailed steps would need to be files that are held elsewhere, but they can be linked from the topic card. For example, if you offer Drupal website consulting, you may have detailed documentation on the SharePoint site for the team responsible for Drupal website consulting. Then you create a Drupal topic card, which has links out to the detailed documentation. In all of these scenarios, there is clear value to being in the middle of a SharePoint page (or later, a conversation) where a topic is referenced and you are able to quickly get to more information about that topic through a pop-up card. Conclusion Note that my tests are missing one of the most compelling aspects of Viva Topics: the automatic creation and updating of topics pages. My tenant consisting of just me doesn’t have nearly the scale to easily manufacture that test. All of my tests relied on manually creating topic pages. With that gap in testing acknowledged, even if the automatic component isn’t as strong as promised, I really like Viva Topics. The idea of being able to surface important topics in other places through Microsoft 365 promises a pleasant user experience that saves a lot of time in accessing important information.&quot;
    },{
      "url": &quot;/2021/microsoft-ignite-news-day/&quot;,
      "title": &quot;Microsoft Ignite News Day&quot;,
      "content": &quot;Today was the first day of the spring edition of Microsoft Ignite, the primary conference for Microsoft IT professionals. The first day always includes a lot of news for upcoming features which tend to get dumped all at once the same time as the opening keynote. After reviewing several of these news blogs and watching video sessions most of the day, here are the features which stand out to me. I will not cover nearly everything, but provided some links for more details. Microsoft Mesh This was the key product in the opening keynote. Many of us have now spent most of a year straight only communicating with our coworkers through screens. Together Mode in Teams certainly helped as it showed everybody together – it’s less cognitive load and does make it feel just a little bit like you’re together – but it’s still staring at a flat screen. But what if instead we all entered a shared virtual reality or augmented reality space, where we could still access our shared resources like files from our Microsoft 365? That would be a significant improvement to the meeting from home experience. Microsoft Mesh promises that kind of future. Mesh is a platform, not a specific product at this point. It allows developers to create applications where we can share holograms and appear as avatars speaking to each other. They didn’t promise any particular implementation of this technology, but along with the meeting idea they did also have some fun like playing Pokemon Go in the park with your HoloLens headset on. Microsoft Teams Teams had several feature announcements, as it often does. Within meetings: Dynamic view: this has been shown before but the flexibility will make larger meetings more pleasant and easier to focus on what you need. Encryption: end-to-end encryption on 1:1 video calls. That means your company or Microsoft couldn’t see your calls even if they tried. Presenter mode: when presenting a PowerPoint, you can choose whether to keep your own overlaid on top of the presentation. That will make it a lot easier for viewers to still see you, helping with things like joke delivery which I have found can be quite awkward in one-directional virtual presentations. There are also new webinar capabilities to allow for public registration to an event, email reminders going out to those attendees, and moderation tools. Teams Connect allows for sharing Teams channels with other organizations. I’ll need to see exactly how this works, but it was high on the list of improvements many wanted to improve interaction with clients and partner organizations. Multi-geo support will allow for different Teams data to be housed in different regions, helping meet data residency requirements for large organizations with offices in different countries. Endpoint transfer on calls will allow you to transfer a call seamlessly from one device to another. Get a call at your desk but need to finish on your phone as you leave the office? No problem, and the others on the call won’t notice. Planner Planner now supports up to 25 labels, a big jump from the previous 5. This is helpful to better organize your tasks. Power Platform There are two big announcements for Power Platform that stood out to me. Power Automate Desktop is now free to any Windows 10 users. Previously this required a separate license. This allows you to automate all kinds of things you do on your computer, not just cloud services. If you find yourself doing some repetitive tasks, check this out as a possible way to save yourself that time. Power Fxwas first hinted at a week or two ago, but it is the simple programming language inspired by Excel formulas and now being deployed throughout the Power Platform. That’s going to add a lot of programming potential with syntax that a lot of people are already familiar with from Excel. SharePoint Syntex SharePoint Syntex is a relatively new tool for automating classification and data extraction from files. A few new features include: A find function: when you’re training your identification model, this will make your life a lot easier to jump to the relevant data you&#39;re trying to tell the model to pay attention to, rather than having to scroll through long documents. It’s a little addition that can add up to a lot of time saved. Retention labels: automatically applied when the content type is identified SharePoint hubs: content types can now be assigned to SharePoint hubs. This will allow for changes to content types to roll out much faster to sites within the hub. The delay to roll out was always one of the more annoying factors when using content types, so this should help significantly. Project Origin This one is outside of the Microsoft 365 space but was still a good sign to me. Microsoft is partnering with some other companies to combat online misinformation through tools like identifying deepfakes. Misinformation on social media is one of the largest problems facing our world at the moment, so it’s promising to see that Microsoft is trying to help.&quot;
    },{
      "url": &quot;/2021/security-essentials-password-manager/&quot;,
      "title": &quot;Security Essentials: Password Manager&quot;,
      "content": &quot;I typically have two main pieces of advice for basic information security that anybody can and should do. Use multi-factor authentication everywhere it is offered Use a password manager to generate good passwords, remember them, and make your life easier with auto-fill My previous post dealt with multi-factor authentication. This post will look at password managers. Passwords are the Worst Why do you need a password manager? The short answer: passwords are the worst, but with a few exceptions, we still need them for everything. There are a few problems with passwords: We need too many of them, so most people start reusing the same few passwords across multiple services. This opens a big security vulnerability: if the password for one service gets leaked, attackers will try the same username / password across lots of other services and get in to them all. It’s hard to think of a good one that is both sufficiently random to keep attackers from guessing it and memorable enough that you won’t forget it when you need it again a year later. This leads to people making simple passwords like “password” or again using the same one across multiple services. They’re annoying to type if you have all the random characters and symbols that are often recommended or required, especially in situations like a media app on a TV where you don’t get a full physical keyboard. These problems are all amplified if you share the account with other people, like your Netflix account that the entire family needs to access. They are a thin line of defense if they are the only thing keeping somebody out of your account. Even if the password is great, that’s still only one thing that anybody from anywhere could guess and break in. The last one is dealt with by multi-factor authentication. Most of the others – other than the typing on TV scenario – are dealt with by a password manager. The Password Manager Solution The gist of the idea of a password manager is that you only need to remember one password to get into your password manager account, and that account remembers all your other passwords. That solves the memory problem. Most if not all password managers also include a feature that will generate random passwords for you, storing them to the password manager all in one step. That solves the picking unique strong passwords problem. Most if not all password managers have browser extensions and auto-fill in apps on phones. You don’t have to look up the password and copy/paste it over. You just click on the right account from your password manager and it fills it in for you. You never even need to look at the password for the account you’re logging in to, let alone remembering it and typing it out. That solves the annoying to type out problem. Most if not all password managers have some mechanism for sharing those passwords with others. If one person changes the password, everybody gets the update and will auto-fill with the new one without ever even knowing it was changed. That solves the shared accounts problem. Most password managers come with some other tools that also help boost your password security: Identifies if you’ve used the same password on more than one service. If you already had repeated passwords, this helps you find them. Identifies which services offer multi-factor authentication, to help encourage you to enable it. Some even function as a code generator themselves and will auto-fill the code for you, although I personally prefer the extra work of having a separate authenticator app. Identifies if any credentials have been leaked, as found on haveibeenpwned.com. If any credentials show up here, you need to change them immediately – on everywhere with that password, not just the one that leaked it. Identifies weak passwords that have low levels of randomness and could be easier for an attacker to guess or brute force. Getting Started Most of the password managers have some level of free trial with limited functionality. That’s a great way to get a sense of what it is like to use one before you commit to spending on a particular service. Personally, I am now using 1Password. It offers all of the things I have mentioned above, in a friendly user interface and a reasonable monthly price. Before that I used the free version of LastPass. For a long time it had the best free offering by far, but that has been scaled back since. Then I used Enpass, which functions a bit differently. Instead of everything sitting in their secure cloud that you pay for on a monthly basis, it instead sets up your password vaults in your existing cloud services like OneDrive or Dropbox and syncs through that. It’s nice in that there’s only an up-front fee, not a monthly one, but needing to set up separate vaults in separate cloud services did make for a hassle trying to share with others. Take your time trying out some different services to find what fits your workflow best. But the most important thing is simply that you start using one regularly and take advantage of what it offers so you get unique, strong passwords that you no longer have to memorize. Plus, unlike multi-factor authentication that makes logging in a bit more complicated occasionally, a password manager is both more secure and saves you a lot of time and mental energy.&quot;
    },{
      "url": &quot;/2021/security-essentials-multi-factor-authentication/&quot;,
      "title": &quot;Security Essentials: Multi-factor Authentication&quot;,
      "content": &quot;I typically have two main pieces of advice for basic information security that anybody can and should do. Use multi-factor authentication everywhere it is offered Use a password manager to generate good passwords, remember them, and make your life easier with auto-fill I’ll look at the benefits of a password manager in another post soon. But first, let’s look at multi-factor authentication. The Idea The idea of multi-factor authentication is that you prove you are who you say you are through an extra “factor” beyond just a password. This is because it is relatively easy for a password to be stolen, distributed, and used. If all it takes for somebody to get into your important accounts is a username and password, that’s a low barrier to cause a lot of damage, especially when you add that most people reuse the same passwords on multiple sites (more on that in the password manager post). Verification Options The extra factor may be something you know like a security question or something you have like a fingerprint. This Microsoft doc breaks down the options in terms of what is available within Azure AD authentication, but most services offer some subset of those options. At the lowest end of the security scale are security questions, which are essentially just an extra password except easier to guess. Many places including Microsoft don’t offer a security question option. In the middle and the most common are SMS codes texted to your phone or voice calls made to your phone saying the code. That’s good enough for most people, but determined hackers can carry out sim-jacking to get through. The highest end of the security scale is an authentication app on your phone (Microsoft Authenticator, Google Authenticator) or a dedicated security key device that must be plugged into your computer via USB (YubiKey). With these protections, a hacker would have to know your username and password AND also have your phone logged in to the authenticator app or the physical security key. That means they would have to physically rob you on top of cracking your password, which significantly cuts down on how many people can realistically pull it off. It also makes it likely you’ll notice before much damage is done. Using an Authenticator App The authenticator apps may sound like more work than a simple text, but it really is easier on top of being more secure. I use Microsoft Authenticator and that’s what I will be referencing specifically, but Google Authenticator and others have similar if not identical functionality. Adding an account is simple. The hardest part may be finding the setting to enable it on the desired account. Some services make it obvious and strongly encourage you to enable multi-factor authentication. Others offer it but tuck it away in settings that you may not ever notice without looking for it. But once you do, if there’s an option to use an authenticator app, choose that and you’ll get a QR code. In the top right corner of Microsoft Authenticator is a menu option with “Add account.” It then offers you the choice of a Microsoft personal account, a Microsoft work or school account, or other. The Microsoft option will allow you to set it up with a simple login, but all of the options allow you to scan the QR code. A quick point of your phone camera at your screen and you’re good to go. The next time you try to log in and get prompted for the multi-factor – and it won’t typically be every time you log in since most services don’t prompt in low-risk scenarios like the same browser on the same computer at the same IP address as a login yesterday – then there are a couple of ways to verify your identity. Microsoft accounts in Microsoft Authenticator will push a notification prompt and all you need to do is select the Approve button. The small risk in this scenario is that people get used to clicking Approve whenever it pops up and may do it without thinking twice when an attacker is trying to get in. Otherwise you’ll need to copy the 6 digit code generated in the app into the web browser or app you’re trying to log into. These codes recycle quickly, generating new ones every 30 seconds, so it is virtually impossible for an attacker to guess the right one in time. That’s it! It’s true that it is more work than just a password, but very little. You won’t have to do it often, mostly just when you use a new computer or phone. In exchange, you get a level of security that stops approximately 99.9% of attacks before they even get into your account. You won’t regret taking that bit of extra time to set up multi-factor authentication, but you will absolutely will regret it if you don’t and somebody gets into your account.&quot;
    },{
      "url": &quot;/2021/wordpress-com-vs-org/&quot;,
      "title": &quot;WordPress: .com vs .org&quot;,
      "content": &quot;For years I’ve built most of my sites using the open source WordPress platform, i.e. WordPress.org. I recently decided to try the free offering of WordPress.com instead. Setup Setup is significantly easier on WordPress.com. If you already have a web host, setting up your own may not be much extra work, but if this first time setting up a website, there is always some degree of hosting configuration to go through before installing WordPress. I won’t detail those steps here and they will vary by the hosting provider, but at minimum you’ll need to pick a hosting provider, set up payment, buy a domain or point an existing domain to it, and install WordPress. It might be much more complicated like setting up databases, uploading the WordPress code yourself, and so on. With the .com, you just say you want a new site, pick a domain, and you’re pretty much done. If you want to get up and going in a hurry, it’s a good way to go. It is somewhat limited. It has WordPress.com ads which you won’t get any money from. You can’t use your own domain. You can’t add your own plugins. You can’t customize your own code. If you want to be able to do those things, you might be cheaper (but more work) looking elsewhere. But if you just want a personal blog like this, it’s enough, easy to use, and free.&#92;&#92;Update: soon after writing this, I started adding portfolio content and I don’t like the lack of flexibility in setting up those pages, so I am likely going to move to a self-hosted WordPress soon.Check out the WordPress.com pricing page for all the details of pricing. Maintenance There is zero maintenance needed for a site hosted on WordPress.com, other than paying for your domain and/or paid account here. All of the software updates are handled for you and you don’t control things like the PHP version on the server. If you’re hosting your own site, it is important that you maintain it. That means running the software updates on a semi-regular basis (at least every 3 months), updating the PHP version (about once a year) and testing to make sure it didn’t break anything, perhaps other hosting details like Plesk updates and security settings. If you don’t want to worry about maintaining your site or paying a third-party provider to do it, then WordPress.com is a good easy way out. Conclusion Both approaches to WordPress are great. Which one is better for you depends on your needs and goal for the site. Hosting your own WordPress site is great if you: Have a host anyway or want to have a host anyway Want to use all the features like editing your own code Are prepared to do the maintenance work or pay somebody else to do it On the other hand, using WordPress.com is great if you: Want something free Don’t need many features past having a public place to write Don’t want to worry about maintenance&quot;
    },{
      "url": &quot;/2021/flight-attendant/&quot;,
      "title": &quot;IT on Film: Flight Attendant Episode 3&quot;,
      "content": &quot;In episode 3 of The Flight Attendant (HBO Max), Megan agrees to do some corporate espionage against her husband’s large company. It isn’t clear what the company is – maybe it will explain as the story continues in later episodes – but it is clear that he has access to some significant trade secrets. So Megan encourages her husband to bring his laptop home, then casually finds the file and copies it to a jump drive. Two things immediately stood out to me as problems with this scene from a basic IT security perspective. The Computer Password This is one that I catch a lot in movies and TV. Why could she just open her husband’s work laptop and be logged in, no password needed? It’s possible there’s some simple user error here, where her husband was just on the computer and didn’t bother to lock it in his own house when he walked away. That is understandable, but given how important these trade secrets of a large company are, I would have thought there would be some stronger policies enforced on the machine. With Microsoft Endpoint Manager (and probably similar in other device management systems), the IT Administrator for this big company with important secrets could force the device to lock after x minutes of inactivity, as low as 1 minute. It could be paired with Windows Hello for Business – a fingerprint reader or infrared camera – so that he could still get in quickly each time it locked, but nobody else could. If this policy was set, Megan would have had to act fast to get in, even when her husband was careless about locking. Windows 10 also has options like Dynamic Lock, which will automatically lock the computer when a paired Bluetooth device (most likely a phone) is out of range. There may not be an option in Microsoft Endpoint Manager to force that, though, and would require the user to do the Bluetooth pairing. Of course none of this matters if Megan’s husband intentionally let her in, but there’s a lot the IT Admins can go to almost eliminate scenarios like her breaking in without his help. Information Protection Ok, so let’s assume that Megan has successfully gotten into the computer and found the file. Maybe her husband intentionally let her in. Maybe he was a little sloppy and either it didn’t have adequate protections or she was fast. That still leaves another problem: she still shouldn’t be able to copy it out to a jump drive. This can be prevented with Azure Information Protection (again, it is possible that similar products from other providers offer the same idea). If this file is as sensitive as the context of the show suggests, it should be tagged with an appropriate protection policy, which can define things like not being allowed to copy it. If that was enabled, Megan would not have simply been able to copy sensitive data onto a USB drive. Of course responsible IT policy wouldn’t make very good TV. In this case, this subplot would hit a dead-end quickly if Megan couldn’t steal a file that was properly protected. But in the real world, if you’re dealing with sensitive data, you should be using every tool at your disposal to keep it from being stolen.&quot;
    },{
      "url": &quot;/2021/civicrm-overview/&quot;,
      "title": &quot;CiviCRM: Overview&quot;,
      "content": &quot;The platform I worked with more than any other in my previous job was CiviCRM. CiviCRM is an open-source CRM system aimed primarily at non-profits that builds on top of an existing WordPress, Joomla, or Drupal website. Drupal is the most powerful because Drupal has great permissions control already and CiviCRM can tie in to those, but the others are fine, too. After a few years, I have a pretty good sense of the strengths and weaknesses that CiviCRM offers and will do a quick breakdown here. The Good CiviCRM does two non-profit functions really well: Contributions CiviCRM is great with contribution pages. If you want to add a donation page to your website, it will do that and it works well. It ties in with third-party payment processors, the most reliable and cheapest being iATS and Stripe, so they securely handle transferring the money to your account while CiviCRM tracks data about the donor and the donation. It’s smooth enough and works well out of the box, with a good set of configuration options. Events Similarly, CiviCRM has some good functionality around public event signup forms. People can sign up online, including payment through a payment processor, and all the data about the event and the participants will be saved in CiviCRM. It even includes functions like a limit on attendees, a waitlist for any beyond that limit, the ability to require an approval for the registration, and more. All of that works pretty well out of the box as well. The Cautions Everything Else If you look up CiviCRM you’ll see it offer several components which sound great, like bulk mailing, case management, and volunteer management. They aren’t lying, but they are really at varying degrees of a complete solution. A lot of the components end up a disappointment once you start putting them to heavy use and realize they don’t offer much beyond the minimum. For example, there is a bulk mail tool and some features of it work great, but with some major caveats: It’s up to you to configure the server in the best way to avoid being marked as spam, which is hard to do. The mailing designer tool is quite limited. There is some handling for unsubscribing from lists or opting out entirely, but nothing out of the box that handles scenarios like making sure you comply with Canadian Anti-Spam Legislation. Bugs All software has bugs. The difference between a big corporate cloud service solution and an open-source self-hosted one is that bugs in the big corporate solution will usually be fixed by that big corporation within a day before most people notice. If there’s a bug in CiviCRM, the best case is that the community has already issued a new release with a fix. In that case, you just need to apply the update. How long that takes depends on your experience and the speed of the server, but could be only around 10 minutes. The rest of the time, you have to make a decision to either leave the bug alone and accept the consequences or try to fix the code yourself. The latter requires a completely different expertise and there’s always some risk in messing around with code that handles sensitive data. Most of these bugs are not security risks. Your data is generally safe. The consequences of the bugs are usually more like being unable to edit contributions of time, or issue tax receipts, or view a certain report. They may not be site breaking bugs or security risks, but it’s also hard to fully trust the system housing important data and processes. It’s Not Really Free A common misconception is that open-source means that it is free forever. This is especially pointed out in contrast to the monthly costs of systems like Salesforce or Dynamics (both of which do offer charitable discounts). CiviCRM is not really free in practice. The code base is free, but that’s it. You still need to spend on: A website host, with some heavier processing power and configuration requirements than your typical website Maintenance work to provide software updates regularly, either your staff’s time or your money going to a third-party provider A consultant to help you set it up properly Programmers who can code any changes you need and maintain that code to continue to work with updates The big question is whether it will still be cheaper for your non-profit. The answer is that it depends. There are several factors to consider, particularly how many users you have and whether the solution can meet your needs out of the box or if you need to add more to it. For example, a potential best-case scenario: if you’re a charity (not just a non-profit) with less than 10 users and Salesforce can do everything you need, you can get that completely for free. That’s a better much better deal than CiviCRM. But if you have 50 users, or if Salesforce doesn’t do everything you need and you have to also buy licenses to several add-ons, that’s going to get much more expensive very fast. No Email Integration One of the most central concepts of a CRM system is that it is provides one system of record for everything you need to know about your constituents. Other CRMs like Salesforce and Dynamics provide tools built right into your email to track your email communications into the CRM, so there is no extra work needed to keep those records up to date. CiviCRM does not do that. The closest it offers is a tool that allows you to cc or bcc a separate email address, which CiviCRM then checks on a schedule (not immediately) and tries to match to a contact in the system. If there is no match, it doesn’t work. If there are multiple matches with the same email, it may put attach it to the wrong one. If you forget to bcc the special email – which in my experience everybody does quickly, no matter how much they liked the sound of the feature – then CiviCRM won’t record it. The result is that CiviCRM is missing most of your email communications, which takes away a huge amount of relevant information and leads to scenarios like trying to sell a client on the same thing that your colleague did last week. Conclusion CiviCRM can be the right choice for your non-profit, but you shouldn’t assume that just because it is designed for non-profits it will be better. Some hints where it may be the best option: You already have a Drupal, WordPress, or Joomla site that you want to integrate into, or you want to build a new website on one of those platforms anyway. You want a nice simple donation page and/or event management built into your website. You don’t need too many code customizations to add features that aren’t provided out of the box. This is particularly true if all you need are the contribution and event components. You don’t need to track email communication, i.e. you don’t have multiple people communicating with the client. If those don’t describe your situation, maybe spend some more time researching and do a full needs analysis before you jump in.&quot;
    },{
      "url": &quot;/2021/desktop-sync-files-onedrive-sharepoint/&quot;,
      "title": &quot;SharePoint: Desktop Sync Files&quot;,
      "content": &quot;What Goes Where? The first question to consider when planning a file structure in Microsoft 365 is what files go where. That’s more complicated in an online cloud collaboration system than it is for one person on the computer. It needs to make sense for everybody and it needs to be able to easily maintain proper permissions. My typical guidance is this: it comes down to who owns the file. If the file is just for you, go ahead and put it in your OneDrive with whatever folder structure you want. But if the file is for others to also access, it should be in SharePoint with a group owning it. That can make it a bit less intuitive for individual users to still have easy access to everything they need, which might now be scattered across their OneDrive plus a handful of SharePoint libraries. The files can be accessed in multiple ways, like in OneDrive in the browser, in SharePoint in the browser, or in Teams. But this post will compare two different strategies for accessing files on your computer, for those who are most comfortable having content synced to their machine and browsing in Windows Explorer. Sync SharePoint Libraries The older strategy is to sync each SharePoint library separately to the computer. To do this, navigate to each library in your browser, hit the Sync button in the toolbar, and follow the prompts to sync on your computer using the OneDrive client. The end result looks like this: It’s a completely different folder on your computer from your OneDrive. The biggest advantage of this over the next option is that there is a clear visual difference. It’s easy to remember which files are just yours in OneDrive and which are shared in SharePoint. That helps avoid scenarios like deleting a folder without realizing that deletes it for everybody, not just you. Add To OneDrive A newer feature is the “add to OneDrive” option which appears in the toolbar when browsing a SharePoint location. Instead of syncing it separately, it will treat the shared file as if it was part of your OneDrive, while still maintaining all of the group permissions. The end result looks like this: Everything is in one place, which many people like better than needing to remember which files are OneDrive and which are SharePoint. There is a little link icon in the status bar, indicating this is a shared library and not owned by that OneDrive, but that might be obvious enough to everybody. The other big advantage is that you won’t have to do this again on every device. The link is kept in your OneDrive, so even as you move between machines, you’ll only need to sync your OneDrive. If you’re on one machine all the time, that might not matter much, but even if you’re regularly switching from mobile to PC, it’s nice to know everything is always where you need it in OneDrive.&quot;
    },{
      "url": &quot;/2021/powershell-updating-site-scripts-designs/&quot;,
      "title": &quot;PowerShell: Updating Site Scripts and Designs&quot;,
      "content": &quot;This post begins a series on SharePoint site provisioning, unpacking some of the problems I’ve faced and overcome in building SharePoint site provisioning solutions. Site scripts and site designs are a great feature of SharePoint. They allow for developing and using templates on SharePoint sites that can do many useful things like: Create a list or library Apply column or view formatting on a list or library Apply a site logo image There is one annoying limitation, though. These scripts and designed are entirely created through PowerShell. That can make it a pain when you’re testing out a new script and need to update it frequently with each change, especially if you’re handling multiple scripts at once. So I wrote some PowerShell that handled some basic logic for me: Connect to the Microsoft 365 account Check if the script already exists If yes, update that scriptreading from the latest file with an incremented version number If no, create a new site script Check if the site design already exists If yes, update that design to include the relevant scripts, with an incremented version number If no, create a new design to include the relevant scripts The code is available on my GitHub account. The current version is minimal, but I may return someday to make some more updates to it like how to automate multiple scripts on the same design. With this PowerShell script, you only need to tweak it to fit your scheme, and then run the PowerShell each time you need to upload your site script’s changes to your SharePoint tenant. The README file on GitHub provides more detail.&quot;
    },{
      "url": &quot;/2021/microsoft-search-introduction/&quot;,
      "title": &quot;Microsoft Search: Introduction&quot;,
      "content": &quot;Microsoft Search may be the most underrated feature available as part of Microsoft 365. Maybe that’s because Microsoft themselves haven’t been promoting it that heavily, or maybe it’s because it is associated with Bing, the mention of which usually prompts the question “Bing still exists?” But those people are missing out on the potential productivity benefits that comes from having one search tool to find your data across all your Microsoft systems as well as yes, public Bing search. This was a common scenario for me in my previous job: I’m trying to help a client with an error they’re encountering. I have an error code or message to work with. I copy the error text into a new tab in my browser and hit enter to run a search. My results will include any company resources, e.g. if we’ve documented this problem before, or chatted about it in Teams. It will also include public Bing results. This makes it a one-stop shop to check the work resources first and then move on to public results if there isn’t anything. But that’s not all Microsoft Search can do. It starts with bringing together search results from many places into one, but also offers some more precise helpful tools. Q&amp;amp;A Microsoft Search supports an admin defining common questions and answers for their organization. This can be a great solution if you’re finding that people in your organization are asking the same questions over and over again. These could be common processes within a workday or could be common administrative questions. Acronyms Do you use a lot of acronyms within your workplace? Not everybody will always know what the acronym means. Defining those within the search will allow them to quickly see what the acronym stands for, possibly with some other extra context. Bookmarks Bookmarks allow you to specify other websites which are important for your users to be able to find. This is particularly helpful if the preferred search result is not at the top of the public rankings, so this way can make sure those answers stay at the top where users see them. Floor Plans I haven’t done a proper test of floor plans yet, but they allow loading office floor plans. Then users can search for another person in the company and see where in the floor plan their office is located. This can be helpful in larger offices where you may need to visit somebody for the first time. Locations If you find your users are regularly looking up certain locations, you can define them as a location with different search terms. The result will show with the map from Bing Maps. Suppose you have another office in Vancouver. You could set up a location titled “Vancouver Office” and people would be able to find it quickly without needing to know the address or trusting the public search. Connectors Connectors have the potential to make Microsoft Search far more powerful. This functionality is relatively new but allows for searching across other data systems as well, like a MediaWiki website, Azure DevOps, or a SQL Server. Without connectors, it’s already powerful to search across Bing and your company Microsoft 365 resources. This gets it a step further and lets you search data you control elsewhere, which eases any pain of keeping some data in non-Microsoft systems since you can still search them from Microsoft Search. Conclusion Many won’t seriously consider using the Bing component like I do. But even without that, there’s a lot of value in Microsoft Search to bring results across all of Microsoft 365 plus connectors plus custom search terms like acronyms into one place. It’s a great way to save time for your users searching and make sure they get to the right answers.&quot;
    },{
      "url": &quot;/2021/github-visual-studio-code/&quot;,
      "title": &quot;Visual Studio Code: Using GitHub&quot;,
      "content": &quot;Working in Visual Studio Code but need that connected to your GitHub repository? No problem. Getting connected to GitHub from Visual Studio Code is straightforward. It’s also possible to connect to other Git servers, but the authentication process is a bit more complicated, so I’ll stick to GitHub which is now my primary code repository. I’m also sticking with Windows, but the general idea is the same for other platforms with Code. Configuration To get started: Install Git for Windows. Open Visual Studio Code in the repository of your code. Click on the Source Control icon in the left menu. Follow the prompts to connect to your GitHub account by authenticating in your browser. Once that is done, the Source Control tab will now give you a couple of options, to either Initialize Repository (set up the source control on the folder but without connecting to GitHub) or Publish to GitHub (set up the source control on the folder and connect to GitHub). Select Publish to GitHub. The toolbar at the top will offer you the choice to change the name of the repository as well to make either a private or public repository. Choose which one makes more sense for your context. Finally, it will ask you which files from the folder to add to the repository. Unselect any that you don’t want to commit and continue. That’s it. It’s a pretty intuitive process even if you are brand new to GitHub. Once it’s set up, you get a few useful features: Visual Cues The most valuable component to me is the visual cues of changes. It’s like having a constant diff between your current version and the last commit. If you add a line, that line will be marked green in the sidebar. If you change a line, that line will be marked blue in the sidebar. If you delete a line, there will be a little red arrow where it was. Clicking on the area of those colour indicators will pop up a full diff of the changed section. That will also include quick options like reverting all changes on that section. Commit and Push Your standard git workflow actions can all be done from the Source Control section in the sidebar. It will show you everything that has changed since the last commit, with options to open the file, discard changes, or stage the file for the next commit. Above the file list you can enter your commit message and hit the checkmark to commit. Other actions like pushing and pulling are tucked away in the ellipses overflow menu. You still have to understand the basics of what is involved in git source control, but you don’t have to memorize the commands for each step. Everything is available visually. GitLens The extension GitLens adds a few more useful features and is worth adding. Among other things, it will give you blame lines that shows who last changed a line, when, and as part of what commit.&quot;
    },{
      "url": &quot;/2021/vs-code-remote-ssh-development/&quot;,
      "title": &quot;Visual Studio Code: Remote SSH Development&quot;,
      "content": &quot;One of the greatest improvements to my website development workflow came the day I discovered I could directly code on a web server in Visual Studio Code. Before this realization I was opening files with FileZilla, which worked but took a few clicks for each change: Browse to file in FileZilla Double-click to prompt opening in Code Switch over to Code, make changes, and save the file Switch back to FileZilla Say yes on the prompt to upload the change Test in browser to see if it did what I wanted it to do Repeat 3-6 if necessary for next change By comparison, when you access the file directly in Code, that process gets cut down to: Open file in Code, make changes, and save the file Test in browser to see if it did what I wanted it to do Repeat 1-2 if necessary for next change If you’re making a lot of changes like testing out new CSS designs which may take several iterations to get just right, all those extra clicks add up to a lot of time. It’s also easier to remember; something that happened for me regularly is I saved the file in Code but forgot to go back to FileZilla to upload the file, then was briefly confused why my changes weren’t showing up yet. Setup I won’t go step-by-step in getting this setup. There are lots of great blogs out there to walk you through it. Peacock extension This extension applies a different colour theme to my workspace for each website I was connected to. This is really valuable especially if you find yourself working on multiple websites on the same day as the visual cue helps avoid accidentally updating the wrong site. You can manually set these for each site, which could be used for scenarios like setting the code window to match the branding of that site for an even stronger visual cue, or you can check the box to surprise me with a random colour whenever one isn’t already set. Git integration I wrote another recent post that breaks down how Git integration in Code in general. The main difference here is how it ties into the next point: if you can’t use the integrated shell because of the security on the system, you also won’t be able to push in the version control interface. Demo of the source control window Shell integration This is not something I can take advantage of due to security settings of the web server I mostly work on, but it is a great idea if your server allows it. With this feature, you can directly use your shell commands within Code instead of needing a separate window in something like PuTTY or the Windows Subsystem for Linux (WSL).&quot;
    },{
      "url": &quot;/2021/onedrive-family-plan-loophole/&quot;,
      "title": &quot;OneDrive: the Family Plan Loophole&quot;,
      "content": &quot;When you get a Microsoft 365 Home plan, you get 1TB of OneDrive storage per user. That’s a good amount of storage, but you might want more. For example, I have a lot of photos going back almost 20 years. Tens of thousands of photos. A significant subset of those also have copies of the original RAW file taken from the DSLR, which are much larger. 1TB is a lot, but it’s reasonable that even for typical consumer purposes you might hit your limit. Here&#39;s how it works: Get a Microsoft 365 Family plan if you don’t have one already. Even without this loophole it is a pretty great deal. In the family setup, share with your family members that you want to have a subscription, whether that’s for OneDrive or the desktop apps or something else. Distribute your family’s files across the family’s accounts. For example, perhaps I own the raw photos while my partner owns the edited jpg photosShare from the person who owns the folder to everybody else in the family that needs access. For example, if I own the raw photos, I then share them with my partner so they also show up in her OneDrive. At this point, the raw photos count against my storage limit and the edited jpg photos count against my partner’s, but we both have access to everything. What if that still isn’t enough storage? If you haven’t used up all of your family slots yet, you can create new Microsoft accounts for free and repeat steps 2-4 with the new account. It’s a bit more hassle to get started than if you just had 6TB of storage in one Microsoft account, but after that initial setup is done, it is functionally the same: you’ve got 6TB of storage, all accessible from one OneDrive account, at a cheap $110 CAD per year (plus that cost also gives you the desktop Office apps, Skype minutes, etc.).&quot;
    },{
      "url": &quot;/2021/accessibility-in-wordpress/&quot;,
      "title": &quot;WordPress: Accessibility Basics&quot;,
      "content": &quot;Accessibility is an important part of designing a website. You want your site to be usable to as many people as possible, right? Here are some things to consider as you develop a WordPress site to make it as accessible as you can. Testing with WAVE The best tool for testing out a website is the WAVE evaluation tool, which is available as an extension for your browser. It will scan the code of the page and tell you any accessibility problems it finds, flagging them with different levels of seriousness. It also has a section for contrast issues when the foreground colour of text is too close to the background colour. Note that with WordPress WAVE will always pick up a few issues from the admin toolbar if you are logged in when you run it. That’s not great, but it’s out of the control of the site developer and it doesn’t impact regular visitors. Pick a Theme The theme decision will determine a lot of your site’s accessibility. Some themes have a lot of accessibility problems built right into them and it will be hard for you to change the code yourself to fix it. Take your time shopping for themes. Activate a theme and use the WAVE accessibility tool to see how well it scores. If there are little things like missing ALT text, you might be able to work around that without changing the theme, but be cautious if it has a lot of issues like inconsistent headers or a search bar without a label. You may not find a theme that’s perfect, but if you put in the time, you’ll at least get pretty close. Contrasting Colours Picking a colour scheme of a site goes beyond simply looking good. There needs to be strong enough contrast between the background colour and the foreground text colour to be sure that people with low vision can differentiate. There are levels to this. Hitting the most strict AAA sometimes can be hard to do in light of other factors like brand consistency, but you want to make sure you at least hit the AA standard. The WAVE tool can help measure your contrast and even help you pick colours for stronger contrast. The WP-Accessibility Plugin Sometimes if your theme doesn’t do a great job hitting all the accessibility requirements, the WP-Accessibility plugin can help. It includes a few helpful tools like: Adds the title of the blog post to the “read more” text. This helps for people with screenreaders. When all the links say “read more,” a screenreader jumping through links just says “read more” over and over again, which isn’t helpful. It’s a lot more helpful when it says “read more Accessibility in WordPress.” Adds an accessibility toolbar on the side of your site for adjusting font size and contrast. Attempts to add a label to the search bar, although I’ve had mixed success with that option. Use the Right Headers That covers the general site setup. Now we’re into features that anybody should consider when writing content like this blog post. Many people misuse header tags. They are not solely for formatting. They also help identify how content is related to each other. Your site should have an h1 tag, either for the site title or page title. Then everything below that should be an h2. Content that is a sub-category of an h2 should be identified with an h3. And so on. You shouldn’t skip tags from an h2 down to an h5, for example, or always use h2 no matter what without considering what should maybe be lower. (Almost) Always Add ALT Text This is the accessibility issue that most often gets discussed, but it’s still very easy to miss in WordPress. ALT text is a technique to describe what a screenreader says to a user that can’t see the picture. You should always set ALT text unless the photo is purely decorative like an icon. WordPress offers the option to set ALT text, but it doesn’t require it. It’s also not quite as in-your-face in the new Gutenberg block editor as it used to be in the classic WordPress editor, so it’s easy to add an image and forget to dig into the settings for the ALT text option. Update: in 2023 I wrote another post discussing how descriptive ALT text should really be, and in some cases you may not want ALT text at all.&quot;
    },{
      "url": &quot;/2021/web-browser-edge-chrome/&quot;,
      "title": &quot;Web Browser: Microsoft Edge vs Chrome&quot;,
      "content": &quot;It should come as no surprise that my web browser of choice is Microsoft Edge, given Microsoft’s offering is my choice in almost everything (except Microsoft News which I don’t like at all). This was not always true. The previous version of Edge was not great for me. Internet Explorer before it was very bad, not just for me but pretty much everybody. That changed when Microsoft opted to rebuild Edge based on the Chromium open-source code base. It now has all the best parts of Google Chrome including themes, extensions, and the rendering engine. On top of that, it does a few things I like better than Chrome. Privacy The key difference for me is privacy. Most of Google’s profit comes from advertising targeted based on a massive amount of data they know about you because they track you all over the Internet. Chrome helps them build up more data about you in a few ways. For one, Chrome syncs your account using a Google account. All of your browsing is data sent back to Google. Microsoft has a similar syncing function but using a Microsoft account instead. It’s a fair argument if you don’t see a difference between giving lots of data to Google vs Microsoft, but Microsoft’s business is not built on data mining in the same way. Google Chrome is also very forgiving of everybody else’s trackers, like Facebook’s. Most websites you visit have a bunch of trackers on them so ad sales companies like Facebook can follow you around the Internet. If you don’t want to be followed around the Internet, Edge is more aggressive about blocking these blockers than Chrome. Speed Edge uses up less memory and runs faster. That may be because of the blocking of trackers, since all of those trackers running take up resources in the system. If you’re somebody who runs a lot of tabs at once, or using a machine with limited memory, this could be a deciding factor on its own. New Tab for Office Accounts If you’re logged in through a business account, one of the options for the New Tab screen is your Office documents. It’s essentially the same screen you get when you visit office.com. Putting that on the New Tab screen is a great way to be reminded and have quick access to your most important documents within the flow of your normal browsing. Timeline When you use Edge, on desktop or mobile, it integrates with your Windows 10 Timeline. This makes it convenient to scroll back through your history if you need something, like if you first viewed a page on mobile and now want to continue on desktop. Update: it sounds like the Timeline sync will be going away for consumer accounts and will only be available for business accounts soon. Vertical Tabs Lining up your browsing tabs on the left side of your screen instead of the top takes some getting used to, but once you get the hang of it, it is a much more efficient use of space at least on widescreen monitors where you already have more width than most websites are designed for, but could really use the extra height. The main downside is that it’s slower to switch tabs using a mouse, so I may get used to Alt+Tab to switch instead of using a mouse so much. It’s Already Installed This is a little thing, and yes, the argument I’m about to make is exactly what got Microsoft into anti-trust trouble years ago. The argument is simple: Windows 10 comes with Edge. Having Edge and Chrome installed means that both update in the background, eating up memory. Having both means double the potential security vulnerabilities. And of course, it simply takes that bit of extra time to go download and configure Chrome. Even if it was otherwise equal to Chrome, there is no reason to install Chrome. If it’s a bit better, as I believe, there’s even less reason. If you haven’t tried Edge yet and you’re not heavily invested in the Google ecosystem, it might be worth a shot for you too.&quot;
    },{
      "url": &quot;/2021/onedrive-sharepoint/&quot;,
      "title": &quot;OneDrive vs SharePoint&quot;,
      "content": &quot;The first question that typically comes up when moving files to Microsoft 365 is this: what’s the difference between OneDrive and SharePoint? Which files should I put where? Permissions The most important difference is the default permissions. In short, files that are for just you should be in OneDrive. Files that are for others should be in SharePoint. OneDrive is individual by default. SharePoint is shared by default. This includes files where you are doing most/all of the work but others might need to access them at some point in the future. You want to minimize friction if somebody else on a team suddenly needs to access that file. At best when the file was put initially in your OneDrive, they need to request that you share it, which could have been avoided by putting it in a shared location in the first place. At worst, what if you aren’t available? They might not be able to get to the file they need right now, or might need to take up the time of an administrator to step in. There is a mechanism for admins to give one user access to another user’s OneDrive. This can be helpful in circumstances like a sudden termination where it turns out the terminated employee had some files that others need. But at most this should be used for times like a new employee taking over the same role as a previous user, where there are files that are only used by that one role. SharePoint sites are most easily set up attached to Microsoft 365 Groups. Those Groups already carry permissions across all the services of Microsoft 365 like Planner and Teams. If you devise some logical groups, your files can neatly into those groups. Users don’t have to think about permissions for each file; they just need to put it in the relevant group. This also extends to search. When files are in SharePoint, anybody with the permissions to see it will also be able to find it within Microsoft Search, unless search indexing is turned off for that SharePoint site. Columns, Views, and Content Types SharePoint brings a pile of functionality that isn’t available in OneDrive. These include: Columns, for tracking custom data about the file Views, for presenting the files in different ways such as sorting, filtering, and grouping the results Content types, for enforcing certain types across multiple document libraries, including possibly with file templates Power Automate processes shared with others Power Apps forms shared with others If your workflow benefits from anything more complicated than just a few people able to see the same file, these extra tools can make a huge difference. This lines up nicely with the permissions aspect, since those functions are also generally most useful when the file is for multiple users.&quot;
    },{
      "url": &quot;/2021/accessing-microsoft-365-files/&quot;,
      "title": &quot;SharePoint: Accessing Files&quot;,
      "content": &quot;Suppose you’ve now set up all of your files for your organization in the ideal way, with some in individual user OneDrives and others in group SharePoint sites. The natural follow-up question is: now how do I access those files within my workflow? There are a lot of options. This isn’t an exhaustive list, but in this post I’ll quickly mention several different ways to access your files that are housed in Microsoft 365 (OneDrive for Business and SharePoint). If you know of more that I missed, leave a comment. Directly in Your Browser You can access in your web browser directly looking at the location of the file, either on the SharePoint site or the OneDrive site. Browsing on the site has some advantages like seeing all the columns (metadata) about the files and views for easy filtering and sorting. This is something that is more worthwhile if you’re going to use other features of SharePoint, particularly if you’ve built navigation between the different sites. If you don’t have an intuitive navigation between the sites, it still creates the problem of having to find the file. It depends on your environment and your workflow, but for the most part, this is more of a fallback plan and not how you are likely to most often access your regular files. Teams If Teams is your primary work dashboard, this is the best solution all around. In this scenario, you create a Team and the channels within the Team. Each channel will automatically create a folder in the document library of the corresponding SharePoint site, named to match that of the channel. This folder is automatically added to the channel as a tab at the top called “Files.” This creates a great intuitive interface where you can do almost everything in Teams. As of this writing there are few exceptions for features available in the SharePoint bar that isn’t on the Teams bar, features which are more for power users: Edit in Grid View Add to OneDrive PowerApps Automate Alert Me Manage My Alerts There is, however, a button to open the document library in SharePoint in your browser. The average user doesn’t need to do any of those functions very often if ever and would be perfectly happy doing all their file access in Teams, but if you do start out in Teams and realize you need more, there’s a quick link to do it. OneDrive Sync Client If Teams is the most modern way to work, using the OneDrive sync client is the most traditional and the most comfortable for a lot of people. In this way, you access files housed in the cloud just as easily as you’re already used to accessing files on your computer. In another post, I broke down two different approaches to syncing your files from OneDrive and SharePoint together to your computer with the OneDrive sync client. Desktop and Mobile Apps Know that you’re looking for a Word document, particularly one that you’ve accessed recently? A quick solution to open it again is to open the Word app. The launch page will show your recent documents, your pinned documents, and documents shared with you. You can also fully browse your SharePoint structure to find the file, although that might not be the most intuitive for many people. This is also true for mobile. There are mobile apps for Word, Excel, PowerPoint, Office (one app with lightweight versions of the core apps), OneNote, OneDrive, and SharePoint. These all work for finding your recent files of that type and with some degree of ability to browse. Windows Search On a related note: Windows Search will turn up results on all recent documents you’ve opened on that computer, not just files that are synced and located on that computer. If you’ve opened it once from Teams and closed it, you can get back quickly using the search bar built into Windows. Office.com (and Similar) There are multiple places that give you more or less the same interface that helps surface your recent documents, pinned documents, documents recommended for you, files shared with you. These include: The Office.com launching page or the Office app in Windows The New Tab page in Microsoft Edge when you’ve set that up and you’re logged in to sync with a Microsoft 365 business account The homepage in Delve The exact details vary, but they share in common a similar idea of trying to help you get back to your most important files quickly. You won’t necessarily find everything this way, but a lot of the time you just want to find that file again that you were using yesterday, and this will help a lot in those scenarios. Microsoft Search I’ve written another post already about my appreciation for Microsoft Search as perhaps the most underrated tool in Microsoft 365. Android Launcher If you’re using Android for your mobile device, you can use the Microsoft Launcher. This is a nice Android launcher in a few ways, but among its features are some quick widgets for accessing things like your calendar and yes, your recent documents. If you find yourself in scenarios like working on a file on your computer and then wanting to continue on your mobile during your bus ride home, this is a helpful way to immediately do that.&quot;
    },{
      "url": &quot;/2021/smtp-google-account/&quot;,
      "title": &quot;SMTP Through a Google Account&quot;,
      "content": &quot;Many applications like business scanners or hosted CRM systems come with features that send email. To do so, it has to be able to connect to an email account that it sends on behalf of. The main challenge is that most good modern email services are strict about allowing emails to be sent on their behalf. Microsoft 365 is particularly strict. Google is a bit easier, but does require an extra step which is not obvious. First, decide which Google account you’ll want to do this. I recommend that it is an account solely for this purpose. It definitely should not be a regular user account that might contain sensitive data. The reason is what will come up below: by nature of this functionality you will need to disable some of the advanced Google security so that it is protected only by a password (generally a bad idea). Create a specific Gmail account for this purpose, like [company name].crm@gmail.com. Configure Settings Next, you need to find the options in the application trying to send the email. Look for something with “SMTP” in it. It could also be something more generic like “outbound email.” Once you find those settings options in the system that you want to be able to send from as your Gmail, fill in the valued needed for Gmail. The values are mostly easy to find on the Internet, but I’ll copy them here as well: SMTP Server: smtp.gmail.com Requires SSL: yes Requires TLS: yes (if available) Requires authentication: yes Port for SSL: 465 Port for TLS (if available): 587 Less Secure Access There’s one more important step that is not obvious. By default, Google accounts do not allow you to access by SMTP over anything it deems a “less secure app.” Your CRM or scanner connecting over SMTP will probably be considered a less secure app. So, you’ll also need to disable that option. You can do that here when logged in: Less Secure Apps option. Note that you’ll also have to turn off two-factor authentication to be allowed to do this. Once that’s done, go ahead and test to confirm it worked.&quot;
    },{
      "url": &quot;/2021/change-profile-picture-guest-microsoft-teams/&quot;,
      "title": &quot;Microsoft Teams: Change Your Profile Picture as a Guest&quot;,
      "content": &quot;If you’ve operated as a guest in a different company’s Microsoft Teams tenant, you may have noticed that you don’t get the same user photo that you do in your own tenant. Instead, you only get your initials in a colour background. If you’re signing in as a guest regularly in a different tenant, that can get a bit annoying. There’s no clear interface option to change it either. Fortunately, Yannick Reekmans has found a way to do it and detailed it in his blog. I won’t cover all the steps here and I have not personally tried it yet, but this sounds like it would be worth it – at least for those of us who are comfortable looking at developer tools in our browser. Check out the details there and let me know if it works for you.&quot;
    },{
      "url": &quot;/2021/data-loss-prevention-policies/&quot;,
      "title": &quot;Data Loss Prevention (DLP) Policies&quot;,
      "content": &quot;Data Loss Prevention in Microsoft 365 is a feature that helps prevent loss of sensitive data (that makes sense) coming out of your system. This can be within emails or within files, although the latter requires a higher license. Here’s how it works. Sensitive Data Types At the heart of the DLP functionality is the idea of sensitive data types and the ability to recognize those types automatically using pattern recognition. A common example is a credit card. In general, you don’t want people in your organization to be able to share credit card information (especially not by email). Credit card numbers follow a particular pattern of 16 digits. That makes it easier for the Microsoft pattern recognition to detect. Other patterns may not be as easy to identify, but Microsoft is able to draw on a massive amount of data points and come up with a level of confidence. It may come back saying that it is 70% confident that this document contains a credit card number. You can also add your own sensitive data types and train them with enough data for the machine learning machine to establish some confidence in what does and does not meet that type. Perhaps you have your own internal system with ID numbers for clients that you want to protect in some way; you could set that up as a sensitive data type. DLP Rules The next question is what you want to do when that sensitive data is identified. This is where the DLP rules come in. With DLP rules you can set multiple options: What sensitive data type(s) should the rule apply to Name of the policy What systems should the rule be monitoring, for example Exchange, OneDrive, SharePoint What to do when the data type is detected: notify the user, email an admin, and/or restrict access to the content Other Uses of Sensitive Data Types These sensitive data types can also inform other types of security and compliance rules throughout your Microsoft 365. For example, you can have retention rules based on the sensitive data type scanner which says, for example: if the file has a passport number, retain for 7 years after last modification and then delete. Learn More This is a very broad introduction to the idea. To dig in deep to what is possible, check out the Microsoft docs.&quot;
    },{
      "url": &quot;/2021/web-development-workflow/&quot;,
      "title": &quot;My (Freelance) Web Development Workflow&quot;,
      "content": &quot;When I work on a freelance website (I have more advanced tools in my day job), especially once I need to deploy some custom code, I have several tools at my disposal I want to set up. Here’s what those tools and that setup process looks like. For the purpose of this post, I’m assuming I already have the SFTP and SSH credentials from the website host. SSH Keys The one-time need is to prepare my SSH keys. This requires three files which can be created with PuTTYgen, part of the package that comes with PuTTY. Create your SSH key using the “Generate” button, if you don’t already have one you could load. Save the private key file using the “Save private key” button. It is often recommended to put this in a folder called .ssh in your user directory, but at least for my purposes it can be anywhere. Create a file named “id_rsa.pub”. Copy the text at the top of the PuTTYgen window and paste it into that file using a code editor and save. Use the “Conversions” -&amp;gt; “Export OpenSSH key (force new file format)” option in the menu. Save that file as “id_rsa” (without the .pub extension). The file created by step 2 will be used in PuTTY. The file created in step 3 will be added to each site’s authorized_keys file as well as GitHub. The file created in step 4 will be used by Visual Studio Code. PuTTY PuTTY’s piece of the puzzle is to provide SSH access for executing shell commands on the site. This may not be necessary if you can use the integrated terminal in Visual Studio Code, but the host I use does not allow that due to security restrictions. Before adding any specific site, I want to change a couple of settings for the default configuration to aid in using the SSH key for authentication. In the sidebar menu, select Connection -&amp;gt; SSH -&amp;gt; Auth, then: Check the box for “Allow agent forwarding.” This is necessary to use the same SSH key to authenticate to GitHub. Under “Private key file for authentication” click the Browse button and navigate to the private key file. Then go back to the top settings option, select “Default Settings” and “Save” to overwrite the defaults. When it’s time to add a new site, there’s little configuration needed: In the “Host Name” field, enter &amp;lt;username&amp;gt;@&amp;lt;hostname&amp;gt;. Enter a new title under “Saved Sessions.” Click “Save.” In the future when I want to open it, I simply double-click on the saved session. Authorized_keys If you’ve done the first part and try to load up a site, you will get prompted for the password. That’s because the web server hasn’t been told it’s safe to authorize with that SSH key yet. To do that, update the file at ~/.ssh/authorized_keys to contain a line with the contents of id_rsa.pub file. You can have multiple lines in authorized_keys if there are more people working on the site. There are lots of approaches to updating that file (cPanel, SFTP, etc.), but I usually do it by connecting first with PuTTY (using a password the first time) and then editing the file with vi: mkdir .ssh vi .ssh/authorized_keys i #enters insert mode #paste the contents of your id_rsa.pub by right-clicking #esc key to exit insert mode :wq! #saves and exits vi chmod 700 .ssh chmod 700 authorized_keys #limits access to this file I then close the session in PuTTY and try opening the saved session again. It should be able to connect and only need the SSH key password, not the site user password. When I first tried this, it also worked to allow me to push from the server to GitHub. That stopped working when I came back a few months later, though. So here’s what I also had to do: Upload the id_rsa file to the .ssh folder Assign it restricted permissions and then add the key chmod 700 ~/.ssh/id_rsa ssh-add ~/.ssh/id_rsa GitHub Any code that I want to customize unique to this site, not just using available plugins and themes published elsewhere, I want to commit my code to GitHub. Here’s a common scenario for me: I’ve created a child theme to apply some tweaks to a published theme elsewhere. When a core theme gets updated, that can create conflicts, so I usually want to see specifically what files got updated. If that includes any of the files I overrode in my child theme, I want to know that. Then I can look at the specific lines that have been changed, and copy those changes if necessary into my child theme. Here are the commands for me to configure the git repository, using the themes folder as the example: cd ~/public_html/wp-content/themes git init git add * git branch -M main git remote add origin [SSH path to repository] git push -u origin main It is important that you use the SSH path to the repository, not the HTTPS path. You’ll need it to be using the SSH path to be able to forward your SSH key using PuTTY to connect to GitHub. Note that it is possible to initialize the GitHub repository all in Visual Studio Code, but it does default to using the HTTPS path instead of SSH, so doing it that way means you’d have to change the .git/config file later. Visual Studio Code Visual Studio Code is my code editor of choice, so this is an essential piece of the puzzle if I am doing any customizations to the site. I’ve written more about Visual Studio Code in the past: Remote SSH in Visual Studio Code. GitHub in Visual Studio Code. When I have a new site to add, I do this: Click on the Remote SSH icon in the left menu. Scroll over the “SSH Targets” header so the little gear icon appears and click on that. It should pop up offering different locations of SSH config files on your computer. Select the relevant one. Add a new section to the file using the format below. I like to organize them alphabetically and use the URL of the site for the host name. Host [URL] HostName [URL] User [SSH username] IdentityFile [Path to id_rsa file] ForwardAgent yes Refresh the list of SSH targets in the remote explorer so the new URL will appear. Click on the “Connect to Host in New Window” icon beside the new URL. Enter the SSH key passphrase to connect. Click on the file browser icon in the left menu and select “Open Folder.” Browse to the root location of the site that you want to work from. Once you’ve opened a specific folder, you can jump straight to that folder from the SSH Targets list. FileZilla FileZilla is a great tool for SFTP access to a website. I don’t need this too often, but it can come in handy if I have to transfer files from my computer to the web server or vice versa. This is all done in the interface. Click on the Site Manager link in the top left. Click the New Site button. Fill in the credentials needed. Click Ok to save, and then Connect to start the connection. After that, I like to export my list of sites. This will serve as a backup if I need to switch computers: This option is available from File -&amp;gt; Export.&quot;
    },{
      "url": &quot;/2021/microsoft-365-guest-users-google/&quot;,
      "title": &quot;Azure AD: Guest Users from Google&quot;,
      "content": &quot;Most users of Microsoft 365 have encountered the idea of guest users at some point. If you want to share company resources to somebody outside the company, you can do that. By default this works with the guest able to sign in with a business Microsoft account (Office) or a consumer Microsoft account (MSA). This is not on by default, but you can take that a step further and allow guest users to authenticate logging in to guest resources using their Google accounts. In many scenarios, this is helpful compared to the default. Especially if it’s something like dealing with volunteers, you don’t really want to force them to make a Microsoft account if they don’t otherwise have one. If they don’t have a Microsoft they probably have a Google. It can also help if you use Google Suite for your main company email but also want some other functionality with Office, so that you only need to login with the Google rather than have a whole new set of users. I won’t repeat the full steps here. Microsoft’s documentation covers that well. I just wanted to share that it was possible as it is a bit of a hidden gem that isn’t obvious when you get started in Microsoft 365.&quot;
    },{
      "url": &quot;/2021/microsoft-conditional-access-policies/&quot;,
      "title": &quot;Azure AD: Conditional Access Policies&quot;,
      "content": &quot;Passwords are inadequate. Even for standard consumer tools, you should have at least two more tools in your toolbox: a password manager and multi-factor authentication. Those help make passwords suck less. But they do leave open some questions like: should you need to perform multi-factor authentication every time you log in? Should access be all or nothing, or should there be any accounting for degrees of risk? Conditional access policies, part of Microsoft Azure AD, goes a step further. It’s built on a zero trust model, meaning that it starts by assuming that you shouldn’t have access and then builds up trust based on several factors. Those factors usually include a password (except for the new passwordless option), but are not limited to them, in order to help cover over all the problems that come with passwords. Some of these other conditions include: Sign-in risk: a tool in Azure Identity Protection that evaluates risk on this particular sign-in, which includes variables like signing in from an anonymous IP address User risk: also part of Azure Identity Protection that evaluates risk based on the user’s behaviour over time Device platform: what kind of device are they trying to sign in on Location: based on IP address, e.g. your office Device state: paired with device compliance policies from Endpoint Management, this can set access rules based on whether the device is compliant or not Conditional access policies allow you to define exactly what you want to happen depending on a variety of risk factors. Along with those conditions, you get options to configure: What users or groups this policy applies to. For example, you probably want stricter conditions for admin accounts. What cloud apps or actions are affected. For example, perhaps in a low user risk scenario, you want to block access to SharePoint but not email. Whether you want to block or grant access. Grant access will allow signing in, but only after some extra security checks, like multi-factor or device compliance. Imagine a scenario like a user taking their work laptop to a coffee shop (use the location for the condition). You may want to allow that but require MFA again. The extra MFA check helps protect against the possibility that the laptop was stolen but it still lets the real user in. Session restrictions which can require signing in more frequently. In that coffee shop example, you may want to require signing in again every hour to limit potential that the user left their machine open to go to the bathroom. Whether the policy is on, off, or report-only. The report-only option can be helpful as a trial run to see how many people meet the conditions over a week before starting to enforce the restrictions. You may discover a lot more people work in coffee shops than you realized and that could alter your strategy of how to best enforce security.&quot;
    },{
      "url": &quot;/2021/enrolling-devices-endpoint-manager/&quot;,
      "title": &quot;Microsoft Endpoint Manager: Enrolling Devices&quot;,
      "content": &quot;Suppose you’ve started to move toward managing your devices in Microsoft Endpoint Manager (Intune). There are a lot of methods available to do that. I’ll highlight just a few of the most interesting: Windows Autopilot If the device was set up with Windows Autopilot, enrolling to Endpoint Manager is one of the options to happen immediately as part of the setup. No further actions are necessary. Auto-enroll Auto-enrollment makes the process very simple. If you’ve set up automatic Mobile Development Management (MDM) and/or Mobile App Management (MAM) for all users or for a group that includes that user, then as soon as the user signs in on the machine with their business account, the device is added to Endpoint Manager. There are some other settings around this including whether they have to agree to a terms of use when they first sign in. Device Enrollment Manager If you prefer a more manual approach, you can designate specific staff (e.g. your IT team) as Device Enrollment Managers. Users with this permission can enroll up to 1000 devices each. Co-management If you have an on-premise network with Configuration Manager, you can use co-management to split MDM duties between the two systems. You can even pick which workloads you want to be handled by Configuration Manager and which you want to be handled by Endpoint Manager. This can be a good strategy if you already have Configuration Manager and want to transition to Endpoint Manager one workload at a time, or could be a more permanent solution if you want to keep certain workloads on Configuration Manager.&quot;
    },{
      "url": &quot;/2021/cannot-modify-header-information-headers-already-sent/&quot;,
      "title": &quot;Cannot Modify Header Information: Headers Already Sent&quot;,
      "content": &quot;Recently I loaded up a client website to do some standard WordPress. The public site was fine, but when I tried to login to the admin, I got the dreaded White Screen of Death. For those unfamiliar, this is a blank white screen, with no error visible to help diagnose the problem. I began doing some general research on the White Screen of Death and trying the different common solutions: disable plugins (by renaming the plugins folder via FTP), update themes, update WordPress core. Nothing worked. Then I checked out the Apache error log and got a big hint: PHP Warning: Cannot modify header information – headers already sent by (output started at [file structure]/wp-config.php:174) in [file structure]/wp-login.php on line 530 So I did some more research on that error and found this: How to fix PHP/WordPress Warning: Cannot Modify Header Information (templatetoaster.com). The bottom line: look at the first file cited. That’s where the problem is. In my case, the error was in my wp-config file. It was as simple as having extra whitespace at the end of the file. That’s enough to take down the entire admin section of the site. Ultimately it was an easy fix, but hard to find, so hopefully this helps you if you have a similar problem.&quot;
    },{
      "url": &quot;/2021/google-maps-error/&quot;,
      "title": &quot;This Page Can&#39;t Load Google Maps Correctly&quot;,
      "content": &quot;You may be browsing a website – your own or somebody else’s – and see this error: “This page can’t load Google Maps correctly. Do you own this website?” It also has “For development purposes only” as a watermark repeating over the whole map. This error has become pretty common in the past few years. It comes out of a change in Google policies. Previously, you could use the Google Maps API to put a map on your website with no extra charge. You did not need to have a credit card on the Google account. That has changed. There’s no notification, so you won’t notice this unless you happen to visit the map on your website again, which most people rarely do after it’s been designed. You now need to meet two requirements: An API key on the site connecting to a project associated with your Google account*. If you’re building a Google Map into a CMS website (Drupal, WordPress, etc.) then it probably includes some instructions on how to do this connection. A valid payment method on the Google account. You can do this from pay.google.com. The billing account of the Map API project has to be connected to that payment method. As with most other scenarios like this, you should have a generic Google account that can be shared by at least a couple of staff members. Do not use an individual user’s Google account – personal or business – as that will cause complications if you ever need to make changes and that person isn’t available. Importantly, you get a certain amount of API credit for free before you have to start paying. Most small to medium websites will likely end up not paying any extra fee. That’s at least true of Google’s rules today; they could easily change someday. You do need to be aware of that slight possibility.&quot;
    },{
      "url": &quot;/2021/windows-autopilot/&quot;,
      "title": &quot;Microsoft Endpoint Manager: Windows Autopilot&quot;,
      "content": &quot;Windows Autopilot is a great system for deploying new Windows 10 devices, especially in the age of COVID-19 and so many working from home. Here’s the official documentation breaking down the details. The high level overview is that the user of the machine receives it, perhaps at home or perhaps in an office. They turn it on. Depending on the configuration options the admin has set up, they may have as few as two things they need to do to get the device ready for use: Connect to the Internet. Login with their business Microsoft 365 email. This is much more straightforward than a typical Windows 10 setup, helping get around all those extra questions that usually come up. It also takes care of adding some key points of configuration, including connecting the device to Azure AD, enrolling in Microsoft Endpoint Manager, and restricting the creation of admin accounts (if you’re effectively being a remote admin).&quot;
    },{
      "url": &quot;/2021/power-automate-create-site-with-sharepoint-rest-api/&quot;,
      "title": &quot;Power Automate: Create Site with SharePoint REST API&quot;,
      "content": &quot;This post continues a series on SharePoint site provisioning, unpacking some of the problems I’ve faced and overcome in building SharePoint site provisioning solutions. This post will look at dynamically creating SharePoint sites using Power Automate. An advantage of doing it this way is to automate different settings that can incorporate variables, as opposed to the standard interface tools for users to create new sites. Check if Site Already Exists This can be done with the SharePoint REST API if you want just a SharePoint site, or a new Group if you want all of the Group functionality (Teams, Planner, etc.). For this example, I’ll use the former. You will likely want some error checking, specifically to see if the site already exists at the requested URL. There could be a few ways to handle what to do if there is a conflict, depending on your context. Maybe you want to stop and send an email to notify the user of the issue. Maybe you want to assign a different URL and create a new site. Or you could allow it to update the existing site. For this demo, I’ll go with the last one. That makes my logic pretty simple: if site doesn’t exist, make it. I’ll use the Get Site API call to see if the site already exists. Details: Action type: Send an HTTP Request to SharePoint Site Address: the root site, or another site that you know already exists Method: GET URI: /_api/SPSiteManager/status?url=’https%3A%2F%2F@{variables(‘tenant’)}.sharepoint.com%2Fsites%2F@{variables(‘projectURL’)}’ Header 1 key: accept Header 1 value: application/json;odata.metadata=none Header 2 key: odata-version Header 2 value: 4.0 Part of what that returns is a SiteStatus value. A value of 0 means that it hasn’t been created yet. A value of 2 means it has. There are also two less common responses: 1 means it is currently being provisioned and 3 means there was an error provisioning the site. In a more intensive real situation, I would also want to deal with those less common scenarios. But for this example, I’ll stay simple and just say that if it is 0, go ahead and make the site. Here’s the condition statement: Details: First piece of equation: int(body(‘Check_if_site_already_exists’)?[‘SiteStatus’]) Boolean test: is equal to Second piece of equation: 0 Create Site We’re ready to create the site, which can also be done with REST API calls. Details: Site address: root site, or some other site you already know exists Method: POST Uri: /_api/SPSiteManager/create Header 1 key: accept Header 1 value: application/json;odata.metadata=none Header 2 key: odata-version Header 2 value: 4.0 Body: &#92;&quot;request&#92;&quot;: { &#92;&quot;Url&#92;&quot;:&#92;&quot;@{variables(&#39;siteURL&#39;)}&#92;&quot;, &#92;&quot;Title&#92;&quot;:&#92;&quot;@{variables(&#39;projectName&#39;)}&#92;&quot;, &#92;&quot;Description&#92;&quot;:&#92;&quot;Project site for: @{variables(&#39;projectName&#39;)}&#92;&quot;, &#92;&quot;Lcid&#92;&quot;:1033, &#92;&quot;WebTemplate&#92;&quot;:&#92;&quot;STS#3&#92;&quot; } You can customize details from here, like adding error checking for the other site statuses, added an owner to the site you create, changing the template of the site, and so on. The next post in this series will work on adding temporary site scripts and designs for the new site.&quot;
    },{
      "url": &quot;/2021/sharepoint-content-types/&quot;,
      "title": &quot;SharePoint: Content Types&quot;,
      "content": &quot;Content types are one of those features of SharePoint where you can be using SharePoint for years and never notice is there. They often aren’t an absolute necessity. But they do make some things much easier. The average user may not need to be familiar with them, but administrators should be. There are two major advantages to using content types: a standardized set of columns (metadata) and file templates. Columns Columns in SharePoint are metadata about a file or about a list item. These can be of a variety of data types including yes/no fields, dropdown option lists, short text, long text, and more. The easy way to remember why they’re called columns is to imagine them in the most common view, with one row in a table for each item and a column in the table for each piece of data about that item. Once they’re all set up, columns can make a lot of things easier including seeing relevant data at a glance, searching for, sorting, and filtering the content. Now suppose you have 20 custom SharePoint lists that are fundamentally the same idea. Perhaps they had to be split to exist on different sites with different permissions, or perhaps they had to be split to keep each list under the 5000 limit threshold where viewing a list gets limited. They should all have the same 30 custom columns of data that needs to be tracked. Without content types, the way to do this is to repeat the process creating a column on every list separately. That’s already annoying. It gets even more annoying if you have to make a change – perhaps you realize a certain column should be long text instead of short text – and go through all 20 lists again. What if you make a mistake and name the column differently on one of the 20 lists but there’s already data in it by the time you notice? That might throw off other pieces like Power Automate processes. With content types, it gets much easier and much less error-prone. You define the columns on the content type in one place, either specific to one site or globally for all sites, and add the content types to the list/library. If globally, it can take a couple of hours to propagate to all of the lists, but that’s it. That’s the whole process. Most of the time the instinct with a new list or library is to add the columns directly to that list or library and completely ignore content types. That’s fine if you know if you’re never going to need the same column set on another list. But if there’s any chance you will need it again, save yourself a lot of hassle and make it a content type in the first place. Templates The other big selling point of content types is specific to file libraries (not lists). Suppose you have a company template that all of your press release statements start from (company letterhead, etc.). Without content types with templates, your process may look something like this: Keep the template starting point file in a SharePoint site where only those who need it can see it. Every time somebody needs it, they go find that file and make a copy of it into the new location where they need a new one. They are then free to edit the copied version. At best this is slow, with time needed to copy the template every time. At worst, it introduces other problems: can you adequately control who can edit the template file? what if somebody accidentally starts editing the template instead of making a new copy, an easy mistake to make? With a content type, this gets much cleaner. Add the template file to the content type. Then when somebody tries to make a new item of that content type, it will start with the templated file instead of a blank document. They can edit the file from there. It’s fast and intuitive, access to editing the template is tightly controlled, and there’s no risk of somebody forgetting to copy before they dive in to edit. SharePoint Syntex I’ll add a third point. SharePoint Syntex is a relatively new product offering that uses machine learning to read data from files and apply that to the columns. It’s a valuable way to help automate applying metadata to files, which is something that people tend to not enjoy doing manually. I have not extensively tested out SharePoint Syntex, but it is my understanding that the machine learning models are applied on specific content types. If you want to use this tool you’ll end up using content types whether you realize it or not.&quot;
    },{
      "url": &quot;/2021/device-configuration-policies/&quot;,
      "title": &quot;Microsoft Endpoint Manager: Device Configuration Policies&quot;,
      "content": &quot;You’ve got your devices enrolled in Endpoint Manager. Now what? This opens up lots of tools including configuration policies. Configuration policies allow for quickly rolling out the desired configuration to the device, without the user having to manually set it up. This can include a lot of different settings and vary by the operating system of the device. Some of the more interesting tools for Windows 10 includes: Email, contacts, calendar, and tasks (add the account corresponding to the user logged in) Restrict apps to only those from the Store Block access to system settings Password requirements Start menu Battery saver settings Edition upgrade (from Pro to Enterprise) Domain join VPN Wi-Fi Windows Hello for Business VPN particularly stands out to me in the age of COVID-19 and lots of people working from home. Setting up a VPN is not something that is intuitive to most users. With a device configuration profile, they don’t have to figure that out themselves or lose time letting an IT staff member remote access to set it up. That’s not a comprehensive list. Ultimately, you can do a lot which helps save the user time configuring themselves and/or helps enforce some security blocking them from features they don’t need.&quot;
    },{
      "url": &quot;/2021/power-automate-temporary-site-scripts-and-designs/&quot;,
      "title": &quot;Power Automate: Temporary Site Scripts and Designs&quot;,
      "content": &quot;This post continues a series on SharePoint site provisioning, unpacking some of the problems I’ve faced and overcome in building SharePoint site provisioning solutions. In the last post in this series, I created a SharePoint site programmatically. Suppose you want to update site scripts or site designs onto that new site. The advantage of doing this is that it can be fully automated based on another causal event setting it off, like filling out a Power App or creating an item in a SharePoint list, and incorporate variables. My simple example will use a variable of a link that will be added to the navigation of this new site. Write the Temporary Script Prepare your SharePoint site script, but leave a gap for the part where you need to put the variable. I recommend doing this in a separate code editor and saving the file. That makes it easier to roll back any changes and to detect syntax errors like a missing ]. Editing code directly in Power Automate can get messy. Once your script is ready, add that to a new Initialize Variable action in Power Automate, setting up a new string variable for the entire script. Within that script, you can insert any other variables you need to. Here’s my simple script: Details: Action: Initialize Variable Name: siteScript Type: String Value: { &#92;&quot;$schema&#92;&quot;: &#92;&quot;https://developer.microsoft.com/json-schemas/sp/site-design-script-actions.schema.json&#92;&quot;, &#92;&quot;actions&#92;&quot;: [ { &#92;&quot;verb&#92;&quot;: &#92;&quot;addNavLink&#92;&quot;, &#92;&quot;displayName&#92;&quot;: &#92;&quot;External Site&#92;&quot;, &#92;&quot;url&#92;&quot;: &#92;&quot;@{variables(&#39;linkURL&#39;)}&#92;&quot;, &#92;&quot;isWebRelative&#92;&quot;: false } ], &#92;&quot;version&#92;&quot;: 1 } Register the Temporary Script Now that you’ve got the script written, complete with variable, you can use the SharePoint REST API action to register that site script on your SharePoint instance. Details: Action: Send an HTTP Request to SharePoint Site Address: root site, or some other site on the tenant you know already exists Method: POST Uri: /_api/Microsoft.Sharepoint.Utilities.WebTemplateExtensions.SiteScriptUtility.CreateSiteScript(Title=’Temporary script for adding link’) Header 1 key: accept Header 1 value: application/json;odata.metadata=minimal Header 2 key: Content-Type Header 2 value: application/json;charset=utf-8 Header 3 key: odata-version Header 3 value: 4.0 I also created a variable to capture the ID of the site script that was just created. You could do without this step, instead referencing back to the body of the response every time you need it, but it is easier to keep track of with a variable. Details: Action: Initialize Variable Name: siteScriptID Type: String Value: body(‘Create_temp_site_script’)?[‘Id’] Create the Temporary Design Once you have a site script, you can now do essentially the same thing with registering a temporary site design that uses that script. Details: Action: Send an HTTP Request to SharePoint Site Address: root site, or another site on the tenant you know exists Uri: /_api/Microsoft.SharePoint.Utilities.WebTemplateExtensions.SiteScriptUtility.CreateSiteDesign Headers: [same as registering the site script above] Body: { &#92;&quot;info&#92;&quot;:{ &#92;&quot;Title&#92;&quot;:&#92;&quot;Temporary site design for @{variables(&#39;siteName&#39;)}&#92;&quot;, &#92;&quot;Description&#92;&quot;:&#92;&quot;Applies navigation links&#92;&quot;, &#92;&quot;SiteScriptIds&#92;&quot;:[&#92;&quot;@{variables(&#39;siteScriptID&#39;)}&#92;&quot;], &#92;&quot;WebTemplate&#92;&quot;:&#92;&quot;68&#92;&quot; } } Note that WebTemplate 68 is a communication site. 64 would be a team site. As with site scripts, I want to grab the ID into a variable to make it a bit easier to reference in future actions. Details: Action: Initialize Variable Name: siteDesignID Type: String Value: body(‘Create_temp_site_design’)?[‘Id’] Delay You may want a bit of a delay before applying the site design. This is to ensure that the site does already exist before you try to apply the design. Rather than guessing a specific amount of time for the site to be prepared (5 minutes has always been sufficient in my tests), you could also use a test to see if the site exists yet with a REST API call like the one above. In that case you could put that within a loop: Enter loop only if site does not exist. In loop, delay one minute, then check if site exists yet. That could result in small gains on average compared to always delaying 5 minutes. For this demo I stuck with simply delaying 5 minutes. You also may need to delay much longer if your site scripts rely on global content types. Those content types won’t be available on the site immediately. They could be up to 90 minutes in my tests. If you are using content types, you may want to do only a 3 or 5 minute delay now, apply one design that does most of what you want, then delay another 90 minutes before applying another design that handles those components. That way the site is at least partially usable in that 90 minute wait. Apply the site design Yet another REST API call will allow you to apply the site design that is now registered. Details: Action: Send an HTTP Request to SharePoint Site Address: the new project site’s URL Uri: /_api/Microsoft.Sharepoint.Utilities.WebTemplateExtensions.SiteScriptUtility.ApplySiteDesign Headers: [same as registering the site script above] Body: { &#92;&quot;siteDesignId&#92;&quot;:&#92;&quot;@{variables(&#39;siteDesignID&#39;)}&#92;&quot;, &#92;&quot;webUrl&#92;&quot;:&#92;&quot;@{variables(&#39;siteURL&#39;)}&#92;&quot; } Clean Up You don’t want to leave a script and design every time this runs, so you’ll want the Flow to delete those temporary scripts. This can also be done with SharePoint REST calls. First remove the design, then the script, since the design depends on the script. Details: Action: Send an HTTP Request to SharePoint Site Address: the root site, or another site on tenant you know exists Uri: /_api/Microsoft.Sharepoint.Utilities.WebTemplateExtensions.SiteScriptUtility.DeleteSiteDesign Headers: [same as registering the site script above] Body: {&#92;&quot;id&#92;&quot;:&#92;&quot;@{variables(&#39;siteDesignID&#39;)}&#92;&quot;} Details: Action: Send an HTTP Request to SharePoint Site Address: the root site, or another site on tenant you know exists Uri: /_api/Microsoft.Sharepoint.Utilities.WebTemplateExtensions.SiteScriptUtility.DeleteSiteScript Headers: [same as registering the site script above] Body: {&#92;&quot;id&#92;&quot;:&#92;&quot;@{variables(&#39;siteScriptID&#39;)}&#92;&quot;} That’s it: you now have a Flow that generates temporary site scripts and designs, applies them, and then cleans up afterward.&quot;
    },{
      "url": &quot;/2021/device-compliance-policies/&quot;,
      "title": &quot;Microsoft Endpoint Manager: Device Compliance Policies&quot;,
      "content": &quot;Once you’ve got devices enrolled in Microsoft Endpoint Manager, one of the very useful things you can apply are compliance policies. These provide you a way to monitor and enforce restrictions on devices which are not following the proper practices that you want in your organization.These compliance policies can be set up for devices of multiple operating systems: Android iOS macOS Windows 10 and later Windows 8.1 and later As is the case elsewhere in Endpoint Manager, Chromebook is the noticeable omission. The exact options from there will vary depending on the platform, but they break down into three decisions: how do you define non-compliance (compliance settings), what happens if it’s not compliant (actions for noncompliance), and who is bound by this compliance policy (assignments). Compliance Settings This tab defines what it takes to be considered compliant vs non-compliant. Windows 10 has the most options and I won’t list them all, but some that stand out include: Require Bitlocker: encrypted hard drive Minimum and maximum OS version: this is a great way to make sure that updates are happening, e.g. you may want to make sure nobody is on a version older than a year Firewall Antivirus Antispyware Microsoft Defender Antimalware, including Real-time protection Microsoft Defender for Endpoint: require that the device is under a certain score if it is enrolled in Defender for Endpoint In general, I suggest that you at least require Bitlocker, firewall, antivirus, antispyware, and Defender antimalware with real-time protection. There are a lot of variables, e.g. if you are using a different anti-virus solution instead, but I think that’s a good baseline. Actions for Noncompliance The next question is what you want to happen if the device violates the compliance rules. There are a few options: Mark as noncompliant: you’ll always want this, but you get an option of how long before that happens. That decision will depend largely on other settings. For example, if this is the only consequence, it can be a good way to monitor violations to mark noncompliant immediately. But if you’re pairing it with something like conditional access, you don’t want to end up immediately locking out a user from company resources because they were a little slow updating Windows. In that case, you’re likely better off sending an email warning right away and not marking noncompliant for a couple of weeks. Send email to end user: this is a helpful tool if the factor not in compliance is something that the end user can easily fix themselves. You can even define a message template to provide instructions on how to fix it. Retire the noncompliant device: this will remove company information from the device. This should be the last resort only if the user did not get the device back into compliance quickly enough. Assignments Finally, there are options for who this compliance policy applies to: all users or specific groups. You may want to apply a more lenient policy to everybody, then have more strict policies for users like senior management who have access to the most sensitive information.&quot;
    },{
      "url": &quot;/2021/power-automate-the-first-function/&quot;,
      "title": &quot;Power Automate: The First Function&quot;,
      "content": &quot;A scenario I’ve encountered several times in Power Automate is needing to get just one item from a data source, such as a SharePoint list, based on a specific column such as Title matching what I am looking for. Power Automate only has a function to get all the SharePoint items that match the criteria, unless you already know the specific ID you are looking for. So you end up with an array returned, even if there’s only one item in it. That then creates a bit of nuisance when you want to access that one item, since Power Automate will go ahead and put that within a for each loop structure. Knowing there’s only one item, it is a negligible difference in Flow speed, but it is suboptimal code looking at a for each loop that isn’t really looping. How do you get around that? The Microsoft Tech Community recently answered that with the first() function. The full details are in the link below: Avoid Unnecessary Looping (Apply to each) in Power Automate – Microsoft Tech Community&quot;
    },{
      "url": &quot;/2021/htwoo-sharepoint-framework/&quot;,
      "title": &quot;HTwoO in SharePoint Framework&quot;,
      "content": &quot;I recently learned about a new framework for a fully HTML and CSS implementation of Microsoft’s Fluent Design system. This can make your life a lot easier when designing web apps that stick to that design. I haven’t tried it out yet – I’m hoping to get a full SharePoint Framework experiment together soon – but it looks very promising. Here are some key links to help: Introduction to HTwoO Adding HTwoO to a React SharePoint Framework web part Microsoft Cloud Show podcast episode&quot;
    },{
      "url": &quot;/2021/yammer/&quot;,
      "title": &quot;The Time I Recommended Yammer&quot;,
      "content": &quot;Yammer is often the forgotten piece of Microsoft 365. Some people would even argue that Microsoft has largely forgotten about it. It’s an enterprise social network. It feels a lot like using Facebook, but it’s restricted to your organization. Several years ago I was tasked with planning an intranet based on SharePoint. A significant portion of the goal was to further enhance communication across the organization, including more casual conversations and not just strictly work conversations. The assumption going in was that SharePoint was the solution for that as well as sharing resource files. Note that this was not long before modern SharePoint began. It was mostly ugly. The conversation web part sucked. It was quite functional for managing files, but was not a pleasant experience. I realized that no, SharePoint is not the solution for everything. But I discovered another part of Microsoft 365: Yammer. Yammer was a perfect solution for what we needed in terms of easy communication. I pitched it to the ultimate decision-maker and at first she was a little tentative to commit to a second app, but she came around to realizing my point. The graphic I’m including below is a newer depiction from Microsoft that helps capture the same idea: Teams is great for the inner loop: the conversations you have with your core teams regularly. Outlook is the ubiquitous targeted communication tool: everyone has email and it’s a good failsafe especially for communication outside your organization. SharePoint is the file system underneath it all, which you may or may not browse directly. And Yammer is the outer loop, great for all-staff and more casual communication. So, would I still recommend Yammer? In some circumstances, yes. Some things have changed since then, like the introduction and centrality of Teams to take up that “inner loop.” But Yammer, while not the most complete tool in the world, is still good for the same thing I recommended it for all those years ago: the “outer loop” user-friendly open conversations across a wide organization.&quot;
    },{
      "url": &quot;/2021/microsoft-cloud-app-security/&quot;,
      "title": &quot;Microsoft Cloud App Security&quot;,
      "content": &quot;If you’re an IT admin, do you know what apps users are putting on devices alongside company data? Do you know all the apps that they are directly putting company data into, thinking it helps solve a problem for them? This is the problem of “Shadow IT.” If it’s a personal device, it’s even worse, as they might be installing all kinds of insecure apps without IT approval and it wouldn’t take much to make a mistake like copying and pasting company data or uploading a file into the wrong app. The first lesson of this problem is that it is very important that IT provides the apps that every user needs. Users shouldn’t have to decide for themselves what to use and whether it is secure enough. If they’re forced to make those decisions instead of IT, there’s a good chance they won’t consistently make good ones. The Microsoft Cloud App Security dashboard helps with this problem. It empowers admins to see the apps being installed on corporate devices. It also allows you to set policies around those apps to help contain data. Discover Step one is discovering what apps are on the devices. The Discover -&amp;gt; Create snapshot report will allow you to create a report based on the logs from services like corporate firewalls. The Cloud app catalog will show all of the apps in Microsoft’s directory. There are thousands. They all come with detailed risk scores providing information on the app, including its security, and legal. For example, SugarCRM scores an 8 overall, with a 7 in general, 10 in security, 4 in compliance, and 10 in legal. That’s pretty good, but depending on the needs of your organization (like HIPAA compliance), there might be some issues to look into there. This is helpful information to decide whether to approve or deny an app for your users. Investigate The Investigate section allows for checking in on reports for users, files, and activity across connected apps. You can set up connected apps in this section as well, so that apps like Office 365 report back to CAS.# PPoliciesThe policies section comes with several great policies by default. Familiarize yourself with these defaults before starting to create a new one, as there might already be something close to what you want that just needs a bit of tweaking. There’s a lot here, so I’ll run down the policy types quickly: Access: these require first having conditional access policies configured. Then you can enforce access requirements logging in to other apps. Activity: these monitor user activity within their apps, like logging in from a risky IP address or mass downloads (which could suggest something like a disgruntled employee grabbing as much as they can on the way out). App discovery: these policies help you respond when a new app is discovered to be in use in the organization. Cloud discovery anomaly detection: these help detect unusual behaviour like suddenly uploading a lot more data than in a normal workday. File: these help with detecting potential issues at a file level, such as using a DLP engine to notice when sensitive data is shared to a different app, or files shared to other domains. Session: these provide real-time monitoring and control over what users are doing in cloud apps, while they’re signed in to that session. They also require conditional access policies. With each policy, you can set the severity level and whether you want to be alerted by email, text message, or start off a Flow in Power Automate to respond in other ways.&quot;
    },{
      "url": &quot;/2021/overriding-sharepoint-sites-home-page/&quot;,
      "title": &quot;SharePoint: Overriding a Site&#39;s Home Page&quot;,
      "content": &quot;This post continues a series on SharePoint site provisioning, unpacking some of the problems I’ve faced and overcome in building SharePoint site provisioning solutions. At this point in the series, we’ve now created new SharePoint sites and applied temporary site scripts and site designs. There is one big feature missing from site scripts and site designs, though: templating the home page. You can’t simply say that a project site design should always contain a home page with these specific web parts. This may not be true forever – Microsoft is steadily improving site templating – but as of working on this project a few months ago, it required a bit of a workaround. The Idea Here’s the basic idea that we’ll do to get around this: Design your homepage on some other SharePoint site, with permissions locked down so that nobody sees it or can edit it who shouldn’t. Within the provisioning Flow, after the new site is created, copy the page from the template site to the new site, overriding the name of “Home” so that it takes over the homepage. The Pro and Con The nicest thing about this approach is that it makes it very easy to update the home page. Anybody with the permission to do so can go to that template site’s home page and edit it. That’s it. There is no PowerShell or JSON to update a site script or design. The biggest negative is the other side of that coin: it requires that nobody delete or rename that homepage file on that template site and there’s no warning if somebody did try to do that. You can recover within 93 days if somebody does delete it and you notice by way of the Flow failing, but it’s still not ideal that it’s even possible for a user to do that without knowing the consequences. The Copy Action If you don’t already have a template site with some locked down permissions, create it and then design the homepage. This homepage can use relative web parts, like the documents library of that site. When that’s ready, it only takes a “Copy file” SharePoint action in the Flow to handle the copying. This can be a bit smoother than some previous actions which required the HTTP REST API calls. Those needed more flexibility using variables defined in the Flow. This one is copying from the same location every time, so it can stay simpler. Details: Current site address: select your template site File to copy: /SitePages/Home.aspx Destination site address: the SiteURL variable used previously, for the full URL of the new site Destination folder: /SitePages If another file is already there: Replace Note that this action depends on the new site already being fully created. If you followed my previous post, there was a delay there for the purpose of applying the site design. If you don’t already have a delay between the request to create the site and copying this file, you’ll need to add one.&quot;
    },{
      "url": &quot;/2021/considerations-migrating-imap-to-m365/&quot;,
      "title": &quot;Considerations Migrating IMAP to M365&quot;,
      "content": &quot;Email migrations are one of those tasks you won’t have to do often, but you’ll really want to make sure you’re doing it correctly when it does need to happen. As much as collaboration tools like Microsoft Teams and Slack have taken over a lot of communication, email is still a central component of many organizations. There are lots of permutations of migrating email from one service to another. In this post I’ll focus on one I’ve done a few times and is more likely with smaller organizations: from a web server to Microsoft 365. These smaller organizations often set up their email on the same server as their website many years ago. Those email services are usually quite bad – slow, low storage limits, bad spam filtering, emails might sync across devices but contacts and calendar don’t, etc. – but maybe they were good enough for a long time and it’s a pain to switch. The need for a migration might also come up when the organization wants to move their website hosting and suddenly realizes the flaw in having their email hosting connected to the same account. I’m not going to break down the detailed steps of a migration. These are well documented by Microsoft. Instead, I’m going to mention a few potential complications to be aware of. Plan Your Time Some of the steps take time. You won’t do this all in one day. But if you plan it properly, it can be a seamless transition for the users. So plan out what your steps will look like, which might be something like this: Day one: create the new accounts in Microsoft 365. Take a look at the migration process to make sure you’ve got a good understanding. Especially if you haven’t done it before, do a trial run with just one of the email accounts. Put out the request for all the credentials you need, including the IMAP server details and the passwords of current users. Make sure to collect these securely, using tools like QuickForget. Also make sure you have any alternate emails for users in case you need to contact them while they don’t have their work email. Wait a couple of days to get all the passwords. Day three: prepare the spreadsheet with all the user names and passwords. Start the migration process. Day four: check in to see how the migration went. Ask a few users to confirm that all their content copied correctly. Notify everybody of the time that their email will change and what they need to do to access it, with some detailed instructions of how to log in to their new accounts and add them to Outlook. They can try to access their new accounts already – they exist, just might not have all the content yet. Day five: once you’ve confirmed everything is copied over well, change the DNS records to point all new email to Microsoft 365 instead of the IMAP server. This could take from minutes to a day depending on the DNS server. Do this outside of business hours if possible, so the users never notice the in-between period where some emails go to the old mailbox and some to the new. Let the users know that they should start accessing from Microsoft 365 any time now, reminding them of the instructions. Day six: confirm everybody is successfully accessing the new mailbox, providing added training or support as necessary. You don’t have to follow this exact timeline. It’s just a demonstration to point out there will be gap times where you have to wait between steps, so plan accordingly. POP and IMAP Make sure people aren’t accessing via POP. If a user is checking their email only using Outlook on a single computer with the POP protocol instead of IMAP, changes they make on their computer (like folder structures) do not sync back to the server. The migration process is server-to-server, so this means that you end up migrating the unsorted mailbox on the server instead of the nicely organized structure of the computer. That’s not good. In this case, the best solution may be to use the PST export and import in desktop Outlook to migrate this mailbox. Walk the user through exporting their mailbox as a PST. Either walk them through adding their new account in Outlook, or have them send you the file and you can do it for them. Import the PST of the old mailbox into the new mailbox in Outlook. Wait for everything to sync back to the Microsoft server, possibly hours depending on the mailbox size. It’s a slow but straightforward process, not too onerous as long as you don’t have to do it for a lot of users. Contacts and Calendar IMAP migrations handle email, including folder structures. They don’t include contacts and calendar. Users have to do that manually. If the users want to migrate those things (they may not, depending how they use them), make sure they have instructions for how to export their address book and calendar and then import to the new Microsoft mailbox. Training I’ve already mentioned this, but don’t forget training. If you spend a lot of time in Microsoft 365 like I do, it might be easy to overlook that you’ve just set up accounts for people who may have never used Microsoft services before. You’ll need to make sure they know everything they need to know. That includes things like: How to check their new email: on the web, desktop Outlook, mobile Outlook or other app Installing the desktop Office apps if they’re starting those subscriptions at the same time, which is likely Some core Microsoft 365 admin functions like adding new users, groups, and resetting passwords for a couple of their trusted admin users&quot;
    },{
      "url": &quot;/2021/movie-directory-1-data/&quot;,
      "title": &quot;Movie Directory Part 1: The Data&quot;,
      "content": &quot;I recently decided to do more experimentation with Dataverse (formerly Common Data Service) in the Power Platform. I did this by making myself an app to solve a need I have myself: a movie directory app. I wanted to be able to: Track in what formats I own a movie (Blu-ray, DVD, digital) Link to JustWatch to find where I could stream a movie Assign a movie to different lists Streamline a process for grabbing the movie cover image There are good apps out there that do these things, but I never found something to do them all in one place. More than intending to use it, though, it was mostly an excuse to try out some things in Dataverse and Power Apps. In this post I’ll describe my data structure in Dataverse. In the next post I’ll walk through a couple complicated features of the Power App itself. Then I’ll detail the Power Automate Flow that I used to help streamline grabbing movie cover images, with some limitations on how much I could do without a premium license. Here’s the table that I created, appropriately named “Movie.” Many of these fields are the defaults that come with any new table, but this is what I added: Cover: data type Image and the flag set for Primary Image. JustWatch: data type URL. Last Watched: data type Date Only. This is used to track when we last watched something, particularly something we owned, to help us easily identify when we completely forgot about a movie we own. Owned: data type Choice, where the choice options are Blu-ray, DVD, Movies Anywhere, Google Play, Microsoft, Vudu. Watchlist: data type Choice, where the choice options are whether this movie is for just me, just my wife, together, or not on any list at the moment. This is to help prevent “Netflix cheating:” watching something independently that the other was expecting to be together. Year: data type Whole Number. This is to help distinguish when multiple movies over different years have the same title. That’s it. It’s a pretty simple data structure. I did at times toy with more like a Streaming choice that allowed for specifying the services the movie is currently on, but cut that out and replaced it with the JustWatch link because it would be much more hassle than it’s worth to keep those up-to-date.&quot;
    },{
      "url": &quot;/2021/civicrm-personal-campaign-pages/&quot;,
      "title": &quot;CiviCRM: Personal Campaign Pages&quot;,
      "content": &quot;Have you ever taken part in a non-profit fundraising event where you get your own personal webpage that you can direct people to as a way for them to sponsor you directly in the event? Personally I am happy to participate in the Coldest Night of the Year every year, which does have this type of system (not in CiviCRM). In the language of CiviCRM, these are Personal Campaign Pages. While CiviCRM does offer this functionality, it has some major holes in it. Like a few areas of CiviCRM, it feels like somebody threw it together quickly and then mostly forgot about it. Consequently, I had a hard time ever recommending it as a good solution, even if a client was already heavily using CiviCRM happily. Here’s the official documentation. Here are the three biggest problems I found when trying to use this feature: Confusing to Set Up The way that CiviCRM presents options to you is confusing. Personal campaign pages can be used to recruit new participants or to gather donations. They can also be attached to either an event or a contribution page. So there are four combinations: On an event, using campaign pages to recruit new participants On an event, using campaign pages for participants to fundraise (the most common I’ve seen, like the CNOY setup) On a contribution page, using campaign pages to recruit participants (this scenario makes the least sense to me) On a contribution page, using campaign pages to fundraise Maybe having all four options is justifiable. The bigger problem is the settings don’t make it clear enough which combination you want until you try it out and realize you got it wrong. No Participant Directory The most natural thing that you want in a scenario like Coldest Night of the Year? A list of all the participants so that visitors to the site can find who they want to support, as well as things like leaderboards. This is common in these fundraising systems. The CiviCRM PCPs do not offer this. If your CiviCRM is built on top of a Drupal site (the best option), you can make something work using views to find all the PCPs of participants tied to an event. But that shouldn’t be necessary; that really should be a feature that’s part of the CiviCRM offering. We put in a lot of effort with views and custom PHP development to try to make some friendly user interfaces for participants and donors and even then it was a little rough compared to other systems out there. No Teams A lot of these types of events go most smoothly when the participants can sign up as part of a team, which adds elements of comradery and competition. The CiviCRM function does not handle this at all. There was an extension that did it, but that extension got abandoned after a couple of years, leaving one of our clients stranded without that working at all anymore and data in their database with no easy way to convert it to something useful. When it was working, it was similarly as rough as our participant directory workarounds.&quot;
    },{
      "url": &quot;/2021/microsoft-excel-sharepoint-lists/&quot;,
      "title": &quot;Microsoft Excel vs SharePoint Lists&quot;,
      "content": &quot;I’ve been asked the question before: when should I use Microsoft Excel and when should I use SharePoint lists? At first I didn’t have a great answer, but I thought about it, looked into it a bit more, and this is what I settled on: Excel is better when you need to do operations on the whole data set. SharePoint lists are better when the list items stand more-or-less as independent objects. Let’s break down a bit of why I concluded that. Excel: Operations Excel can do a lot of data crunching that SharePoint lists don’t on their own (maybe with Power BI, but let’s ignore that for now). You want to make an easy chart summarizing the data? That’s Excel. You want to sum a column? That’s Excel. You want to calculate different multipliers based on different variables, like a complex budget or quote spreadsheet? That’s Excel. SharePoint List: Editing SharePoint lists are much easier to edit if you need to change one particular item. You can open up a form, either using the default SharePoint forms or a connected Power App, to edit that particular item. You can also edit that one through a stand-alone Power App, through a Flow in Power Apps, or through the mobile Lists app. Much more than with Excel, it is designed to see a single item on its own and make edits as necessary. SharePoint List: Sorting and Filtering Excel can do some sorting and filtering, but it isn’t completely intuitive. It’s easy to accidentally end up sorting just one column instead of all the associated data in other columns, ruining the data (hit undo). Sorting and filtering is much more intuitive in SharePoint lists. You’ve likely encountered the same kinds of interface on many other websites with rows of data. Just click on the heading to sort or bring up the filters menu. You won’t ever accidentally jumble your data because every row is a distinct object. SharePoint List: Saved Views Related to sorting and filtering, SharePoint makes it easy to save different views. These views can be simple tables but with different sorts and filters applied, or could be much more complicated with list and column formatting. These views can be saved and shared with everybody else that uses the list, or could be kept private for just you. This is extremely valuable functionality.&quot;
    },{
      "url": &quot;/2021/movie-directory-2-gallery-filters/&quot;,
      "title": &quot;Movie Directory Part 2: Gallery and Filters&quot;,
      "content": &quot;In Movie Directory Part 1, I described what I was aiming to achieve with this personal project, as well as the Dataverse structure. In this post, I’ll describe at a high level the design of the Power App and the gallery view and filters. Most of it I’ll keep at a high-level, not step-by-step, as I used a lot of the out-of-the-box functionality. Screens There are four screens involved in the app: Gallery view that shows all of the movies, with the cover art behind the title, year, and last time we watched it Filters screen that allows you to specify filters that apply on the gallery view View details screen of a single movie Edit details screen of a single movie, including a new movie All Movies The majority of this screen is a Gallery component, with some minor changes to show exact details I wanted to show. The biggest change is incorporating the filters from the filters screen, which required this in the OnVisible field to handle whether there are filters set or not: Set(MoviesFilter1,If(!IsBlank(varWatchlist)&amp;&amp;varWatchlist&lt;&gt;&#92;&quot;None&#92;&quot;,Filter(Movies,Text(Watchlist)=varWatchlist),Movies)); Set(MoviesFilter2,If(!IsBlank(varOwned),Filter(MoviesFilter1,varOwned in Owned),MoviesFilter1)); Set(MoviesFilter3,If(!IsBlank(varLastWatched),Filter(MoviesFilter2,&#39;Last Watched&#39;&gt;=varLastWatched||IsBlank(&#39;Last Watched&#39;)),MoviesFilter2)); And this in the Items field to only show those with the right filters: SortByColumns( If(IsBlank(TextSearchBox1.Text),MoviesFilter3, Filter(Movies,TextSearchBox1.Text in Text(Name))), &#92;&quot;crfa4_name&#92;&quot;, If(SortDescending1, SortOrder.Descending, SortOrder.Ascending) ) The OnSelect field of the Overlay on the gallery has this, to navigate to viewing that specific movie: Navigate(ViewMovieDetails,ScreenTransition.Fade) Setting the background image was a bit simpler, putting this into the Image field of the Image component in the Gallery: ThisItem.Cover.Full Another minor tweak included adding the Filters icon, which navigates to the filters screen with this formula in the OnSelect: Navigate(MovieFilters) Filters The filters screen is nothing flashy, but functional. The watchlist field is a simple dropdown, where the items field comes from the Dataverse tables: Choices(Movies.Watchlist) Similar for the Owned Platform field: Choices(Movies.Owned) Hide Movies Watched Since is a standard date picker field, but those are hard to blank out if you had that set and then wanted to turn it off, so I had to add an accompanying button with this OnSelect value: Reset(LastWatchedFilter) The hardest part is the Save button, which needed to update the filter variables and navigate back to the gallery view, so this goes into the OnSelect for that button: Set(varWatchlist,WatchlistFilter.SelectedText.Value); Set(varOwned,OwnedFilter.Selected.Value); Set(varLastWatched,LastWatchedFilter.SelectedDate); Navigate(AllMovies) That’s it for the gallery view and filtering. The next post in this series will describe viewing and editing of a single movie.&quot;
    },{
      "url": &quot;/2021/movie-directory-3-view-edit/&quot;,
      "title": &quot;Movie Directory Part 3: View and Edit&quot;,
      "content": &quot;This continues a short series about a movie directory personal project exploring Power Apps and Dataverse. In the first two posts, I’ve laid out the data structure in Dataverse and the gallery and filters screens in the Power App. This post will tackle viewing and editing a single movie record in the app. View Screen The view screen started out as a pretty bland list of fields, but rapidly evolved into something more attractive and more functional: The text fields are straightforward. The icons are a bit more complicated. There is one data field with choices for all the different platforms that we own a movie, but I wanted to show it differently, with one icon for everywhere we owned it while hiding the icons for platforms we don’t own it. There are six icons for ownership plus a JustWatch link. I grouped the six into two rows, putting Blu-ray, DVD, and Movies Anywhere on one line and Google Play, Microsoft, and Vudu on the next. Whether each icon is visible or not is determined by this value in the Visible field: If([@Owned].&#39;Blu-ray&#39; in BrowseGallery1.Selected.Owned, true, false) Making that more complicated, though, is that it looks awkward if you have the third icon in a row without the first two, e.g. Movies Anywhere without Blu-ray or DVD. To solve this, I dynamically set the width of the icon to 0 if we don’t own in that format, then set the X value for placement of the next icon in the row to be a certain distance after the width of the previous one. Here’s the formula for the width of the Blu-ray icon: If([@Owned].&#39;Blu-ray&#39; in BrowseGallery1.Selected.Owned,142,0) DVD then gets this for an X value: If(BlurayLogo.Visible,50 + BlurayLogo.X + BlurayLogo.Width,BlurayLogo.X + BlurayLogo.Width) And finally the X value on the Movies Anywhere button: If(DVDLogo.Visible,50 + DVDLogo.X + DVDLogo.Width,DVDLogo.X + DVDLogo.Width) This does not handle dynamic placement on the Y axis. It can still be a little weird to look at to have one icon on the top row and then all three icons on the second row. Hypothetically a similar approach would work with some more complicated formulas to improve that, but I left it at this point. The other unique part of this view that adds a lot to the attractiveness of it is the cover image taking up the background. This is a straightforward image component stretched across the background, with this value for Image: BrowseGallery1.Selected.Cover.Full and a Transparency of 0.6, to make sure the text above it is still readable. Edit Screen The layout of the edit screen is quite a bit simpler. The fields are standard editable fields tied to the Dataverse column. I did add one more complicated piece, though. I was worried about having the delete icon so close to the save button, despite that being the default placement within Power Apps. So, I built a confirmation dialogue. The OnSelect value for the delete button is this: UpdateContext({showPopup:true}) This initiates a change that displays a rectangle in the middle of the screen which has the Visible property set to: showPopup There is also another larger rectangle that is solely to darken the movie screen behind it, to increase focus on the delete dialogue. This rectangle has a Fill property of: RGBA(0, 0, 0, 0.6) and the same Visible property as the other delete dialogue boxes, showPopup. Clicking on the Cancel button will reverse the action of the initial Delete press and take you back to editing the movie, thanks to this OnSelect property: UpdateContext({showPopup:false}) Pressing the Delete text will confirm the deletion, removing the movie and redirecting the user back to the gallery, with this OnSelect property: Remove(Movies,BrowseGallery1.Selected); UpdateContext({showPopup:false}); Navigate(AllMovies) What’s Left At this point, I have a data structure and a fully functioning app. What I haven’t covered yet is how to most efficiently load in 300+ movies. A final post in this series will look at that problem, through a Power Automate Flow that I wrote to import all the movie information from our previous system and help collect cover images for all these movies quickly.&quot;
    },{
      "url": &quot;/2021/movie-directory-4-import/&quot;,
      "title": &quot;Movie Directory Part 4: Import&quot;,
      "content": &quot;This continues a short series about a movie directory personal project exploring Power Apps and Dataverse. In the first three posts, I’ve laid out the data structure in Dataverse and the app itself. This post will tackle how I was able to quickly fill in the 300+ movies in our collection converting from our previous system, files on a computer arranged for use on a Plex server. This will all be done in Power Automate. The Old Structure Here’s how the old structure was, to work nicely with a Plex server: There was a folder in my OneDrive differentiating where we owned each movie. Some movies were only on disc. Others were in one or more digital stores as well. As I tried to get more and more digital copies of movies we already owned, I would move a file from the general Movies folder into the correct digital store folder. This wasn’t a complete system, though. For example, it didn’t differentiate Blu-ray vs DVD, and if I had something in both Google and Microsoft it would only be in the Google folder. Every file was named in the format “Movie Title (Year).” The year is valuable in Plex to help differentiate if there are multiple movies with the same title. The Variables To start the Flow, I declared several variables: ownedPlatform: will come from what folder it was in fileName movieName: parsed from the file name movieYear: parsed from the file name yearStartIndex: to help with the parsing imdbAPIkey: my API key for IMDB, which I will use for grabbing cover images Get the Movies First I’ll get all the movies. This is straightforward with a OneDrive “get files” action. I did find that I could only do a certain number of files at once, so I did it in chunks by having a separate “To Copy” folder, in which I put the next batch as I needed to copy them. I’ll then enter a loop of all the results, handling one movie at a time. This is a simple Apply to Each loop using the “value” from the previous step’s results. Parse Name and Year Start by grabbing the file name and putting it into the variable: To parse the fileName, I needed to find where the movie name ends and the year begins. The easy way to do that is to look for the last instance of a closing parenthesis. That will make it easy to get the movie name and year. Add to Excel I’ll now add details for the movie into Dataverse. I also had some checking to confirm that the movie wasn’t already in the Dataverse table, but I’ll skip those details. Of note here: The Name and Year are filled by the variables we parsed from the file name earlier. The Owned dropdown is determined by the folder the file was in. In my case I just changed this manually between running it against each folder. IMDB API I’ll now use the IMDB API to get the cover image for the movie, which I’ll upload to my OneDrive to make it easier to attach them to the right Dataverse object. I likely could have gotten as far as it automatically uploading the image directly into Dataverse with a premium subscription, but I didn’t have that and didn’t find a way around it in a reasonable amount of time. Part of what that returns is a URL to the cover image for the movie. I’ll now upload that file into a designated folder in OneDrive. Conclusion There were still some pieces I had to do manually, like attach the image to each movie, but even that was a much quicker process. In a different Flow, I also added some logic that guessed whether a movie was Blu-ray, DVD, or digital only based on the size of the file. That wasn’t very complicated, but I left it out of this version as I found I often needed to clean up the owned column anyway because of the number of other exceptions (a movie in both Google and Microsoft). Ultimately, I ended up not actually using this movie directory database app. The main reason was that it was too slow. It is not equivalent in performance to a native Android app. But it was a great test, achieving all the features I wanted to do, and I definitely learned a lot more about Dataverse, Power Apps, and Power Automate along the way.&quot;
    },{
      "url": &quot;/2021/google-microsoft-personal-business-accounts/&quot;,
      "title": &quot;Google vs Microsoft Personal and Business Accounts&quot;,
      "content": &quot;I recently started doing an email migration into Google Workspace. As I was creating the users, I encountered a wrinkle: one of the email addresses already had a personal Google account associated with it. Google gave me two options: Notify the user of the personal account, prompting them to allow the account to be claimed by the organization. Create a different user. There was no option to allow the email to be associated with both a personal and a business account at the same time. This is a different approach than Microsoft. Microsoft does allow you to have the same email address on both a personal account and a business account. This made me think about the pro’s and con’s of both approaches. The Problem with Google’s Approach This new situation did remind me of hitting a similar situation when I got my first Android device. I had a Google account, but it was technically a business account, created years ago when GSuite was free. I initially set up my Android with that. Then I started hitting some limitations, like not being able to use family sharing of Google Play purchases. So I was stuck with three options: Keep using the business account and lose that functionality. Use a different Gmail for Android, and consequently other services like Google Assistant. I did have one; I just didn’t use it for anything except the occasional testing of email tools. Delete my business Google account and create a new personal one, losing all the data that was previously associated with it, which wasn’t a lot but did have a few things like YouTube subscriptions and history. I ultimately went with number 3. It took longer. Yes, I did lose my YouTube history, although that might have been a blessing in disguise as YouTube’s horrible algorithm had been pushing a lot of terrible videos at me and it seems to have been better since. I got to keep logging in with my primary email. I do still have one smaller problem with it. I have a Google account that is the same email as my primary Microsoft account. My calendar is on the Microsoft account. I would love for Google Assistant to see the calendar, but it can’t read the Microsoft calendar directly and I can’t share from Microsoft to Google because that is normally possible by sending an email and in this case that would just send the invite right back to the Microsoft mailbox sending it. Being able to put an alias on a Google account like you can a Microsoft would solve it, but Google doesn’t offer that. It will be a much bigger problem if I ever move that domain to Google business email. Aliases would also make this easier. With this particular domain, that is unlikely. But if I was advising somebody else on this problem, I would suggest number 2. Just use a separate email @gmail.com that is explicitly for all things Google. Don’t try to be cute with your custom domain, or you could end up in a very bad situation. It would be nice if Google warned you about this possibility when you sign up with a different domain (maybe it does in small print I missed). That situation demonstrates the advantage of Microsoft’s approach: you can choose to keep your personal content and your business content separate even if you intentionally or accidentally used the same email address on both. Plus Microsoft offers aliases which can help you untangle them if you later need to; I would be a lot more sympathetic to Google’s approach if they offered that. The Problem with Microsoft’s Approach The problem with Microsoft’s account is essentially the reverse. You’re allowed to have the same email on both a personal and a business account. That’s great to keep things separated. Now you need to remember which account you used for that thing you’re trying to do. If you’re diligent about keeping work in the work account and personal content in the personal content, that’s not too bad. That’s not always possible and people aren’t always going to be good at it. Here’s another scenario I’ve seen: an organization built a SharePoint portal system for partners across lots of organizations. Some of those partner organizations use Microsoft 365. Some do not. To let the partners in who are not using Microsoft 365, the portal has to allow logging in with consumer Microsoft accounts. In theory, the goal should be that every partner who does have Microsoft 365 business account uses their business account, while personal accounts are the fallback for everybody else. In practice, it got a lot messier than that. Some people had business accounts, but didn’t understand they could use that, so they created a separate personal account, maybe using their same business email and maybe using a different email. Then they go to the portal and it says they don’t have access. They confirm they are logged in with their work email, so that’s strange. They give up and contact me for support. I dig into the Azure Active Directory and discover this partner used a personal Microsoft account, not a business account. So they end up needing to log out of their business email account they use for everything else and logging in with a personal Microsoft account with the same email address that they’re using for just this one thing probably by accident. That’s confusing. This is the value of Google’s approach: there is no confusing dialogue asking you whether you want to sign in with the business or personal account. There is no need to remember what services are associated with which account on the same email. Winner? For the majority of normal users in the majority of cases, Google’s scenario is better. It’s easier. Most average people just want a separate personal account from their business account using different emails and they’re not concerned about custom domains. Google’s approach enforces the simplicity that most people want. For Microsoft power users like me who can keep it straight, their approach is better. I’ve considered whether I should move my primary personal email (a custom domain that was signed up in the days of Windows Live Domain) from a consumer Microsoft account to a Microsoft 365 account. If I do decide to do that, here’s how I would: Put an outlook.com alias on the consumer account Set up the domain and DNS on the Microsoft 365 business, create the user there Delete the custom alias from the consumer account Going forward, I can use the custom domain for the work account and the alias for the personal account There is no scenario where I don’t lose massive amounts of personal data if I ever decided to put my custom domain in Google Workspace. What Microsoft doesn’t offer is transferring an alias from one account to another. I have a better outlook.com alias on a different Microsoft account that I would love to switch to my main account, if I gave up the custom domain alias. I would pay extra for the ability to merge them / migrate the alias. As a few accounts with tech giants own more and more of our lives, I think it’s going to be more and more important that they offer a way to solve problems like this. The real ideal to me is probably to restrict it like Google, but have aliases like Microsoft. That’s the best of both worlds to me: clear separation of accounts while having the flexibility if you need to make a change.&quot;
    },{
      "url": &quot;/2021/web-hosting-distinctions/&quot;,
      "title": &quot;Web Hosting Distinctions&quot;,
      "content": &quot;There’s a confusing thing that has come up with several clients who don’t know all the intricacies of website work (completely understandably; that’s why they pay me). When we talk about hosting services around website work, there are a few different things we could be referring to: the domain, the DNS, the website content, and email. To make it more confusing, sometimes all of them are with the same provider and other times they’ve divided up in different combinations. Domain The first step in the chain is registering the domain. Once you’ve got the domain registered, you own it and can do whatever you want with it. For example, I own the domain alliterationapplications.com that I bought through a particular domain registrar that I like. The important things to know about the domain registrar: Keep it renewed. You’ll have to pay a recurring fee to keep it or you risk everything associated with it going offline and then potentially somebody else swooping in and stealing it. Depending on the registrar, you can often buy multiple years at a time. Usually there is an option to auto-renew when it is ready to expire, but even that does require some attention that your payment details stay valid (i.e. credit cards expire every few years). WHOIS privacy is a great idea, especially if you’re not a big company. This hides your contact details from the public. Set the nameservers to point to the DNS provider. This may be in the same place as the domain registrar or may be elsewhere, like where the website content is hosted. DNS The Domain Name Service specifies details about what services are associated with the domain. The big two of these are the next items for clarification: email and website content. But there could be other things as well, like the Microsoft Endpoint Manager service for device management. DNS related to email should have at least two components: MX records: these specify where email sent to the domain should be directed. This could be Microsoft, Google, a web server, or something else. TXT record: a particular type of TXT record called an SPF record can be used to help differentiate legitimate email sent from the domain vs illegitimate. When somebody receives an email that says it from your domain, their email provider looks up the SPF on the domain. It compares what the SPF record says to where the email actually came from to determine if the sending server is allowed to send on behalf of that domain, and is likely to mark it as spam if there isn’t a match. It isn’t the only way to help make sure your emails don’t go into spam, but it’s the simplest action for the biggest impact. DNS related to website content is typically a simple A record pointing to the IP address of the web server. Some DNS providers also have a FWD record type which will forward all browser traffic to a different URL, e.g. my ryanrobinson.ca domain currently forwards to my LinkedIn (although I think I will merge it together with this site at some point). You rarely need to change your DNS – only when you change those other services – and there typically is no recurring fee, with your DNS hosting most often combined with either your domain hosting or website content hosting. Website Content The content of your website, usually consisting of several files and at least one database, reside on a web server. I won’t dive into all the considerations choosing a web content host, but if you’re talking about a simple site like this one, you don’t need anything special. There will always be some recurring fee for website hosting, although it may get combined into a package with domain or DNS. If the server is well-managed by a good support team, then you won’t need to worry about much other than keeping the account active. If it is a web server where you have to manage it yourself, you will need to pay attention to details like security updates and PHP versions. Email Finally, you may or may not have email associated with your domain. This email might be hosted on a web server (the same as the website content) or could be somewhere else like Google or Microsoft. If it’s on a web server with the website, it’s probably part of the package or a small fee extra. It is very low-quality email compared to Microsoft or Google: low storage limits, no sync of contacts/calendar, bad spam filters, etc. If the email is hosted elsewhere like Google or Microsoft, you do have another recurring fee (unless you’re a non-profit getting it for free), but you get much higher quality email as well as a variety of other services like cloud storage which can provide huge productivity advantages.&quot;
    },{
      "url": &quot;/2021/microsoft-teams-hidden-features/&quot;,
      "title": &quot;Microsoft Teams: Favourite Hidden Features&quot;,
      "content": &quot;Microsoft Teams is a very big program, essentially combining together equivalent functionality to Skype, Slack, Zoom, Google Drive, VOIP phone systems, and a few more all into one… plus the ability to develop your own apps for it. New features keep coming at a rapid pace. That makes it easy to miss some of the helpful little things that Teams offers. I’ll have more posts soon about organization admin settings and policies for Teams, but here are a few of my favourite tricks that most typical users can take advantage of in their own workflows but may not be obvious. Title on Conversations Having conversations in the Teams channels is great. It is a useful tool for having quick conversations in a nice threaded interface that stay available for search later. One common problem, though, is that when there’s a bunch of conversations, it is hard to find your way back to the one that you’re looking for. One feature that really helps with this: add a title to the conversation. By default when you try to start a new conversation, you only get a little text book to enter the content. But there’s more you can do. One of those in the bottom left is a little format icon. Click on that and you’ll see a few options including bold, italics, underline, quote, code, and alignment. My favourite, though, is that it adds a place to add a title for the post. This makes it a lot easier when you’re scrolling through old conversations to see a summary of what that conversation is about, in larger font. Tasks This one is slightly less hidden. It might even be more obvious in the sidebar in your organization. But for me, when it became available, it was tucked away where it wasn’t obvious. If you use one of Planner or To-Do to manage your tasks, the Tasks app is helpful. If you use both, it is almost essential. It will combine together the tasks from all of your Planner boards and your To-Do list into one place, including tools like sorting. If your work setup is something like: Planner contains tasks for shared group projects and To-Do contains your individual tasks – basically what each are designed for – then this is currently the only way to see them all in one place. It is missing one thing, though: flagged emails don’t appear here. They do appear in To-Do and will appear alongside your tasks in Outlook, but it is missing from Tasks in Teams. So it still isn’t quite all your tasks in one place, but it’s a big step in the right direction. Task from Conversation Speaking of tasks, how common is it to be having a conversation and you realize you need to flag this as a task to take care of later, before it risks getting buried under too many other conversations? If you’ve flagged an email in Outlook, it’s the same concept. This was not originally an out-of-the-box feature, and I even made myself a Flow to handle it for me because I needed it so much. It is available now, but might be a little hidden. Scroll over the conversation so you see the reaction menu and an ellipses. Click on that ellipses to see more options. Scroll down to More Actions to see even more options. To add to confusion, you might see “Tasks” twice: one with the To-Do logo and one with the Tasks in Teams logo. If you want to make a task in To-Do, feel free to use either one. If you want to put it into Planner, you’ll have to use the Tasks in Teams one. Team Tags What if you have some sub-groups within a Team? For example, in my previous job, we had a Team dedicated to responding to website hacks. This Team included members from the website hosting team, the custom software team, and the web services team. In the context of a website hack, you might not want to slow down and be picky about who exactly in the team should take care of a certain task. You might prefer to simply say “we need somebody from hosting to come at this part of the problem while somebody from web services comes at it from this side.” In that type of scenario, how do you go about saying the “somebody in hosting” part, without tagging every member individually? You create a tag. A tag can function as short-hand for multiple members within a group. You can @ mention a tag the same way you can @ mention an individual or the entire Team.&quot;
    },{
      "url": &quot;/2021/microsoft-teams-app-policies/&quot;,
      "title": &quot;Microsoft Teams: App Policies&quot;,
      "content": &quot;Suppose you want to lock the sidebar in Teams for some users to ensure that they don’t have different experiences, which can help with providing support and documentation as well as make sure nobody accidentally loses their easy access to the app they want to use. But you don’t want to lock all users; there are some like your IT department that might need some freedom to use their Teams differently. This can be done with an app setup policy in the Microsoft Teams admin centre. For this example, I’ll set the taskbar to contain: Activity Chat Teams Calendar Files I’ve removed the Calls app from the default, since I don’t use Teams for a phone system and the Calls app is somewhat underwhelming without that. I’ll also default install Tasks by Planner and To Do, but not force it on the taskbar. Edit the Global Policy Start out in the Teams Admin Centre, at https://admin.teams.microsoft.com. In the navigation, go to Teams Apps -&amp;gt; Setup Policies. Click on the Global (Org-wide default) policy. Change a few settings here: Flip the toggle for “Allow user pinning” off. Under “Installed apps,” click on “Add apps,” search for Tasks by Planner and To Do, and add Under “Pinned apps,” select beside Calls and then click the Remove button Click Save when done. When a user signs in, they’ll now have this as their default app bar: Furthermore, if they try to pin any other apps to their sidebar, they will not see the “pin” option anymore. We now have a default locked sidebar for all users. Create a Custom Policy The next step is that we want there to be an exception for one user. Back on the setup policies screen in the Teams Admin Centre, I’ll add a new policy. On the screen to edit the policy. This time I’ll: Set the toggle for “allow user pinning” to On Leave everything else the same as the default Click Save when done. Apply the Custom Policy Now that the policy is created, you need to assign it to a user. Select the policy, then click the Manage user button. It will pop out a sidebar with a people search box to look up your users that you want to apply the policy to. Select Add for that user. Once you’ve added everybody, click Apply. Sidebar for adding users to a policy That’s it. You’ve successfully modified the default policy a bit more strict but then allowed an exception for a smaller group of users.&quot;
    },{
      "url": &quot;/2021/microsoft-teams-network-tests/&quot;,
      "title": &quot;Microsoft Teams: Network Tests&quot;,
      "content": &quot;If you’re going to deploy Microsoft Teams in your business network, you’ll need to confirm first that your network can handle it. That means fast enough bandwidth speeds to handle everything you want all of your employees to be able to do, as well as having all the ports open for the necessary traffic to get through. If you’re going to need hundreds of users on the same network sharing their screens alongside their video and audio in meetings of 50 people, that will eat up a lot of network resources. Microsoft has offered a few tools to help with this preparation. Required Bandwidth Here’s the guidance from Microsoft for speeds required to use Teams well: Peer-to-peer audio: 30 Kbps (approximately dial-up Internet) Peer-to-peer audio + screen-sharing: 130 Kbps Peer-to-peer 360p video calls: 500 Kbps Peer-to-peer 720p HD video calls: 1.2 Mbps Peer-to-peer 1080p HD video calls: 1.5 Mbps Group video calls: 500 Kbps / 1 Mbps HD group video calls on a 1080p screen: 1 Mbps / 2 Mbps Most home high-speed Internet living in a city will be fast enough to cover a few people on a call at once pretty comfortably. But that might get a bit more complicated when you’re dealing with large numbers of people in a single office. Network Planner and Network Testing Companion Network Planner is a tool in the Teams Admin Centre that helps you determine just how fast of network you need to support all your users. You enter details about how much you think you’ll need to use Teams and it will calculate for you what bandwidth you need. Start out by selecting Network Planner from the menu. First, check the personas. These are the different types of users and what you expect they will need. Check the default ones to see if they meet your needs, or add more custom ones if the defaults don’t. Then select to add a new network plan and name the plan. Then select that plan, and specify all of the locations this plan includes (all your offices / remote users). Now you’re ready to run a report to see the results. Click to add a new report. You’ll be asked how many users you have of each persona. After you generate the report, it will tell you exactly what speeds you need for that many users to carry out the needed tasks. A related tool is the Network Testing Companion. This is an app that can be run on your PC to test out your network’s configuration and tell you things like what ports need to be opened. QoS Quality of Service configuration is a tool that allows you to prioritize certain traffic if you’re pushing the limits of your network. Generally speaking, if you’re on a video call with audio, it is more important that the audio gets through than the video. QoS can help make sure the audio gets through even if the video doesn’t.&quot;
    },{
      "url": &quot;/2021/microsoft-teams-lifecycle/&quot;,
      "title": &quot;Microsoft Teams: Lifecycle&quot;,
      "content": &quot;If you’ve been paying attention to Microsoft 365 products in the last 5 years or so, you’ve likely noticed that things have moved toward a much flatter architecture where users have more freedom to set up their own Teams / SharePoint sites, etc. In many ways this is great, but it does carry some risks of sprawl caused by users casually creating data structures and then forgetting about them. Fortunately, Microsoft does offer some mechanisms in the Teams lifecycle to help with this. Templates First up, when you’re creating a Team, a helpful tool to create them properly is templates. Some are available by default plus you can create your own. These help you establish if you have common needs for a Team, e.g. if you have a Team for every website project you can start that Team with a Web Project template that provides the exact channels and apps you need on website projects. This doesn’t stop the sprawl of creating too many groups, but it does help ensure that when groups are created, they are created properly and consistently. Team Creation Policies By default every user in the organization can create Groups and Teams. That can be restricted, though. Especially in larger organizations, you may want to restrict to only a select few others outside the IT team who can create Teams. This can be done by creating a security group in Azure AD and granting them and only them the permission to create groups. Expiration Policies One helpful tool to follow up on any groups that may have been created and never used is the expiration policy. The policy checks to make sure the group is still being used every x number of days. If it is, it will automatically renew. If it is not, it will email the group owner asking them if the group needs to be renewed or not. If the owner does not renew the group, it will be deleted. Naming Policies If you have a lot of teams, you want to name them consistently. Group naming policies can be used to add prefixes or suffixes to group names. For the example of project teams, maybe you want every team to start or end with the word Project. Archive, Restore, Delete Archive is another option for a team that mostly shuts down the Team, but keeps all the content available in a read-only format. It’s a good way to say that we’re done with this Team, but we might still need to reference it in the future, or keep it for legal compliance reasons. If a team is archived, it can be restored back to the fully active mode. Deleting is the permanent action. You will have 30 days that an admin can recover the group, but otherwise, that’s it. In general, you probably should archive groups that had significant content in them, while delete is only used for cleaning up accidental creations or obsolete groups where all the relevant content has been moved elsewhere.&quot;
    },{
      "url": &quot;/2021/guest-access-teams/&quot;,
      "title": &quot;Microsoft Teams: Guest Access&quot;,
      "content": &quot;Microsoft Teams is not just for users in your organization. You can also use it to collaborate with people from other organizations or no organization at all. Here are a few things to keep in mind: Chat: External and Guest The “chat” app in Microsoft Teams allows you to message users one-to-one or in small groups. This includes users in your organizations, but (if the organization allows it) also includes being able to message users with Teams in other organizations, in Skype for Business for other organizations, and in Skype consumer. It’s the Teams users in other organizations that have one of the more confusing issues with Teams: external vs guest. Guests are users in other organizations that you have invited into your organization’s Azure AD, most likely to join a Team or SharePoint site to share resources. They would be signed in to their account but on your organization’s resources. External users are users in other organizations, signed in to their usual tenant. Where it gets confusing is when somebody has become a guest in your organization. If you try to start a chat with them, you’ll see options to message them either as a guest or an external. Unfortunately, the Teams client only allows you to receive notifications from one tenant at a time. That user has to be either signed in to their tenant (which would make them external to you) or your tenant (which would make them a guest to you) at any given time, not both. You may need to have some preliminary conversations to know which one they expect you to message them on. Guest Users The bigger question is adding guests to the resources within your tenant, to use within Teams as well as possibly elsewhere. Depending on what you want guests to be able to use, you need to enable guest access in a few Microsoft 365 workloads: Azure AD to add guest users to groups SharePoint Admin Centre to allow sharing files with guests Teams Admin Centre to allow adding guests to teams Anybody with permissions to do so can add guest users to a team. The user accepting the invitation is not always intuitive if they do not already have some familiarity with Microsoft 365, so you’ll want to have some documentation ready if you’re going to do this often. Guest permissions in Teams The Teams Admin Centre has plenty of options to configure exactly what you want guests to be able to do within the categories of calling, meeting, and messaging. You may want them to be able to do virtually everything an internal user can do, or you may want to significantly limit them to certain functions. It all depends on the relationship with your partner and the nature of the work you need to share. There are also a couple smaller options that can be changed per team for whether guests can create, update, and delete channels in that team. Access reviews Azure AD access reviews are not only about guest access, but they are a tool in Azure AD for prompt group owners regularly to make sure they don’t have anybody in their groups who don’t still need to be there. Having extra users in a group are not quite as big of a deal with internal users. Internal users might be able to see it anyway and are more likely to remember to remove themselves from a group. Guest users are more likely to have been forgotten about because of a project a couple of years ago. Access reviews can be a good way to catch those.&quot;
    },{
      "url": &quot;/2021/microsoft-teams-information-barriers/&quot;,
      "title": &quot;Microsoft Teams: Information Barriers&quot;,
      "content": &quot;What if you need to partition your organization so that some members of your tenant cannot communicate with others in Teams? For example, maybe you have interns who you do want to have Teams access but not to any sensitive information. Information barriers are the solution you need. You can block interaction across the barriers so that the interns cannot see any content from anybody else or vice versa. Creating the barriers is done by PowerShell. There is not currently a web admin interface to do it. Create one organization segment for interns: New-OrganizationSegment -Name &#92;&quot;Interns&#92;&quot; -UserGroupFilter &#92;&quot;Title -contains &#39;Intern&#39;&#92;&quot; Repeat for another segment called staff: New-OrganizationSegment -Name &#92;&quot;Staff&#92;&quot; -UserGroupFilter &#92;&quot;Title -not (-contains &#39;Intern&#39;)&#92;&quot; Now that you have the two segments, create the barrier between them: New-InformationBarrierPolicy -Name &#92;&quot;Interns&#92;&quot; -AssignedSegment &#92;&quot;Interns&#92;&quot; -SegmentsBlocked &#92;&quot;Staff&#92;&quot; -State Inactive Finally, apply the information barriers: Start-InformationBarrierPoliciesApplication It could take several hours to go into effect.&quot;
    },{
      "url": &quot;/2021/microsoft-teams-phone-numbers/&quot;,
      "title": &quot;Microsoft Teams: Phone Numbers&quot;,
      "content": &quot;I personally have not used Microsoft Teams as a phone system. My employers have stuck completely with Internet-based communications, mostly to others in the same organization. But if you want to integrate your organization’s phone system into Teams and have your phone calls ring through in Teams, that’s absolutely possible. Here’s an introduction on some of the things to consider: Calling Plan or Direct Routing Your first decision in setting up a phone system in Teams is between: Calling plan: simplest solution, letting Microsoft handle everything Direct routing: allows for more complexity and keeping a previous phone provider Add Phone Numbers There are two basic options when you want to add phone numbers to your Teams deployment: add new numbers, or port existing numbers from a previous provider. You can do this from the Teams Admin Centre -&amp;gt; Voice -&amp;gt; Phone Numbers. Ordering numbers has a few types to choose from: User: a regular number for a single user Dedicated conference bridge (toll or toll-free): for conference rooms with multiple people calling in at once Call queue (toll or toll-free): used to define how to route calls between multiple agents, with features like wait music Auto attendant (toll or toll-free): attendant for callers to be able to select how they would like to be routed, e.g. select 1 for tech support, 2 for accounts, etc. Along with the type, you’ll need to specify a location – country and area code – as well as the quantity for how many you need. Emergency Addresses Every phone number has to have an emergency address tied to it, so that if a user calls emergency services they can trace it back and find them. With Teams phone systems, you can and should have one emergency address configured for each of your office locations. When you are assigning the number, you then specify at which emergency location that person is working. Dial Plans If you’ve worked in most large offices, you’ve probably encountered the use of something like “dial 9 for external numbers.” These kinds of scenarios are handled with dial plans. You can specify what digits (up to 4 digits) must be used to dial out to external numbers.&quot;
    },{
      "url": &quot;/2021/favourite-visual-studio-code-extensions/&quot;,
      "title": &quot;Visual Studio Code: Favourite Extensions&quot;,
      "content": &quot;I’ve written before about Visual Studio Code, my preferred text editor. One of the great things about VS Code is the large number of extensions that are available, while continuing to be a lightweight and efficient program even with lots of extensions installed. Here are some of my favourite VS Code extensions in my workflow: General GitLens: If, like most, you are using Git for version control, GitLens is a nice addition on top of the default Visual Studio Code capabilities for Git. It adds features like seeing who committed what line of code on what date, within the editor as you’re going, which can inform your decisions especially when examining legacy code. GitLab Workflow: If GitLab is the home for your code repositories and you make use of some of the GitLab project management features such as issues and merge requests, GitLab Workflow can be a good addition to your VS Code installation. I track much of my task list using GitLab Issues and it is very handy to be able to see them directly alongside my code, even if with only a subset of the features you would get in the web browser (e.g. you can view all the conversation but not change details like due date). TODO Tree: Many developers leave notes to themselves and their team in the comments in the form of writing TODO or FIXME. This extensions adds a sidebar pane showing you where all of those notes are across all the files in the workspace, making it easier to return to them. vscode-icons: If you’re browsing a large file system, it can get a bit visually overwhelming to see long lists of directories. This extension helps by providing icons to differentiate more easily at a glance. Prettier: Code style is important, especially in a team. Prettier helps maintain consistent style. Web Development Remote – SSH: I’ve written about this in more detail, but if you’re working remotely on some other server, this is essential. It’s a lot easier to work directly on one of those servers than to write on your computer and upload with every change. It even offers an integrated terminal so you don’t need a separate terminal app. Remote – SSH: Editing Configuration Files: In conjunction with the Remote SSH extension, this extension helps with syntax highlighting when you need to edit your SSH config files, which you will need to do setting up each site. Apache Conf: If you ever need to edit Apache configuration files, then Apache Conf will help by giving you syntax highlighting. Peacock: Peacock lets you change the colour palette of your workspace window. This isn’t just for attractive style. If you’re regularly jumping around between lots of servers – even dev vs staging vs production of the same site – then it can be really easy to accidentally change code or execute a terminal command in the wrong site. Having different colours for each one is a nice shortcut to cue your brain which one is which. WordPress Hooks Intellisense: Developing on WordPress? Install this extension to help with prompts and syntax highlighting for WordPress functions. Drupal Syntax Highlighting: Similarly, if you’re developing on Drupal, this extension will help with Drupal-specific syntax and functions. For any of these, you simply need to look up the extension name in the extension directory, available within your VS Code window. In some cases you’ll also be prompted with a recommendation based on editing a file of a certain type.&quot;
    },{
      "url": &quot;/2021/simplytest-me-drupal-project-test/&quot;,
      "title": &quot;SimplyTest.Me: Drupal Project Test&quot;,
      "content": &quot;Have you ever wanted to test out a Drupal module without actually installing it on your site? SimplyTest.Me is the answer. All you have to do is pick a module or theme, including a specific version, that you want to test and it will generate a sandbox site for you to check it out. It’s fairly comprehensive and spins up the sandbox within a few minutes. As with many similar tools, whether this is useful to you or not depends on your workflow and your team’s requirements. If you are the sole developer and you have a safe testing environment already, you’ll have a more complete test by going ahead and installing the module on your test site. That way you can see not only how the module operates but also how it interacts with the rest of your site. But if you are just checking something out to get the basic idea of a module, especially a more intensive module like Drupal Commerce, or you don’t have your own playground site, then this can be a good first step.&quot;
    },{
      "url": &quot;/2021/mysql-workbench/&quot;,
      "title": &quot;MySQL Workbench&quot;,
      "content": &quot;You know what desktop app made the largest improvement to my workflows when I discovered it? Not Visual Studio Code, as much as my writing sometimes hypes it up as my preferred text editor. There were already good text editors. No, the desktop tool that made the biggest improvement to my workflows was MySQL Workbench. If you work with MySQL databases at all, this will make your life so much easier than navigating those databases in a command line. Advantages It is much easier and more user-friendly to browse a database. You can see all the tables in a database along the side panel and clicking on one inserts it into a query, avoiding typos copying it. When you run a SELECT command and get lots of results in a terminal, it often doesn’t stay lined up very well. It’s virtually impossible to read large data sets not lined up. But in Workbench, everything stays perfectly in its columns. Along the same lines, it is a lot easier to apply sorting by a column, by clicking on the headers in the results. This can be faster than writing out the query. If you want to edit a particular record from the results you’re viewing, all you need to do is hit the Form Editor option beside the results. You only need to worry about writing UPDATE queries when making bulk edits. You still need to know enough MySQL to write many types of queries, but an added bonus is that it provides syntax checking. If you know a bit but aren’t an expert in writing MySQL queries, this can be a big help. You can have multiple sites/databases saved to quickly switch between. You can also have multiple query tabs to jump back and forth between, which can be quite helpful when investigating several tables at once. Finally, MySQL Workbench includes a variety of features such as importing from csv and exporting to csv. That pair of features can be great if you need to migrate to another system with some data cleanup in between. Download You can download MySQL Workbench for free. It will prompt you to create an account, but there is also a link to download without an account if you’d prefer.&quot;
    },{
      "url": &quot;/2021/domain-change/&quot;,
      "title": &quot;Domain Change&quot;,
      "content": &quot;This site is now ryanrobinson.technology instead of alliterationapplications.com. I felt that it was more representative of what this site has evolved to become. It is a repository of my technology knowledge and experience. It isn’t just about the rare freelance work that I do under the Alliteration Applications banner. Many of the blog posts came from personal study, personal projects, and (generalized versions of) work done within a full-time job – not from Alliteration Applications at all. With that rationale out of the way, I thought I’d break down how I changed the URL. I believe I could have done this a bit simpler by changing the URL on the same site, rather than making a copy first, but I decided to err on the side of caution where I would always have the old version if anything went wrong. Rearrange Content I won’t get into all the details, but I had to rearrange a bunch of the content to make sense with the new broader goal for the site: the title of the site, the content of the homepage, and so on. Change Domain A Record To tell the wider Internet what to do when they try to visit ryanrobinson.technology, I had to go into my DNS records for the domain. It was previously using a simple FWD record to redirect any traffic to it to alliterationapplications.com, but now I want essentially the opposite. ryanrobinson.technology needs to be hosted and alliterationapplications.com needs to redirect traffic away instead. So, I removed the FWD record and added an A record to my IP address instead. Add Domain to Server Account I’m on a shared hosting service with a cPanel. Step one was to add the extra domain ryanrobinson.technology to the same account as alliterationapplications.com. I specified that I wanted it to be a different document root than the existing site. I also took advantage of the AutoSSL feature in cPanel to generate an SSL certificate for the new domain. New Database I created a new database and new database user for the new site installation. Duplicator I used Duplicator to make a copy of the site and install it in a new folder. At this point I had a functioning site at both domains. .htaccess on Old Domain I then wanted to redirect traffic from alliterationapplications.com to ryanrobinson.technology, and I didn’t want to simply redirect everything to the homepage. I wanted specific URLs to go to the matching URL. To achieve that, I added this to the .htaccess of alliterationapplications.com ``` RewriteEngine on RewriteRule ^(.*)$ https://ryanrobinson.technology/$1 [R=301,L] Cleanup Finally, I cleaned up everything from alliterationapplications.com that I no longer needed: the database and all the files in the alliterationapplications.com document root other than the .htaccess file. That’s not truly necessary, but does keep my storage usage down.&quot;
    },{
      "url": &quot;/2021/browser-extension-stylus/&quot;,
      "title": &quot;Browser Extension: Stylus&quot;,
      "content": &quot;The stylus extension is a new addition to my browser (Edge, available for all Chromium browsers). The core idea: it allows for changing stylesheets on websites while you browse without actually changing the server. This might be because it’s a website you don’t own. Get really annoyed at the new Twitter font (I actually really like it)? Write your own stylesheet so that whenever you’re browsing you see something else. It also can be a good technique for trying out a change over a period of time without actually needing to change the stylesheet on the server. This can simplify potential conflicts like others in your team editing at the same time or being confused by your in-progress changes. How It Works You can write your stylesheet directly in the Stylus extension, including specifying which domains the change should apply to for you. Turn on the stylesheet in the extension and begin your regular testing. If you’ve worked with CSS on complex sites before (e.g. writing your own stylesheet on top of a WordPress or Drupal theme), you have probably encountered a scenario where the change you made applied it either too narrowly (missing a spot) or too broadly (changed something you didn’t mean to). Where Stylus can really be an advantage is that you can continue to browse the site as normal while working on other things. That gives you more opportunity to identify the problem before you actually save the change to the server. If there’s a problem, simply edit the CSS again and go back to browsing. Once satisfied, you can add the CSS to the site’s server for real. Do I Need It? Whether you need this extension or not depends on the nature of your workflow and your team. If you are the only person making changes on a development server, you probably may as well edit directly on the server right away instead of using Stylus for an extra test tool. But if others might also need to edit on the server, or if you want an extended period of time where you can test it without others seeing a change, then Stylus is a nice tool for the toolbox.&quot;
    },{
      "url": &quot;/2021/microsoft-365-tools-recognizing-sensitive-data/&quot;,
      "title": &quot;Microsoft 365: Tools for Recognizing Sensitive Data&quot;,
      "content": &quot;There are a handful of tools in Microsoft 365 to help you with information protection. Which one is the best option will depend on the scenario you will need to use it for. Once you’ve identified sensitive data using one of these tools, you can use it apply compliance rules like blocking sharing outside the organization or retaining for a certain number of years. Sensitive Information Types (DLP) Sensitive information types, the backbone of data loss protection (DLP), relies on pattern recognition. Many sensitive types of data such as credit cards, passport numbers, and social insurance numbers have strict formatting requirements. These formatting requirements can be used to help identify them when they appear in a document, email, Teams chat, etc. Many common sensitive information types are already defined in Microsoft 365. If there’s something missing, you can add your own custom types. Document Fingerprint Document fingerprints are valuable for documents that were created from a shared template. If you use a template document as the starting point for all invoices, and you want to implement some compliance policy on all invoices, a document fingerprint is the answer. This tool identifies the sensitivity of the document based on recurring word patterns. Keyword Dictionary A keyword dictionary is useful when you want to enact a policy on any instances of a specific set of data, where that specific set of data may change occasionally. A good example for this is employee IDs. Occasionally you’ll need to add a new employee ID or remove an old employee ID from the dictionary, but you always want to keep anything with those IDs protected. Trainable Classifier The trainable classifier is a newer tool within Microsoft 365. Unless the other types, the trainable classifier does not depend on you being able to explicitly define what is and is not the definition of the data type. Instead, it relies on machine learning. This makes it a great candidate for scenarios like resumes. Often HR will want to hold resumes of job applicants for a certain period of time but then have them automatically deleted. In the meantime they don’t want those resumes to ever be shared outside the organization. But a resume isn’t clearly defined: there are no guarantees of having the exact same words in a sequence, or templates, or strict numerical formats. Humans just know what a resume looks like because we’ve seen lots of them. Trainable classifiers will apply that same approach but with machine learning. You’ll need to provide many examples of what the data type looks like and occasionally monitor it to see if it missed anything or flagged anything it shouldn’t.&quot;
    },{
      "url": &quot;/2021/visual-studio-code-mysql-extension/&quot;,
      "title": &quot;Visual Studio Code: MySQL Extension&quot;,
      "content": &quot;In a recent post I talked about how great MySQL Workbench is. A few days later, I discovered a Visual Studio Code extension simply called MySQL. It is not as robust in some ways as Workbench (e.g. bulk importing and exporting). However, it does meet all the same core functionality such as: browse the tables and databases available to the user, with all the data kept in nice clear and easy-to-read lines write your own queries a table and get results in a nice table where you can do things like click sort by different headers edit entries directly in the table (no form editor mode like Workbench) It does not do all the more intensive functionality that Workbench does, like importing and exporting with CSV files, but that’s the kind of thing that you likely don’t have to do regularly. The big advantage that it has over Workbench is being a part of Visual Studio Code which you are already using. That means a few little things can go faster and more lightweight on your computer’s resources. The two little things that I can imagine make a meaningful difference over time are: less steps to login, since you are already SSHed to the host the ability to have a database open as a tab right beside the code, rather than jumping back and forth between Code in one window and Workbench in another Of course it’s great to also have Workbench available in your toolbox if you need more intensive functionality, but speaking for myself, I think I might start relying on this extension more often for the majority of my lightweight work.&quot;
    },{
      "url": &quot;/2021/visual-studio-code-grep/&quot;,
      "title": &quot;Visual Studio Code: Grep&quot;,
      "content": &quot;This is a quick post about a feature I discovered by accident in Visual Studio Code that I really like: I was working in a Linux machine and ran a grep in the integrated terminal to find a particular piece of code. The terminal gave me the results in the usual way, with the file names highlighted, the line in the code, and a bit of the code around what I searched for. Then I happened to scroll over the file name in the results and it showed me a tooltip offering that I could open the file by holding Ctrl and clicking on the path. So I tried it, and it worked as advertised. This is one of those tiny things that will really add up. I use grep a lot. Most of the time when I do I want to open one or more of the files found. Without this little feature, that means copying the path to the file into a new command. That’s a small amount of time, but it does add up.&quot;
    },{
      "url": &quot;/2021/dev-to-and-tealfeed/&quot;,
      "title": &quot;Dev.to and Tealfeed&quot;,
      "content": &quot;This isn’t the only place I’m posting my writing. I’m also posting to dev.to and Tealfeed. These are other knowledge sharing networks, free to use, and both with options to set your own blog as the canonical URL. Here are the early pro’s and con’s of each: Dev.to Dev.to has two major positives: It is a big and supportive community The editor is easy and accepts copying from WordPress Dev.to is a very well-known community of tech writers. It’s not just a blogging site. Even on my first post, I quickly had a positive comment offering some more helpful information building on top of what I wrote. The editor to write a post is straightforward. It even accepts copying and pasting code from WordPress with no issues, even for images. Each post has a featured image which is not restricted in dimensions, so it’s easy to use the same one used on this site. That makes it very simple to quickly copy over posts I did here to share on dev.to as well. It also has one minor negative: Dev.to is specifically for developers, as the name implies. I am only posting web development related things there, not Microsoft IT things or anything else I feel like sharing. Even on this point, there is some positive trade-off, as it means that the tags will allow you to get much more precise, such as “css” instead of “web development.” Tealfeed Tealfeed is a less well-known platform, at least in developer circles. My first impressions were not great. I created an account and logged in for the first time. The page spun for about 15 minutes before I gave up because I had other things to do. I came back a week later. This time it loaded my homepage with no issues. It may have been a one-time fluke, but some unfortunate timing. I then created my first post. I tried to do what I do with dev.to: copy the WordPress page. That did not work at all. It does not interpret HTML tags and does not ignore the WordPress specific tags. It does not support a lot of formatting like headers at all. It does support images and separators, but not as HTML tags. You have to use the editor interface. So even for those features that are supported, you have to do it all over again. It does take a featured image, but it’s restricted to certain dimensions, so if you care about that and don’t already have your image in the right dimensions, you’ll spend more time to create an alternate image copy. All told, while dev.to takes about 2 minutes to copy a post from WordPress, Tealfeed takes more like 15 to reformat everything. It’s got a few other oddities that demonstrate it simply isn’t very far allow in its development. If I use dark mode, which I always prefer, there are a handful of places in the key post creation process where it has black text on a black background. Not helpful. The one thing that does make it more interesting is that it is not limited to developers, or even technology as a whole. You can follow lots of topics like psychology and photography as well. Conclusion Dev.to is absolutely worth the effort to copy posts from here to there, at least for those that fall broadly within the realm of “developer.” There’s no debate on that one to me. Tealfeed is a bit more complicated. It’s hard to say yet whether it’s worth the effort to copy posts, especially since it takes so much longer to reformat. I’ll continue to give it a shot for at least a month or two before making a final judgement. I can pretty easily talk myself into keeping an account for the sake of following other interesting topics, though. It’s too bad they don’t have a mobile app, because I could definitely see it being an app I scroll for interesting stuff outside of work hours.&quot;
    },{
      "url": &quot;/2021/microsoft-teams-information-session/&quot;,
      "title": &quot;Microsoft Teams: Information Session&quot;,
      "content": &quot;I recently gave a presentation introducing some key concepts and answering questions around Microsoft Teams. The context was for those who already have accounts and have used it throughout Covid-19. They didn’t need the day-to-day basics like how to start a meeting, but they needed a better understanding of how all the pieces fit together and when it is best to use what. It was not a full consultation process, but I did put out a call for questions a couple weeks in advance and received several that I made sure to integrate into the presentation. I’ve edited my notes to be a bit more generic and included those below: Caveats A couple caveats before I begin getting technical: Teams is massive. One of the hard things with talking about Teams is that you can’t easily compare it to just one competitor. It’s a lot of tools rolled into one. And it’s just one part of Microsoft 365. We aren’t going to cover nearly everything that is possible in 45 minutes. I’m going to hopefully give enough of an introduction answering some key questions I’ve heard come up a few times. One of the other hard things talking about Teams is that Microsoft is kind of bad at branding. They use a lot of the same words repeatedly to refer to related but not exactly the same things. So you do get scenarios like having a Teams app on your device which has the Teams app in the sidebar which has multiple Teams in it and all of those Teams are associated with SharePoint Team sites. Don’t feel bad if somebody says a term and you don’t know what it means. Big Picture This is an older diagram that Microsoft has used in the past and I have held on to because I think it is a helpful explanation of the big picture. There are a lot of tools in Microsoft 365 that overlap, but this summarizes pretty well what the most important value is for the big communications pieces. Your inner loop is Teams. This is where you’re going to spend the bulk of your conversations with the people that you work with the most, like your department. The outer loop is Yammer. It’s an enterprise social network. Where it can be good is to have organization-wide conversations, especially more casual conversations. That’s as much as I’m going to talk about it today. Email is for targeted communications. It’s also ubiquitous. You’re going to need it sometimes, but if you’re talking to that inner loop, you might want to pause and consider whether it would work better in Teams or not before you send an email. In the middle of everything is SharePoint. SharePoint can get a lot more complicated than this but for today, you can think of SharePoint as shared file storage. Underneath it all is Microsoft 365 Groups. These provide the permission structure that spans across all those tools and some more. If you are added to the Group, you get access to all the tools defined for that group: Teams, a shared email, shared calendar, SharePoint files, Planner for task management, and more. Generally speaking, in a large organization, only IT and a few more can create Groups. That’s done intentionally so that there isn’t too much sprawl of thousands of groups and everybody using a different strategy for them. Owners of the groups usually have some control over who is in the group as well as some other settings. Exactly who can control what setting is often determined by a policy set by IT. Teams Overview With that big picture out of the way, I’m going to do a quick walk through of what you see in the Teams app. Search bar: across the top in the middle is a search bar. This can be used to search across everything in Teams, including people, files, and messages. It does not search across other aspects of Microsoft 365, such as SharePoint sites not associated with teams or associated with teams you aren’t in. There is a relatively new tool called Microsoft Search that has been slowly rolling out over a few years. That is available in some other places and gives you results from everywhere in Microsoft 365. I will quickly demo that later. The Teams search does not do that yet but probably will eventually. For now, it just gives you results from Teams. Settings: hidden in the little ellipses menu in the top right are settings. Teams gets updates very often, so I do recommend looking in here every few months because you might find something useful that’s new that would help your work. Profile: beside that is your profile photo. This is your Microsoft 365 profile. On the desktop and mobile apps, you can also add a personal Teams account. I won’t get into that, but it is also an option. Running down the main tab menu on the left, this is most often called the app sidebar or app bar. On desktop you can rearrange this menu by clicking and dragging an icon into a different position, but possibly with some limitations set by IT. If you can’t rearrange something, or you do but then it reverts back later, that’s because of IT policy. The Activity tab is essentially your notification hub. Anything that has sparked a notification across all your Teams will show up in the Activity section. If you think you missed a notification, this is a good place to look. Treat it the way you treat your Outlook inbox, as the place you check regularly for anything you may have missed. There are some options here like being able to filter your notifications against a search term, and if you right-click on a notification you can mark it as unread. The way I work at least, that can be a useful way to remind myself to come back to it later, if I still have unread notifications. Chat and Teams I will break down a bit more in a minute, but that’s a lot of the heart of the communication system. Calendar: the Calendar section is what you would expect from the name. It shows your calendar. Where it is especially useful is with meetings that happen in Teams. You can schedule meetings in here or join meetings previously scheduled from here. That also includes webinars with features like registration forms for people to sign up, and live streamed events which can be viewed by the public. Files: this is another window into your files. They can be files across your teams as well as in your individual OneDrive. Sometimes you can also add other cloud services like Google Drive and Dropbox in here, but that’s determined by organizational policy. Calls: the Calls section, in my opinion, is not that useful unless you’re using the Teams phone system. With that, your calls from regular phones would ring in Teams, which could be on a computer or a dedicated phone device. If you aren’t using Teams phone systems, this Calls section is a pretty basic dashboard of recent calls and speed dial. Then there’s the ellipses. There are plenty of other apps available to install within Teams. Teams really is a platform in its own right, not just an app. Some of these apps that can be added are from Microsoft, some from other providers, and your organization could even build your own app and deploy it just for you. Depending on your workflow, some of these other apps may be helpful to you, so it is worth taking a look at what is available. Chat vs Teams The biggest question I have heard when it comes to communication in Teams is when to use the Chat area and when to use the Teams area. I’ll do Chat first because it is a bit simpler. You can think of Chat as something like Skype or Skype for Business. You can send short text chats and have audio and video calls with one other person or a small group of people. You can even message people outside your organization if they are using Teams, Skype for Business, or consumer Skype. It appears as one long running conversation. If I start talking to somebody about one issue, then we switch to a different issue, then go back to the first issue, it’s all going to be jumbled together, so there isn’t an easy way to say “show me everything about this one issue.” The other important detail is that these conversations are visible only to the people in the conversation. The Teams section is much more extensive. If you’ve used Slack, it’s the closest comparison. It’s a lot more than just Chat. It’s more of a complete project dashboard for a group. You can be a part of multiple Teams. There is technically a limit but it is very large. Each Team can have multiple channels. Each channel has threaded conversations, which makes it a lot easier to keep track of multiple conversations happening at once, or to find them later using search. Exactly how to divide up the channels will vary by the team. In my demo channel, we essentially have a channel for each project or technical system we maintain. Each channel is set up with some tabs at the top. Tabs allow you to add different content, which could be in Microsoft 365 or could be somewhere else. That’s part of how it really becomes a project dashboard. The idea is that if somebody new were to join the project, they would be able to look at this one screen and they would see everything they need to see: past conversations about the project, task lists, documentation. None of that is available if all the conversations happen in Chat instead. To add new tabs, you just use the little + button. As with the apps in the sidebar, there are a lot available. There’s also a handful of other useful features on the Team that you don’t get with Chat. One of my favourites is that you can share an email to a channel, if the team’s policy allows it. This is great in scenarios like getting an email but you need to consult with your group before you respond, so that you can coordinate who is going to answer what. There is a button in Outlook that can do that, or you can look up the email address for a channel and forward an email to it. So to summarize: Chat is good when you need to message somebody outside your organization or somebody who you don’t have regular ongoing work with as part of a Team. But the Team should be the default for most conversations with that “inner loop” of people you work with regularly, so that you can have the full advantages of the project dashboard approach. Notifications One of the questions I got was about notifications, specifically how to know when the person you are trying to contact will be notified and when they will not. One way people get notified is if they are subscribed to a channel. Anybody can choose to subscribe to any channel they are a part of. Then they will be notified based on the settings they choose. I recommend this for anybody who is effectively a project manager to be subscribed to all conversations on the channel. Those people won’t necessarily need to respond to every conversation, but they probably should keep a high-level overview of what’s happening. If the person you are messaging isn’t subscribed, or if you don’t know whether they are subscribed, you can still notify them using an @ mention: @ mention them individually. My tiny tip with this is that after you select the name of the person to @, you can backspace and delete their last name with the @ still working. It’s just a little easier to read and a little more personal. @ mention a tag. Tags are set by the team owner. They can be used to specify a subset of the team to easily notify, especially useful when you have big teams that may not always know each other. Say somebody new joins the team and they have a question about a certain subject area, but they don’t know who specifically to ask; they could use the tag instead. @ mention the whole group or a channel. People will also receive a notification if you respond to a conversation started by them. You don’t have to @ message the person if you are replying to their conversation. And finally, they will be notified if you react (like) to a message by them. So the rule of thumb is that if you are in doubt and you need somebody or a small group of people to see it, use an @ mention. But you don’t have to when responding to their message or if you know they’re subscribed to the channel. Files I’m going to move on to the biggest set of questions I got which had to do with file management. How do I find my files? Where do I put a new file? Who has access to the file? I’ll say upfront that there is a bit of a blessing and a curse scenario here. There are a lot of entry points to get to your files. The curse of that is that it can be a bit overwhelming. The blessing is that you have a lot of choices to decide what works best with your workflow and you can ignore the rest. I will talk through some of these options, but you don’t need to be comfortable with all of them. You can use an analogy like travelling to the office. When we’re all coming to the office again, we’ll all have the same destination, but we all have different starting points – our houses – and different routes to get there. What matters is that we get to campus. What matters in Microsoft 365 is that we are working from the same files. How we access that file is not as important. This is a vast improvement over the older model where you might end up with lots of copies of the same file floating around because they’ve been emailed as attachments and now everybody has their own copy. When you email a copy as an attachment, you aren’t sharing a destination. You all have your own destinations, your own copies of the files, which can result in different content in the file as people change their own copy over time. As a general rule, do not share a copy of a collaborative file as an attachment to anybody within your organization. You should only resort to attachments if you’re sharing external or if it’s a one-way communication where you don’t need to know if they make any changes to it. What does help is a little more knowledge of the destination, the file system. This goes back to that diagram from the beginning when I said the file system underneath everything is SharePoint. One of the questions I got a few times was OneDrive vs SharePoint. OneDrive really is just a simpler version of SharePoint with less features and different default permissions. In many ways this parallels the Chat vs Teams discussion. Chat is for conversations owned by only a few participants while Teams is for bigger conversations owned by the groups with all the extra features. OneDrive is for files owned by just you. SharePoint is for files owned by a group with all the extra features. And the Teams distinction does line up with the file distinction. If I’m having a chat with somebody and we put a file in the Files tab, where do you think that file is going to be saved? It will be in the OneDrive of the person who created it, shared with the other. The file is owned by who created the file, not a group. If I put a file in the Files tab from a Teams channel, where do you think that file will go? It will go in the SharePoint site tied to the Team. How about the video recording of this session right now? I created this meeting myself, sending an invite from Outlook. So who owns this meeting? I do. The recording will end up in my OneDrive. What if we had a Team that included everybody here and I created this meeting from a channel on that Team? Then the file would be saved on the SharePoint site associated with the Team instead. The meeting is owned by the team and so is the recording. In other words, Teams is not its own file system. Teams is built on top of SharePoint. Every Team is associated with a SharePoint site. When you look at that Files tab in a Team channel, that’s really a window into a specific folder on a SharePoint site. For the most part, you can do everything you need from Teams and you often don’t need to look at SharePoint directly for these team day-to-day project files. You do have to look at SharePoint directly if you have something like an intranet portal that isn’t linked to Teams, but it’s not unusual if most of the members of a specific team never actually look at the SharePoint site for that specific team directly. It’s not Teams vs SharePoint. It’s Teams AND SharePoint. It’s Teams as one of the possible routes into the destination of SharePoint. If your channel is already that project dashboard and you’re already in there having conversations anyway, then using that as your main way to get to files is probably the easiest. But it’s not the only way. If you prefer other routes to your files, you can use them. In some ways I know this can sound very complicated with all the different possible scenarios. But it really comes down to one question: who owns that conversation or that file? Teams is structured in such a way that once you are in the right spot for a conversation, all the files will also be lined up. If I’m having a conversation in one team about a specific project, I don’t need to stop again to think about where to put files related to that project. I can just go to the Files tab and know that everything in there is visible to the team. If you’re going to make a new file and don’t know where to put it, think about who needs to be able to see the file. And I think it is important to qualify that as who will ever need to see it. Maybe 99% of the time it will be you using the file, but somebody else needs it when they need to cover for you while you’re on vacation. Or if something happens and you suddenly aren’t working starting tomorrow, will everybody else have everything they need to continue your work? So, a lot of things like documentation of processes, or files in a project that others may need to reference in the future, those should all be in SharePoint, typically through a Team. There may be more things private to you that stay in your OneDrive. In my work, that means things like configuration files for all my developer environment tools, and a copy of the profile photo I use everywhere. Nobody else ever needs to see those. But most of what I’m doing is in SharePoint, through Teams, available to others if or when they need it. Permissions in both OneDrive and SharePoint can get a lot more complicated than the defaults – you can share from OneDrive with others – but the more you stick to the defaults the easier your life is going to be. Syncing Files That’s the most important piece, understanding the destination and that Teams is one route into that destination. But it’s not the only route. I’ll cover a few more ways to access files and I’ll reiterate what I said earlier: you don’t need to do all of these. You need to find which one or ones fit your workflow. The next most common one is synchronizing your files. You may want to synchronize files that are in a SharePoint site to your computer. This makes it so that you have a copy on your computer and whenever a change happens in either the cloud version or your desktop version, the change will be synchronized to the other location so that everybody continues to be working from the same content even if technically there is a copy. Why would you want to do this? There are three major scenarios: If you don’t have reliable Internet and you need to access the file while offline. If you have file types other than Office apps that you need to act on in other ways. For example, in my work, I might have a bunch of image files that I’m going to need to upload to a website. To do that, I’ll need the files on my computer. And the last good reason is simply that you are used to browsing for files in Windows Explorer. You will still need to have enough understanding of the cloud locations in order to start the sync, but after that, you can have the benefits of it syncing to the cloud while still doing everything the way you’re used to doing it on your computer. There are also now two ways to achieve this. The older way is the Sync button which is visible both in Teams and SharePoint in the browser. Hitting that will open the OneDrive app on your computer to set up the sync. The newer way is a “Add shortcut to OneDrive” button which shows up in SharePoint but not in Teams. It’s too big of a tangent to break down the advantages of each, but I do have a post about it. Other Ways to Access Files Those are the two ways most intuitive for most people to access their files: within Teams and synchronized to your computer. But there are a lot more which I will mention but not demo all of them. I mentioned Microsoft Search earlier. There are a few entry points to Microsoft Search. One of them is a Bing search. Cue the jokes about somebody actually using Bing. But I have my default browser search set to Bing. The main reason is Microsoft Search. If I want to try to start a search, I can enter that in a new tab address bar like I would any other search, and part of the results is this “Work” or “School” section (depending on your organization type). That will give me lots of content across Microsoft 365 including documents, groups, people, Teams conversations, and so on. A scenario where I use this a lot is if I’ve encountered a new bug on a website. I’ll do my search, and first I’ll look at the Work/School tab to see if this is a problem anybody in my organization has dealt with before. Maybe there’s a file documenting it, or maybe a Teams conversation about it. If I don’t find anything internally, I flip back to the All results tab and see what is available elsewhere. And speaking of a new tab, if you use Edge as your primary browser, you can set your New Tab screen to show your Office documents. I like this because I open new tabs a lot and sometimes I’ll quickly glimpse that a colleague has edited a shared file relevant to me, so it gives me the extra prompt to look into that. There are a few other places that look more or less the same as this. You can go to office.com. There is a Windows and a mobile app simply called Office. If you use Android, there’s a launcher called Microsoft Launcher that can show recent cloud documents. What those all have in common is that the focus isn’t necessarily on browsing everything, but they can prompt you to quickly reload a file that the Microsoft algorithm thinks you may want to see again. You can also open files directly in the Office apps like Word and Excel, whether web, desktop or mobile. Those actually give you a bit of both where you can see the recommended files or you can browse through SharePoint to find something. If these files are housed in SharePoint or OneDrive, they do not also have to also be synced to your computer to open them in the desktop app. You can open directly from the cloud. There are also OneDrive and SharePoint mobile apps, which can be good if you want to be able to browse everything a bit more freely or you need to do a lot more work on the go. You can also browse SharePoint in your desktop browser, as I mentioned before, but there isn’t necessarily navigation to get from one site to another if your organization’s IT hasn’t set that up.&quot;
    },{
      "url": &quot;/2021/github-copilot/&quot;,
      "title": &quot;GitHub Copilot&quot;,
      "content": &quot;A few months ago it was big news in the programming world when GitHub Copilot was announced. This week, I was accepted into the preview, so now I have a chance to try it out and share my thoughts. Concerns Before I do that, I’ll make a few caveats. When it was announced, the reaction was mostly positive. It is an incredible service that can really help speed up development. But there were a couple concerns I saw being raised, which I will address quickly. One concern was that developers would allow Copilot to write large chunks of their code and not actually understand it. I’m sure this will happen and it is certainly not ideal. It’s also not that different than copying and pasting from StackOverflow without understanding. That’s why ultimately this part doesn’t concern me much. The other concern I think is a bit more serious: licensing. Copilot combines code from publicly-available code repositories. Those repositories may have a wide variety of licenses on them. Some are completely fine with you using them in a commercial product. Some do not want you to do that at all. Some are ok with it as long as you provide credit. The problem with Copilot grabbing the code for you is that it does not filter by license and it doesn’t even point you to the original repositories that helped inspire the code. The counterpoint is that Copilot is not strictly copying code from the repositories – it is writing new code with inspiration from many repositories – but it still feels a bit closer to the ethical line than many are comfortable with. My First Test With that out of the way, I’ve now had a chance to try it out. The first thing I tried was in a WordPress plugin that looks for Bible references and wraps them in links. I’ll have a post about this when it’s done, but I did this years ago in a previous job and decided to try to recreate it as a portfolio project. I had two major problems left when I got access to Copilot. The first was easy but would take some time: I needed to list all the books of the Bible and all the common abbreviations for them. I did have a shell of a function already, but only had done anything for Genesis and Exodus. So, I wrote a comment saying this is exactly what I needed to accomplish. /** * Define abbreviations for all books of the Bible */ Copilot kicked in. Sort of. It gave me what looked like it was exactly a slightly-older version of my code right above it, only giving me Genesis and Exodus. I can say it was kind of cool to watch the suggestion pop up, even if that excitement faded quickly once I realized it wasn’t giving me anything helpful. The second was regex. I needed it to recognize a few different potential patterns. I also needed it to ignore if the link was in a header or another link. Similar to the first problem, I did already a shell of an idea here, but I tried to get it to help me. This time it gave me nothing useful at all – it didn’t even try to write regex. Maybe this use case was too niche, although I imagine there are enough other uses on GitHub for a list of Bible abbreviations. Maybe it was struggling because I had already started something previously and so it stuck to that instead of starting fresh. I’m certainly not giving up on the idea. I’ll leave it enabled as I do more work. Hopefully it will offer something a bit more valuable as time goes on, and if it does, I’ll try to blog about that. For now, my grade of Copilot has to be an “incomplete.” Update Since this initial test, I have kept Copilot turned on for more normal operations. It has helped a little with Drupal PHP work, but not much. It has helped more with CSS: sometimes it will generate something for me and I’ll try it out and adjust from there, rather than writing my own from scratch. It has prompted a good idea or two for me that way. The most I’ve found I’ve accepted the suggestions is actually not in the code itself but in writing comments; it is quite good at finishing my sentences. So, it continues to not be as dramatic for me as I’ve seen others demo, but it is certainly worth turning on.&quot;
    },{
      "url": &quot;/2021/koa11y/&quot;,
      "title": &quot;Koa11y: Accessibility Testing Tool&quot;,
      "content": &quot;Koa11y is another useful website development tool I came across recently. It is a downloadable desktop app that can be used to test out a website’s accessibility. It is not the most user-friendly app – not as intuitive as the WAVE extension in your browser, for example – but a web developer should have no problem sorting out a few extra steps. Most importantly, it did return more detailed issues with the website I first tested against compared to WAVE, which can be valuable information to fix as many accessibility issues as possible. Koa11y Demo To try it out, simply download the zip file (Windows, in my case) from the link above and extract it. Launch the main executable to see a screen like this: Enter a website that you want to run accessibility tests against. For example, I tried for ryanrobinson.technology. It will generate an HTML file saved on your computer with all the details of the report. Pa11y It is built on top of pa11y. Pa11y is a tool that iterates over a webpage and tells you all the accessibility errors found. It comes with a variety of configuration options. I first went down this route as I was investigating tools that might help me enforce more accessibility testing as part of a GitLab CI/CD pipeline. Unfortunately, it can only check websites which are publicly accessible, and our dev servers are protected and only accessible by our team. I wouldn’t have been able to integrate it into our pipeline until any changes had already gone to production, which defeated the purpose. But it did lead me to Koa11y which can still be a useful tool to have in the belt to periodically review sites which are publicly-accessible. If you’re looking for something a bit more programmatic in your workflow, though, give Pa11y a try. If Koa11y’s results are any indication, it’s a pretty powerful tool.&quot;
    },{
      "url": &quot;/2021/yubikey-early-impressions/&quot;,
      "title": &quot;Yubikey: Early Impressions&quot;,
      "content": &quot;I recently obtained two Yubikey security keys to boost my personal and professional security. I picked up one Yubikey Bio and one Yubikey 5 NFC. They recommend that you always have a backup in case you lose one, and from what I had learned, I wanted the Bio in several services that would support it, but also the 5 NFC for other services and for mobile NFC authentication. It’s now been a week, so here are some initial thoughts. Support I set up on multiple services. These were the big ones to me that supported both keys: Microsoft (personal) Microsoft (Office, as long as the IT department has enabled it) GitHub 1Password Twitter WordPress A couple others only accepted the Yubikey 5 NFC: Google (personal) GitLab Finally, here are some of the services I tried which do have at least one form of multi-factor authentication, but did not support any kind of security key: LinkedIn (that one surprised me, since it is owned by Microsoft) PayPal Amazon Docker The setup process in general is easier than setting up with an authenticator app or text messaging. You just need to plug it in and enter your same PIN every time, instead of going back and forth copying a numerical string from one device to another. Bio vs 5 NFC The Bio, unlike the 5 NFC, has a fingerprint reader. When initially setting it up, I still had to use a PIN, but after that, I could simply tap my finger on it instead of entering the password. You still need to set a PIN on it and that will be a fall back if you can’t use the fingerprint reader, but for the most part it’s a bit easier to tap your finger. It does not work as a fingerprint reader with Windows Hello. I thought it was supposed to, but doing some research now, nothing suggests that I can. I’ve tried it on 3 different computers (1 is work-managed) and it does not work. That’s disappointing. I thought this was a good way to get a security key and a Windows Hello device all in one. On the other hand, there are three advantages to the Yubikey 5 NFC compared to the Bio: NFC support means it will work for logging in to services on your phone, by tapping the key to the back of your phone that has NFC turned on. The Bio can’t do much for mobile logins. More services support it (see above). It’s cheaper. Conclusion They are great. They are more secure than using an authenticator app (somebody could hypothetically hijack my authenticator app in a way they couldn’t for the key without physically taking it from me) and easier to use. The question is whether they are great enough to justify spending on them. If you’re doing a lot of logins with apps in your work, the answer is probably yes. If you are a casual user who only has 5 or 6 services with two-factor authentication to deal with, the answer is probably no – stick with the authenticator app on your phone. If you decide to get some, get at least two so you have a backup. Two of the Yubikey 5 NFC’s are likely all that most people need for cheaper. If you’re logging in a lot, you may want the combination like I did: a Bio for all the desktop services that support it and a 5 NFC for mobile use and the rest.&quot;
    },{
      "url": &quot;/2021/vagrant-oracle-linux-vm/&quot;,
      "title": &quot;Vagrant: Oracle Linux VM&quot;,
      "content": &quot;This is the first in a series setting up a basic DevOps pipeline through local virtual machines, GitLab, a dev server, a staging server, and a production server. This first post will look at the local VM setup. The next step will move on to using this local VM in connection with GitLab for version control and CI/CD. Oracle Linux 8 LAMP Stack The VM will be using Vagrant and Oracle VirtualBox. Every user will need to install both, as well as the VirtualBox extensions, for this to work. The configuration for the Vagrant box can be found on my GitHub. For the sake of this demonstration, it is a fairly simple Oracle Linux 8, using a build in the Vagrant repository. The first script, provision.sh, sets up the basic VM with a LAMP stack: Apache MySQL PHP 8.0 Several PHP extensions Composer That’s a nice generic LAMP stack. If you want an Oracle Linux 8 LAMP stack for some other purpose, you can stop there. Drupal 9 Local Dev In my case, the purpose of this vagrant box is for the first stage of development on a Drupal 9 website. So I’m going to go a few steps further to get it ready for the specific Drupal 9 project. That includes: Sync the user’s SSH key from their computer’s folder to the vagrant user’s home folder, so that it can be used to connect to GitLab Configures git including pulling the latest version of the code from the GitLab Imports a MySQL database if one is provided Self-signed keys so that https browsing will be possible I may write up specific details of some of these later, but the bottom line is the code in GitHub. This part is in the provision script d9.sh, which also references a separate script specifically for the database import called importdb.sh. The User Experience The idea is that at the end of this, the person setting up their machine has very few steps: Git clone the repo for building the virtual machine on their PC Put a copy of the database to import, if available, in the specified location Put a copy of the SSH key into the specified location Run vagrant up to build the virtual machine SSH into the VM and then run the separate script for the Drupal 9 components Optionally add the dev URL to the hosts file of their computer so that it is browsable at the URL and not just at localhost&quot;
    },{
      "url": &quot;/2021/android-email-apps/&quot;,
      "title": &quot;Android: Email Apps&quot;,
      "content": &quot;I recently got a Pixel 6. It’s great. For the previous 3 and a bit years I had been using a BlackBerry Key2. There was lots I liked about the Key2, but 3 years later I had never received an Android version update. I was still getting security updates, but I decided with the Pixel 6 launch it was time for an update. That context to say that I was using the BlackBerry apps for email, contacts, tasks, and calendar. The exception was my work email, which doesn’t allow app passwords and the BlackBerry app incredibly did not support modern authentication with multi-factor. So, I decided with the new phone that it was a good chance to evaluate Android email apps again. My qualifier is that I use Microsoft email – personal and business – not Gmail. So I immediately ruled out the Gmail app. Here’s a quick review of the ones I checked out: BlueMail Some positives: Calendar and contacts integrated nicely Unified inbox with all accounts combined to one list, or view them separately Good notification options including LED colour Some negatives: Stalled out when I added the second account. Maybe a fluke bug, but not a great start. I closed the app, came back in, and it was fine. I didn’t love the design, although I couldn’t nail down exactly why. The dark mode is not that dark. The colour combinations are a bit odd. Conclusion: on paper it hits my biggest requirements, but there’s just something about the design that led me to move on quickly. Newton Not free after the first 14 days. Nevermind. Edison First, it appears in the app screen as simply “email.” That was confusing when I had all of these apps installed at once, but I see the value in that decision normally. In normal contexts where you only have one email app, having that app called “email” is nicer than remembering it’s called Edison. Some positives: Asked me up-front if I wanted Focused Inbox, which I do not, rather than turning it on and forcing me to find the switch to turn it off like the others with a similar feature. Some smart assistant features like identifying your travel itinerary and subscription review to help you unsubscribe from emails you don’t need anymore Unified inbox, or view accounts one at a time Good configuration options like notification quick actions and colour-coded accounts Some negatives: No calendar or contacts view Lots of little prompts trying to sell me on the premium subscription Conclusion: definitely does some good things, but it doesn’t solve the calendar/contact piece and I know I would regularly get annoyed at the ads for premium. Spark There’s a lot I like about Spark, including a lot that overlaps with what I liked about BlackBerry Hub: It had the simplest login. Most of the others required first selecting the email provider (Gmail, Outlook, Exchange, etc). A lot of people would know this, but not everyone. Even if you do, it’s a few extra clicks. Spark will look it up for you – you just have to know the address and it will redirect you to the appropriate login. Attractive, easy-to-use, and a good dark mode. A reasonable amount of customizing the app like being able to set the quick actions on a notification, the LED colour of a notification, and whether to notify on all or only on known contacts. Multiple accounts and unified inbox to see them all together or split them out to view one at a time. Email signatures, including different ones default for different accounts. It had one big hole: Does not include calendar or contacts at all. I’m really hoping for one app that does everything. Finally, a neutral for me is that it uses a smart inbox to predict what you consider most important and prioritize them in your inbox. I know some people like this idea. I do not. I also turn off the equivalent feature in desktop Outlook. I am an aggressive manager of my inbox already and having something “smart” hiding things from me just confuses my own system. It can be turned off, so this isn’t a big deal. Conclusion: other than the lack of calendar and tasks, it did feel like it hit on a lot of the things I liked about the BlackBerry app. Nine I didn’t get too far with Nine before encountering a couple things that worried me. First, it says it won’t limit any basic features for 14 days. In other words, it won’t be free after 14 days. It doesn’t give any hint within the app of what it will cost. I had to jump through a few websites before I found a price, and even that wasn’t entirely clear if that price was a one-time price, a yearly price, or a monthly price. If it clearly said “$15 one-time fee” I would be a lot more interested than having no idea what it will cost. I got over that and decided to start trying to set up an account. It didn’t ask me what type of email it was. I was prepared to list that as a positive as I have for a couple others. I entered my email address. The next screen told me to enter the password to my IMAP email, not taking me to a Microsoft sign-in screen. My email is a Microsoft email, but with a custom domain, from the days of Windows Live Domains. So that’s not great that it didn’t figure that out. Fortunately, there was an option to manually specify, so I went back and chose instead to specify that it was indeed a Microsoft account. After that, this email app is all positives for me. It has all the positives I listed above for Spark, but also integration of calendar, contacts, tasks, and notes. There are widgets to easily pin those different components to the home screen, even though it’s one app. After those first couple bad impressions, Nine is fantastic. Outlook This is the one I had already tried. Some good things: Includes calendar and contacts management. Dark mode; generally attractive and easy to use. Voice dictation and voice reading my emails From Microsoft so I can pretty confidently say that it will steadily get better for years to come Some negatives: Slower getting emails for some reason Not quite as many configuration options as some of the others (like LED notification colours, quick actions on notifications) Conclusion Nine is my winner. I will gladly pay $15 USD one-time for such a good app. I’m happy that I found an app that checked all my boxes and without a subscription fee.&quot;
    },{
      "url": &quot;/2021/microsoft-enterprise-administrator-expert/&quot;,
      "title": &quot;Microsoft Enterprise Administrator Expert&quot;,
      "content": &quot;Last week, I passed the MS-101 exam. This completed a goal I have had for a few years, earning me the Microsoft Enterprise Administrator Expert certification. I first got into SharePoint about 6 years ago. It was before modern SharePoint, so not the most pleasant experience, but I still found myself fascinated by it. I decided I wanted to learn more about that space and pursue some certifications in it. I began working toward the MCSE which would take 3 exams. I got the first one, the 70-346, in November 2018. When I got back that day and went to book the next one, Microsoft had announced changes to the certifications. My 70-346 was not going to expire but was effectively obsolete. So I needed to start again, with the MS-100 that is roughly what the 70-346 was. I got that one in December 2019. The job I was working at the time left me with no real free time, so it was slow but eventually I decided to book the MS-700 on Teams administration. I booked that in November 2020 for March 2021, giving myself lots of study time. I booked the MS-101 for soon after. But this was in the middle of the pandemic (so far). The testing centre remained closed. Multiple times in a row I got notified that it needed to be rescheduled. I eventually wrote and passed the MS-700 in September. Then I wrote and passed the SC-400 on Microsoft Information Protection. That one wasn’t a big priority for me, but I got a free exam through an Ignite challenge and it helped me study for the MS-101. And then I finally got the MS-101 last week. It was almost a year after initially booking it, and about 6 years since I first decided to get these certifications, so this was a huge sigh of relief. I just earned another one through another Ignite challenge, so I may be looking at squeezing in one more late winter / early spring. I’m thinking PL-900: Power Platform Fundamentals, which is something I already know fairly well. But first, a bit of a break to focus on other things including wrapping up some of the 28 drafts I currently have sitting on this website to finish and publish 🙂&quot;
    },{
      "url": &quot;/2021/windows-10-hosts-file/&quot;,
      "title": &quot;Windows 10: Hosts File&quot;,
      "content": &quot;Configuring a hosts file on your computer allows your browsing traffic to go to a different server than is listed by public DNS. This can be essential for a few scenarios, such as: migrating a site to a new server and needing to test it before changing the public DNS development on dev/staging servers which do not have public DNS listings Open the File The hosts file in Windows is located at C:&#92;&#92;Windows&#92;&#92;System32&#92;&#92;drivers&#92;&#92;etc&#92;&#92;hosts. I find I routinely forget that, so I added a shortcut to my primary document folder so I can get back to it faster. To do that in Windows, browse to the folder where you want the shortcut. Right-click and select New Shortcut. In the prompt that opens, enter C:&#92;&#92;Windows&#92;&#92;System32&#92;&#92;drivers&#92;&#92;etc&#92;&#92;hosts for the location of the item. Click next, then provide it a shortcut name – that could stay simple as “hosts” or something else. Save it, and now you’re good to go. Side note: Linux has a similar system for overriding public DNS. The hosts file there is typically located at /etc/hosts and is structured the same way. You will need to edit this file as an administrator in order to make changes. You can approach this either way: Open your text editor as an administrator initially by right-clicking on the icon to launch it and select “Run as administrator.” Some editors such as Visual Studio Code will allow you to open the file first as a non-administrator, then when you try to save it will fail and prompt you if you want to save it as an administrator instead. Since I use VS Code for my main editor anyway, I tend to use option 2 as a bit faster most of the time. File Contents The contents of the file are straightforward. Each line has an IP address and at least one domain. When you view that domain in your browser, it will go to that IP address instead of checking public DNS. For example, if you want to browse your local VM at https://domain1.com, add this line: 127.0.0.1 domain1.com If you want to have multiple domains on the same IP address, like a dev server that has a couple different test sites on it, you can structure it in one of two ways. The shorter version has one line per IP address, followed by all the domains: 127.0.0.1 domain1.com domain2.com domain3.com You can also structure it with a new line for each domain, even if that means repeating the IP: 127.0.0.1 domain1.com 127.0.0.1 domain2.com 127.0.0.1 domain3.com If you’re manually editing the file, you’re probably going to do the first one, since it is shorter. But the latter can be useful if you’re automating scripts that would benefit from adding a single domain as needed. That’s it! It’s straightforward once you know how to do it, but it’s one of those things that is (understandably) hidden from the average user so you likely never hear about it without looking for it.&quot;
    },{
      "url": &quot;/2021/mail-tester/&quot;,
      "title": &quot;Mail Tester&quot;,
      "content": &quot;A common scenario with many web services like a CRM is sending mail and realizing that it is often going to the junk of recipients. There are several reasons why a message may go to spam. Some of those reasons come from the content of the message. Some are because of filters set up by the recipient or their email provider. And some factors come down to technical configuration on the part of the sender. It’s this last group which can be aided by a tool like mail-tester.com. It is simple to use: Visit the site and copy the unique email it gives you. Send a message to that address. You want this test to be as close to your real scenario as possible. If you’re trying to test from a website, send from the website. If you’re trying to test from Outlook, send from Outlook. And so on. Similarly, make the content a copy of a real message. Return to the site and hit the “Then check your score” button. It will provide you with a report including a score. If you’re hitting at least an 8 or 9 out of 10, you’ll be fine sending to most email services. It is true that some services weigh factors differently, though. Most of the recommendations it will give to improve your score are not that hard to implement for an IT person, so aim for everything you can. It is also true that sometimes email providers have their own block list which you could have ended up on through no fault of your own, e.g. maybe your website has a similar IP to a spam offender and the provider blocked a bunch of addresses including yours along with the real offender. A good sign that this is happening is if you notice patterns, like if Gmail never receives your messages but Outlook does, or vice versa. If the latter happens with Gmail or Outlook and you’ve done all the other standard things that mail-tester can help you identify, you can file a request with them to get off the list.&quot;
    },{
      "url": &quot;/2021/microsoft-teams-search-filters/&quot;,
      "title": &quot;Microsoft Teams: Search Filters&quot;,
      "content": &quot;I recently was asked whether it is possible to search in Microsoft Teams for a file but only within a specific site. The short answer is yes, but that’s not the default search behaviour. When you use the search bar in Teams, it will default to all types of results across all Teams you are a member of. But then you can filter from there. This is significantly improved by some recent changes. So, if I know I am looking for a particular file in a particular Team, I can do that this way: Teams Start the search in the usual way by typing your search term in the search bar at the top of Teams. By default you’ll see all types of results from all Teams, and it might not be obvious that you have more filter options than the basic content type filters. But, once you click on those, you’ll get more filters specific to that type of content. For the example of Files, you’ll see the ability to filter by Team, File Type, Modified by, and Date. For the example scenario originally asked of me, the goal would be achieved by using the Team filter. In other words, Teams search starts at universal, then provides filters to help you get more specific. SharePoint By comparison, SharePoint search behaves a bit differently. When you are on a particular site other than the Home site, it will default scope to only that site and others in the same hub, even if you are a part of other sites. Then you have filters to expand. It’s essentially the opposite direction. It can assume based on the current site that you are looking for content on that site. This difference makes sense, as the Teams search bar stays in the same place no matter what you are currently viewing. There isn’t an easy way for Teams to know in advance which Team you want to search. So it makes sense to start at everything and move backwards, while SharePoint can start specific and allow you to move outward.&quot;
    },{
      "url": &quot;/2021/drupal-pecl-uploadprogress/&quot;,
      "title": &quot;Drupal: Install PECL UploadProgress&quot;,
      "content": &quot;Drupal often recommends that you install and enable PECL UploadProgress to show better feedback when uploading files, but it doesn’t make it obvious how to do that. This quick post will run through what it took for me to get PECL UploadProgress on an Oracle Linux 8 server. Add Required Packages There are a few preliminary packages that you’ll need: gcc, make, php-devel, and php-pear. You can install in Oracle Linux 8 using this command: sudo yum -y install gcc make php-devel php-pear You don’t have to provide the -y. That will pre-approve any other prompts that come up, including if it requires installing other dependent packages. If you want to be more careful, don’t use that, but it can come in handy in contexts like installing this through a script. Install PECL Now that you’ve got the dependencies, you can install uploadprogress: sudo pecl -y install uploadprogress Add to php.ini The packages are now in place, but your PHP installation will not make use of it unless it is recognized as an extension in your php.ini file. In the default Oracle Linux 8 setup, this file is located at /etc/php.ini. You’ll need to add this line to your file: extension=uploadprogress You can edit that file directly using your preferred editor to add this line to the file, or if you are using this within a script, you can have the script write it to the end of the file like this: sudo echo &#92;&quot;extension=uploadprogress&#92;&quot; &gt;&gt; /etc/php.ini If you first need to check whether that line is already in the file – so that you don’t end up writing it multiple times in a context where a script might add another line every time – you can use this in a bash script: if [ ! -z $(grep &#92;&quot;extension=uploadprogress&#92;&quot; /etc/php.ini) ]; then sudo echo &#92;&quot;extension=uploadprogress&#92;&quot; &gt;&gt; /etc/php.ini fi Restart Finally, you’ll need to restart Apache and PHP for this to go into effect: sudo systemctl restart php-fpm sudo systemctl restart httpd This script is available in my GitHub.&quot;
    },{
      "url": &quot;/2021/pa11y-ci-oracle-linux-8-installation/&quot;,
      "title": &quot;pa11y CI: Oracle Linux 8 Installation&quot;,
      "content": &quot;I’ve mentioned before that pa11y is a great tool for accessibility testing, specifically in the context of the koa11y desktop tool. In this post, I’ll run through installing pa11y-ci on an Oracle Linux 8 server. NodeJS Pa11y requires NodeJS. To add that to Oracle Linux 8, run this command: sudo yum install nodejs -y If you want to add an efficiency check within a larger bash script, you can first check whether it is already installed: #Install NodeJS and NPM if [[ &#92;&quot;$(yum list --installed | grep nodejs | wc -l)&#92;&quot; -eq 0 ]]; then sudo yum install nodejs -y fi Pa11y Now you can install Pa11y itself using npm: sudo npm install pa11y -g --unsafe-perm=true --allow-root Note the extra flags for unsafe-perms and allow-root, which don’t sound like good things. But without them, it will fail to have the permissions to write everything in all the directories it needs to be able to write. As above, you can add an efficiency check if you’re doing this within a script: #Install pa11y if [[ &#92;&quot;$(npm list nodejs | wc -l)&#92;&quot; -eq 0 ]]; then sudo npm install pa11y -g --unsafe-perm=true --allow-root fi Dependencies Pa11y is now installed, but trying it out, it won’t actually work, citing other libraries that are missing. There are several of these. To install all of them: #Install libraries sudo yum install -y pango.x86_64 libXcomposite.x86_64 libXdamage.x86_64 libXext.x86_64 libXi.x86_64 libXtst.x86_64 cups-libs.x86_64 libXScrnSaver.x86_64 libXrandr.x86_64 GConf2.x86_64 alsa-lib.x86_64 atk.x86_64 gtk3.x86_64 nss libdrm libgbm #Install fonts sudo yum install -y xorg-x11-fonts-100dpi xorg-x11-fonts-75dpi xorg-x11-utils xorg-x11-fonts-cyrillic xorg-x11-fonts-Type1 xorg-x11-fonts-misc As above, in a scripting context you might want to check if it is already installed, with something like this: if [[ &#92;&quot;$(yum list --installed | grep pango.x86_64 | wc -l)&#92;&quot; -eq 0 ]]; then sudo yum install pango.x86_64 -y fi The full script is available in my GitHub.&quot;
    },{
      "url": &quot;/2021/sharepoint-hub-association/&quot;,
      "title": &quot;SharePoint: Hub to Hub Association&quot;,
      "content": &quot;A long-awaited feature has arrived in SharePoint: the ability to associate one hub site with another hub site. Over a year ago I saw references to a PowerShell cmdlet to do this in the official documentation pages, but with a note that it was in limited preview so most of us wouldn’t be able to do it yet. Now it has finally rolled out to general availability. Why Do This Associating one hub with another hub changes one thing: search results. If you’re in one hub and do a search, results from other associated hubs can be prioritized alongside other sites in the same hub. This is a nice extra level of controlling your search results for a pretty common scenario. For example, imagine being in a small team with lots of projects. You want a site for every project, associated to your team’s hub site (for the search results, for a sites web part, etc). But your team is also a part of a larger department and you may have a lot of useful general resources on that site. Now when you search from your team’s SharePoint you can get results in both directions: the projects in the same hub and the general documents of the associated department hub. What It Doesn’t Do It does not change any of the other things that come with the first level of hub association, most of which makes sense if you think about them. Navigation doesn’t change. There is a shared hub menu. Nothing from the associated hub’s navigation will appear. But it makes sense that it doesn’t really work to have two conflicting navigation areas. The automatic application of site designs for all sites in a hub also stays as only sites in that hub. This makes sense for the same reason navigation does: you can’t really have two site designs applied to the same site at once. The sites web part, with the setting to filter by hub associations, doesn’t change. This one I think could make sense to expand, for scenarios much like the search: it could have one filter setting for only sites in this hub and another filter for sites in associated hubs as well. I may be missing something for why that couldn’t work, so maybe that feature will come later. More Details Check out SharePoint Maven’s recent post for more details.&quot;
    },{
      "url": &quot;/2022/drupal-7-hide-label-node-display/&quot;,
      "title": &quot;Drupal 7: Hide Label on Node Display&quot;,
      "content": &quot;I ran into a problem on an old Drupal 7 site where labels for a custom field were displaying, even when there was no content associated with that field, creating a page that was simply a series of headers in a row. That’s not very user-friendly. If I wanted to always hide the label for a field, that’s easy enough in the default Drupal display settings for the content type. But what about this scenario where we want to show the label when there is content and hide the label when there isn’t content? In theory, the logic for this is simple. In the node template for that content type – in the node–content-type.tpl.php in the templates folder of your theme – we want to add some simple logic that says to not show the field at all if the value of the field is empty. If you don’t already have a node–content-type.tpl.php, copy your default node.tpl.php file, rename it to node–content-type.tpl.php, and then the changes you’ll want to add are before the main content variable is rendered for the page. A couple wrinkles did make it a bit more complicated, though. First, you cannot check if the value of the field is set. It is always set. That’s entirely reasonable, but not obvious, so I still mention it. Second and a lot less reasonable, I couldn’t even use a test for if the field is empty. This is where I spent the bulk of my time being confused. I would write the value of the field to a log or message to test and it should show nothing. I could inspect the code and there was nothing – not even a space or a hidden HTML tag. But if I tried to test for empty() or the string to equal ”, it still said no, it’s not empty. Eventually I checked the length of that string and discovered it was 2, not 0. I do not know if that is universal in Drupal 7 or if there was something weird with this one site. It’s possible you’ll be able to get away with a simple empty() test instead of the strlen() test. So this was my final code addition to the node template file: //hide some labels if the corresponding value is empty $field_value = $content[&#39;field_field_name&#39;][&#39;#object&#39;]-&gt;field_field_name[&#39;und&#39;][0][&#39;value&#39;]; if ((strlen($field_value) &amp;lt;= 2)) { unset($content[&#39;field_field_name&#39;]); } You could also do this in one line instead, depending on where you fall in the debate between easy readability vs minimal code. I left it as two different lines to be easier for somebody looking at it to understand what was happening. //hide field label if the corresponding value is empty if ((strlen($content[&#39;field_field_name&#39;][&#39;#object&#39;]-&gt;field_field_name[&#39;und&#39;][0][&#39;value&#39;]) &amp;lt;= 2)) { unset($content[&#39;field_field_name&#39;]); }&quot;
    },{
      "url": &quot;/2022/drupal-gitpod-container-1-dockerfile/&quot;,
      "title": &quot;Drupal GitPod Container 1: .Dockerfile&quot;,
      "content": &quot;GitPod is a great tool for cloud-based containers when developing. If you’re developing and want a safe and efficient cloud container to try things out, it’s a pretty good way to go. You even get 50 hours per month for free, which is pretty great if you only need occasional side project and not full-time work. It also works with Visual Studio Code – although that has not been working for me lately – so you can use it in the browser or in your desktop editor. When you browse to a GitHub or GitLab repository with the extension installed, there’s a simple button that will launch the container with that repository’s code, making it quick and easy to see how it works as well as make changes. In this mini-series I describe how I created a generic Drupal-friendly container working with GitPod. It is available in my GitHub. Note that since this is some code I may continue using over time, the code there may change beyond what is covered in this article. The Dockerfile This solution starts from the GitPod-provided MySQL image. That does not give everything you need for a functioning Drupal site, but it does meet the basics of a LAMP stack. FROM gitpod/workspace-mysql The biggest catch with using this image is that you might end up with the latest PHP version, and there’s a good chance that your Drupal site doesn’t support that yet. So you probably want to add a section to change the PHP to one that you know currently works: RUN update-alternatives --set php /usr/bin/php8.0 That doesn’t cover everything, though. There are a few more things I want for every Drupal 9+ site. PHP packages Here’s how to add PHP packages needed within an Ubuntu Dockerfile, as well as APCU and uploadprogress which are also recommended by Drupal. I wrote recently about installing PECL UploadProgress for Oracle Linux, which is similar but a bit different than these Ubuntu commands. USER root # Install other needed packages RUN apt update RUN apt install -y php-pear php-apcu php-json php-xdebug build-essential mysql-client sendmail RUN pecl install apcu RUN pecl install uploadprogress Composer Drupal can work without composer, but your life is going to be a lot easier with it. Here’s how I added that to my Dockerfile: # Install latest composer RUN php -r &#92;&quot;copy(&#39;https://getcomposer.org/installer&#39;, &#39;composer-setup.php&#39;);&#92;&quot; RUN php -r &#92;&quot;if (hash_file(&#39;sha384&#39;, &#39;composer-setup.php&#39;) === &#39;906a84df04cea2aa72f40b5f787e49f22d4c2f19492ac310e8cba5b96ac8b64115ac402c8cd292b8a03482574915d1a8&#39;) { echo &#39;Installer verified&#39;; } else { echo &#39;Installer corrupt&#39;; unlink(&#39;composer-setup.php&#39;); } echo PHP_EOL;&#92;&quot; RUN php composer-setup.php --install-dir /usr/bin --filename composer RUN php -r &#92;&quot;unlink(&#39;composer-setup.php&#39;);&#92;&quot; At this point, the server is ready to provide the core packages that run Drupal well. Pa11y accessibility testing I’ve also mentioned pa11y recently, the accessibility testing tool. I always want that available. Here’s how to do that in this Ubuntu-based Dockerfile: # Install pa11y accessibility testing tool, including NodeJS RUN curl -sL https://deb.nodesource.com/setup_16.x -o # Install pa11y accessibility testing tool, including NodeJS and Chromium RUN curl -sL https://deb.nodesource.com/setup_16.x -o nodesource_setup.sh RUN bash nodesource_setup.sh RUN apt-get install -y nodejs libnss3 libatk1.0-0 libatk-bridge2.0-0 libcups2 libdrm-amdgpu1 libxkbcommon-x11-0 libxcomposite-dev libxdamage-dev libxrandr-dev libgbm-dev libgtk-3-common libxshmfence-dev software-properties-common RUN apt-get install -y apparmor snapd apparmor-profiles-extra apparmor-utils kdialog chromium-browser libappindicator1 fonts-liberation RUN npm install pa11y -g --unsafe-perm=true --allow-root Ports Finally, I’m going to open the standard ports: 80 (HTTP), 443 (HTTPS), and 3306 (MySQL). This doesn’t seem to be actually necessary but it can help to remind yourself what ports you’re using. EXPOSE 80 EXPOSE 443 EXPOSE 3306 What’s Next In an upcoming post, I’ll continue with the other large file to make this work: the .gitpod.yml. A third post will cover a few smaller changes I had to make to other configuration.&quot;
    },{
      "url": &quot;/2022/drupal-assign-permissions-username-file/&quot;,
      "title": &quot;Drupal: Assign Permissions Based on Username File&quot;,
      "content": &quot;Here’s a recent scenario I encountered: a Drupal role needs to be assigned to certain users. The site is using a single sign on (SSO) system with a lot of users who could log in. But only some of those should be granted a certain permission. The list of those who can access the special permission role is automatically generated and put in place on the server on a daily basis, in a simple format with one line for each account name. I have pulled out a generic version of this code and made it available on my GitHub. Hook on Login First, we need to hook on login. When a user signs on, we want to find out if they should have the extra permission. This can be done with the hook_user_login hook. /** * Implements hook_user_login */ function permissions_from_file_user_login(&amp;amp;$edit, $account) { // assign the &#92;&quot;test role&#92;&quot; role, if necessary. _permissions_from_file_update_permissions($account); // do not redirect password reset requests. if (!isset($_POST[&#39;form_id&#39;]) || $_POST[&#39;form_id&#39;] != &#39;user_pass_reset&#39;) { if (isset($_GET[&#39;target&#39;])) { drupal_goto($_GET[&#39;target&#39;]); } } } Read File Next we have another function that checks to see if the username is in the file or not: /** * Reads from file and determine whether the specified user is in the list **/ function _permissions_from_file_is_role($account) { $contents = file_get_contents(&#39;/var/usernames/roleusernames.csv&#39;); return preg_match(&#92;&quot;/$account-&gt;name/&#92;&quot;,$contents); // returns if name is in the file. } Assign or Remove Role Finally, we can assign permissions based on whether or not their username is in the file. /** * Calls function that checks if permissions should be found **/ function _permissions_from_file_update_permissions($account) { $is_role = _permissions_from_file_is_role($account); $role = user_role_load_by_name(&#39;test role&#39;); if ($is_role) { // add permissions to that user user_multiple_role_edit(array($account-&gt;uid), &#39;add_role&#39;, $role-&gt;rid); } else { // delete the role assignment, if it exists user_multiple_role_edit(array($account-&gt;uid), &#39;remove_role&#39;, $role-&gt;rid); } }&quot;
    },{
      "url": &quot;/2022/drupal-gitpod-container-2-gitpod-yml/&quot;,
      "title": &quot;Drupal GitPod Container 2: .gitpod.yml&quot;,
      "content": &quot;This post picks up in a mini-series describing how I created a generic Drupal-friendly container working with GitPod. The first covered the GitPod.Dockerfile to build the core LAMP stack image. The entire project is available in my GitHub. More of the remaining work is done by the .gitpod.yml file, including referencing the Dockerfile image described in the previous part, as the starting point. image: file: .gitpod/GitPod.Dockerfile Tasks It then runs a handful of tasks, split into two sections. The tasks under “init” will run only when the container is created for the first time. The tasks under “command” will run every time the container is started, even if you’re re-opening an existing container. Create the database Creating the database is straightforward and familiar enough to any who have used MySQL from a command line before: sudo mysql -e &#92;&quot;CREATE DATABASE IF NOT EXISTS drupal&#92;&quot; Install codebase The contributed codebase can be built using composer install: composer install I’m also going to copy the settings file into place: sudo cp .gitpod/drupal.settings.php web/sites/default/settings.php That settings file includes several alterations including where to find the configuration and content sync directories, how to connect to the database, and which URL is allowed to login from. Most of it will be familiar to those who have used Drupal settings files before, but I will cover anything unique in my next post for this series. Drupal Settings and Content This is the hardest part of working with Drupal websites. All of the configuration and content is held in the database, not the codebase. Drupal (since 8) offers a configuration sync tool out of the box, and a content sync tool as a module. Broadly speaking, the configuration tool is usually pretty reliable, while the content tool has had some occasional glitches in my time using it. I install the site using the drush site-install tool: vendor/drush/drush/drush -y site-install minimal --existing-config The key there is the –existing-config tag. This starts the installation using the configuration details you already have in the configuration sync folder. The main thing this accomplishes is keeping the UUID of the site in sync. Config sync and content sync will not work if the UUIDs don’t match. Now the content sync will also work: vendor/drush/drush/drush -y content-sync:import --entity-types=block_content,file,node The reason I added the –entity-types instead of simply importing all entity types was because of some experiences with menu_entity_index. I won’t get into those details here, but that created another entity type for menu links, which tended to not synchronize very well mainly because the content sync does not sync the node ID of each content. That meant that a page might have a different ID on one installation from the content sync than on another. Menu items were then pointing to the node ID, which is now different content, and it created some nasty URL aliases. Similarly related to the node ID sync is the homepage. The configuration is saved with specifying which piece of content is the homepage using the link to the node ID. But that might no longer be the content you want on the homepage. To solve this, I added a query to find my real homepage and set it again each time: home_id=$(vendor/drush/drush/drush sql-query &#39;SELECT nid FROM node_field_data where type=&#92;&quot;home&#92;&quot; and status=&#92;&quot;1&#92;&quot; and title=&#92;&quot;Home&#92;&quot; limit 1;&#39;) if [[ ! -z ${home_id} ]] then vendor/drush/drush/drush cset -y system.site page.front /node/$home_id fi The key for that structure to be maintained is that the homepage is of the content type “Home” and with the title of “Home.” If that isn’t met, this workaround will fail. Admin Password By default, the site install at this point created a default super-admin user with a random password. For the purpose of this demo, I wanted to control what that admin password was so I could put it in the GitHub README and keep it saved in my password manager. So, I used drush to always reset the admin password back to what I wanted it to be: vendor/drush/drush/drush user:password admin &#92;&quot;ZNB*ufm1tyz4rwc@yzk&#92;&quot; Cleanup There are two more small acts of clean up that I want, to minimize warning errors that would otherwise appear as soon as I sign in. One is to rebuild the content permissions and the other is to run cron. vendor/drush/drush/drush php-eval &#39;node_access_rebuild();&#39; vendor/drush/drush/drush cron Drush Alias I want to be able to use the command drush instead of always having to reference the full path to drush, as I did above. So I add an alias to my .bashrc. You can also do this by adding the location of drush to the PATH. Arguably that is more correct but this was simpler in a GitPod scenario where containers are constantly being built and destroyed and so don’t always have to be precisely the long-term ideal. echo &#39;alias drush=&#92;&quot;/workspace/drupal-gitpod/vendor/drush/drush/drush&#92;&quot;&#39; &gt;&gt; ~/.bashrc Command The command tasks accomplish two simple things: start Apache, and source the bashrc to make sure it notices the drush alias. source ~/.bashrc apachectl start Ports The GitPod.yml file optionally defines ports and what to do when they open. You don’t need to have this section. The ports will open either way. But it can be nice to specify in advance what you want to do when each port opens. In this case, I’m ignoring everything except for 8001, which I will open in the browser, since that is the port for the Apache access. If you don’t specify, it will give you a popup with each port opened asking what you want to do, so this saves some time. ports: - port: 8001 onOpen: open-browser - port: 8828 onOpen: ignore - port: 8829 onOpen: ignore - port: 3306 onOpen: ignore GitHub This section starts with a somewhat misleading key “github.” These functions are not exclusive to GitHub – they can be used with any of the supported version control platforms with GitLab and Bitbucket as well. The main valuable thing here to me is defining prebuilds, which enables GitPod to start building the new image as soon as code changes are submitted to the repository. That saves time when you’re ready to start working and it’s already good to go, instead of waiting a few minutes. github: prebuilds: master: true branches: true pullRequests: true pullRequestsFromForks: true addCheck: true addComment: false addBadge: true addLabel: true Extensions The final section of this file is to install VS Code extensions. There is one big qualifier here: it can only install extensions that are available in the Open VSX registry. This may not be everything available for VS Code, with the most notable exclusion for me being GitHub Copilot. Here’s my Drupal 9 friendly set of extensions: vscode: extensions: - eamodio.gitlens - skippednote.VS-code-drupal - cweijan.vscode-mysql-client2 - esbenp.prettier-vscode - gruntfuggly.todo-tree - mblode.twig-language - vscode-icons-team.vscode-icons Almost There… In the last post of this series, I’ll look at a few other configuration changes I needed to make for this to work.&quot;
    },{
      "url": &quot;/2022/drupal-gitpod-container-3-settings-config/&quot;,
      "title": &quot;Drupal GitPod Container 3: Settings and Config&quot;,
      "content": &quot;This post continues a mini-series in which I describe how I created a generic Drupal-friendly container working with GitPod. The code is available in my GitHub. The first two posts covered the two big files: GitPod.Dockerfile and .gitpod.yml. This final post will cover a few minor changes I had to make to other configuration files. Apache Conf Only one thing needs to change from the default Apache configuration file: telling it to look at /web for the actual website content, instead of /var/www/html. That’s because a Drupal project typically has a top level that includes several necessary pieces of the site like the composer.json file, private files, and configuration sync files, outside the folder where the public site’s content is found. That’s done by changing the document root from &amp;quot;${GITPOD_REPO_ROOT}&amp;quot; to &amp;quot;${GITPOD_REPO_ROOT}/web&amp;quot;, so the relevant section of the file looks something like this: # Directory and files to be served DirectoryIndex index.html index.htm index.php DocumentRoot &#92;&quot;${GITPOD_REPO_ROOT}/web&#92;&quot; RewriteCond %{REQUEST_FILENAME} !^/$ RewriteCond %{REQUEST_FILENAME} !^/(files|misc|uploads)(/.*)? RewriteCond %{REQUEST_FILENAME} !&#92;&#92;.(php|ico|png|jpg|gif|css|js|html?)(&#92;&#92;W.*)? RewriteRule ^(.*)$ /index.php?q=$1 [L,QSA] AllowOverride all Require all granted To use this conf file, I saved my complete file in the .gitpod directory and then had the GitPod.Dockerfile copy that file into place: COPY .gitpod/apache2.conf /etc/apache2/apache2.conf PHP.ini The PHP.ini is similarly close to the default Ubuntu, but not quite. We’ll have to add support for a few extensions at minimum. There are a couple ways to do this. One approach is to have the script add the lines to enable the extensions, without touching anything else. The advantage of this is keeping everything else to the default, which may matter as the GitPod base image changes over time. That would look like: echo &#92;&quot;extension=uploadprogress&#92;&quot; &gt;&gt; /etc/php/8.0/apache2/php.ini The other alternative as I’ve done here is to completely overwrite the php.ini file with my own. The advantage of that approach is to also control all the other variables like the memory limit. The more you want to change from the default, the more this approach makes better sense than adding individual lines. So in that case, the file is copying using this line in the .gitpod.yml: sudo cp /workspace/Drupal-GitPod/.gitpod/conf/php.ini /etc/php/8.0/apache2/php.ini You could also accomplish this using sed to replace lines of the file in place, which is nice if you want to keep a perfectly-structured file replacing a default value. For example, to change the max_execution_time from 30s to 300s, I might do this: RUN sed -i &#92;&quot;s/max_execution_time = 30/max_execution_time = 300/g&#92;&quot; /etc/php/8.0/apache2/php.ini Drupal Settings Most of the Drupal settings are similar to the default. The largest change is familiar to any Drupal developer: set the database credentials. Since this is a GitPod container which gets created and destroyed on the fly regularly, I went simple using the root user and no password. A production server would have a different user and a good password. $databases[&#39;default&#39;][&#39;default&#39;] = array ( &#39;database&#39; =&gt; &#39;drupal&#39;, &#39;username&#39; =&gt; &#39;root&#39;, &#39;password&#39; =&gt; &#39;&#39;, &#39;prefix&#39; =&gt; &#39;&#39;, &#39;host&#39; =&gt; &#39;127.0.0.1&#39;, &#39;port&#39; =&gt; &#39;3306&#39;, &#39;namespace&#39; =&gt; &#39;Drupal&#92;&#92;&#92;&#92;Core&#92;&#92;&#92;&#92;Database&#92;&#92;&#92;&#92;Driver&#92;&#92;&#92;&#92;mysql&#39;, &#39;driver&#39; =&gt; &#39;mysql&#39;, ); Beyond that, there are a couple of tweaks both related to the fact that the GitPod domain is going to change. First I set the trusted_host_patterns. The valid domains for browsing this site are either as some subdomain of gitpod.io, or as localhost. $settings[&#39;trusted_host_patterns&#39;] = [ &#39;^.+&#92;&#92;.gitpod&#92;&#92;.io$&#39;, &#39;^localhost$&#39; ]; I also opened up the cookie_domain to allow anything. This is so that it doesn’t object to you first logging into the site at one GitPod subdomain, then spinning up a new container and trying to login with that one. /** * Used to track what domains you&#39;re logged into * Set to * to accept any GitPod domain */ $cookie_domain = &#39;*&#39;; That’s it! I’ve now covered having a functional Drupal-friendly GitPod container.&quot;
    },{
      "url": &quot;/2022/gitlab-devops-php-lint/&quot;,
      "title": &quot;GitLab DevOps: PHP Lint&quot;,
      "content": &quot;Here’s another piece in a GitLab DevOps setting: when code is committed to GitLab, I want to run a PHP linter on the custom code folders of Drupal (modules and themes) to make sure there aren’t any glaring syntax bugs that snuck through. My personal favourite error is when a “:wq” gets inserted into a file trying to exit vim, after doing all the testing. I split this into two pieces, taking advantage of GitLab’s ability to extend other functions, even functions in other projects. My real work scenario is using GitLab, hence using GitLab CI. For the purposes of sharing here, I’ve put them in my personal GitHub. I know that’s an unintuitive combination. The Generic CI/CD Functions To be able to test, I first need a general Oracle Linux 8 image that can run PHP 8.0 and composer for a Drupal site. ## Build an Oracle Linux 8 container, used by other tests below ## .ol8_lamp_build: stage: test image: oraclelinux:8 before_script: - dnf -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm - dnf -y install https://rpms.remirepo.net/enterprise/remi-release-8.rpm - dnf -y module enable php:remi-8.0 - dnf install -y php php-gd php-pdo zip unzip git php-curl php-mbstring php-zip php-json php-xml php-simplexml php-mysqlnd php-pecl-apcu wget curl - php -v - php -r &#92;&quot;copy(&#39;https://getcomposer.org/installer&#39;, &#39;composer-setup.php&#39;);&#92;&quot; - php composer-setup.php - php -r &#92;&quot;unlink(&#39;composer-setup.php&#39;);&#92;&quot; Part one is a project containing a few generic CI job templates. One of those, in the tests.yml file, is a PHP lint test. Saying that I want to test every php file in a certain folder is not as easy as you’d think – there is no flag for applying it recursively – so it requires a loop: ### PHP lint test ### .php_lint: stage: test extends: .ol8_lamp_build variables: DIRECTORIES: &#92;&quot;./&#92;&quot; EXTENSIONS: &#92;&quot;php&#92;&quot; script: ## Recursively checks for files of specified extensions in specified directories and completes php lint on them - cwd=&#92;&quot;$(pwd)&#92;&quot; - | for DIRECTORY in $DIRECTORIES do cd $DIRECTORY for EXT in $EXTENSIONS do files=&#92;&quot;$(find -name *.${EXT} -type f)&#92;&quot; for file in ${files} do php -l ${file}; done; done; cd $cwd; done; This has variables for the directory to start testing from, as well as what extensions to test. The extension variable defaults to only php files, but it can be overridden to include other extensions, which is valuable with Drupal where other extensions get used for php code like .module and .theme. The Project’s CI File Part two is the specific project that then references those generic templates. Using the full project reference, under the assumption that in a real situation this would be in a differernt GitLab project, that looks something like this in the project’s .gitlab-ci.yml file: # Includes general CI jobs to extend from include: - project: &#39;[path to the GitLab project with CI]&#39; ref: main file: test.yml ## Stages ## stages: - test ## Test Jobs ## ### Tests for PHP errors in custom code ### php: extends: .php_lint variables: DIRECTORIES: ./web/themes/custom ./web/modules/custom EXTENSIONS: php module theme inc This example will run the PHP syntax test against all .php, .module, .theme, and .inc files located with the Drupal 8+ custom themes and custom modules directories.&quot;
    },{
      "url": &quot;/2022/gitlab-devops-gitlab-runner/&quot;,
      "title": &quot;GitLab DevOps: GitLab Runner&quot;,
      "content": &quot;This relates to the GitLab DevOps process. GitLab can automate deployment to a server, meaning that you do not have to log in to that server separately to pull the changes and carry out any other needed steps for the installation. But there’s a catch: if your server is not publicly-accessible, such as behind a VPN, which is pretty common for development and staging servers, you’ll need to install a runner. This is because GitLab cannot initialize a connection to a hidden server, so the server will have to start the connection instead. Here’s how I installed a GitLab runner on a server. This was specifically for Oracle Linux 8, but it will be pretty close for other distributions. Install GitLab Runner Package First you need to install the GitLab Runner package on your server. GitLab will give you the instructions to do this, but here’s the general idea. First, copy the package to your server, in the /usr/local/bin folder: sudo curl -L --output /usr/local/bin/gitlab-runner &#92;&quot;https://gitlab-runner-downloads.s3.amazonaws.com/latest/binaries/gitlab-runner-linux-amd64&#92;&quot; Add permissions to execute that file. You’ll need to be able to run it. sudo chmod +x /usr/local/bin/gitlab-runner Navigate to that folder and install: cd /usr/local/bin sudo ./gitlab-runner install --user=libgit --working-directory=/home/libgit Register a Runner Once the runner is installed, you’ll need to register at least one runner on the server. sudo ./gitlab-runner register --url https://gitlab.com/ --registration-token [token] The script will prompt you with a few configuration options. Some are fairly obvious, but I’ll note a couple others. The token can be found in the GitLab Project -&amp;gt; Settings -&amp;gt; CI/CD -&amp;gt; Runners. For the tag, I recommend having a standard convention across all your servers, like [site shortcode]-[dev stage]. For example, site1 will have: site1-dev site1-staging site1-prod1 site1-prod2 And site2 will have: site2-dev site2-staging site2-prod For what executor to use. Shell is the simplest, at least in a scenario like this of running it on a Linux server. That’s it. It may take a bit to appear, but the runner will now be associated with the GitLab project.&quot;
    },{
      "url": &quot;/2022/drupal-fluid-ui/&quot;,
      "title": &quot;Drupal: Fluid UI&quot;,
      "content": &quot;In doing research on accessibility tools that could be incorporated into a Drupal site, a co-worker recommended Fluid UI. This is not the same Fluid Framework used by Microsoft as the backbone of the new Microsoft Loop tool, or the Fluent UI Microsoft design language. There’s lots of use of Fluid and Fluent out there. This one is an open source group out of Toronto focused on web accessibility, and they offer their tools in a few different formats. One is as a convenient Drupal module. I installed it on a dev site and took a look at what it offered. The toolbar It creates a toolbar across the top of the site, hidden by default behind a “+ Show Preferences” button. When expanded, there are a few options which can be valuable accessibility improvements. Change font size or spacing between lines Change to a high-contrast colour palette Emphasize links and buttons Display a table of contents The only one I don’t quite understand the value of is the table of contents. It gives a bland unstyled list of links, programmatically generated based on the headers on the page. I’m not sure who this is for. Sighted users can see the headers. Blind or low-vision users can navigate in a screen reader by the header levels already, much more useful than a table of contents at the top. I assume there is a use case because these developers know a lot more about accessibility than I do, but I have failed to think of what it is. Feel free to comment if you know. It’s also a bit odd visually, needing to scroll sideways to see all the options. I’d like to think there would be a better way to display them all. Configuration It has limited admin configuration options both around what pages the toolbox should appear on: Display on admin pages (yes/no checkbox). By default this is off, meaning it only shows on the public pages, but if you’re aiming to improve accessibility of administration as well, you’ll want to turn this on. Hide the toolbox on these pages (text area to specify pages). There might be some unique pages where it doesn’t work very well, so you can name those as exceptions. I am a little tempted to come up with a basic settings page that can turn on or off each component, instead of it being an all-or-nothing, and maybe be able to change the order of the components, then offer that back to the developers. It’s not a big need, though; most are going to be happy with leaving everything on. Conclusion Overall, it’s a good tool. It is a little rough, so I do hope they smooth it out some more, but it’s certainly much better than not offering these accessibility improvements at all.&quot;
    },{
      "url": &quot;/2022/drupal-sync-configuration/&quot;,
      "title": &quot;Drupal: Sync Configuration&quot;,
      "content": &quot;Having a workflow that keeps your code in sync across development, staging, and production servers – like in the series of GitLab DevOps posts I’ve been sharing recently – is important. But that doesn’t synchronize the database, which contains two major subcategories: configuration and content. It also doesn’t synchronize user-uploaded files, but that’s a subject for a different post. Fortunately, Drupal 8 introduced a new system for syncing configuration across copies of the site. What’s Included It is not a full database backup, but it does include most settings, such as: Enabled extensions and themes Theme configuration Views Blocks Module settings What it does not include is node or entity content. With the possible exception of initially building the site, that shouldn’t have any meaningful deficit on completing the work. This distinction between configuration and content makes sense in a couple ways. Configuration changes are usually made first in your development environment, tested thoroughly, and then pushed through the pipeline. That’s not true for content. Content is often going to be created or edited first by the regular site users on production. Those don’t need technical teams to approve and test everything and are available to more users to do directly and have go into effect immediately. You also don’t need to synchronize all content in the same way, since for the most part the presence or absence of an extra basic page node does not affect the ability to sufficiently test anything new that needs to be tested. It’s ok if you let your production site’s content get ahead of your development environment in a way that you don’t want to happen with configuration. Git Workflow The ideal flow for these changes is integrated with your code git flow. You can export your configuration on dev, add it to the git repository, push through to the next server in your CI/CD chain, and import it there. Much of this can be streamlined with CI/CD tools, but here are the manual steps: To export the configuration: drush config-export Add to the git repository and commit: git add sync/config/* git commit -m &#92;&quot;Updated configuration&#92;&quot; git push Switch over to the other server and do essentially the reverse: git fetch git pull drush config-import More on the drush command can be seen here. Manual You can also sync config manually one group of settings at a time. This is a slower process, so you wouldn’t take this route in a normal workflow of pushing dev changes out to production. I won’t detail that here, since the interface does make it fairly intuitive, but you can approach it halfway – sync the files but then import them one at a time from the synchronize tab – or you can avoid syncing entirely and instead manually copy the configuration file shown on one Drupal site into the other.&quot;
    },{
      "url": &quot;/2022/microsoft-teams-guest-notifications/&quot;,
      "title": &quot;Microsoft Teams: Guest Notifications&quot;,
      "content": &quot;Microsoft Teams has the great ability to invite users who are in other organizations to join your Teams in order to collaborate, assuming your IT admin has not turned that feature off. Guests in the Team can’t do everything that an internal member can do, but it’s pretty close, so it’s a strong environment for connecting with partners. But there is one pretty significant catch to be aware of: notifications. Here’s a way to think about it that I find helpful: it comes down to who owns the data. When somebody messages in a Team channel thread, that data is owned by that Team. If you message in a Team, notifications will pop immediately for anybody currently in that tenant, whether internal or guest. A message will not pop for users who have guest access to that team but are not currently operating in that tenant (probably because they’re in their own tenant). Notifications do not cross tenants. What they do get is the same as if they missed a notification in their own tenant: an email summarizing that a notification is waiting for them about an hour later, and a notification waiting for them in their Activity tab the next time they sign in to the relevant tenant. So here’s the real question: suppose I am in my tenant, which does include a guest user, and I want to get that guest user’s attention immediately. I don’t want to wait for the email notification to go out, or for them to think to check in to my tenant’s notifications. How do I do that? The good news is that the presence indicator is tenant-specific. Anywhere you see their profile, like when you tag them within a channel thread or you try to start a chat with them, you can see if they are online in your tenant. If they are, it’s easy to proceed with messaging within your tenant to them as a guest. If they are not in your tenant, the next option is an external message instead. External and guest aren’t the same thing. External sends a chat message (not a Team channel thread) from your tenant to theirs. Guest messages them within your tenant and it can also use the full Team channel collaboration tools. When you message somebody in Chat, you can see whether they are Guest or External, and you can have the same contact as both Guest and External at the same time, which can be confusing to remember which to message when. So, if they aren’t online in your tenant and you have to reach them in a hurry, the best thing you can do is send a message to the external user asking them to switch over to your tenant. Then you can continue the conversation in the Team channel. It can be unintuitive at first, but once you’ve got the basic idea of guest vs external, it does make sense.&quot;
    },{
      "url": &quot;/2022/drupal-ckeditor-abbreviation/&quot;,
      "title": &quot;Drupal: Ckeditor Abbreviation&quot;,
      "content": &quot;One of the HTML tags that many are not aware of is the abbr tag. This is a useful tag to help provide explanations of abbreviations within the body of the text. If you’re reading along and see an abbreviation for WP and you know what it is, you can keep reading. If you’re reading along and see an abbreviation and don’t know what it is, you can hover over it and it will show you an explanation. It’s a win-win, providing an explanation for those that need it without needing to add more text in the main body which may come across as over-explaining for those that don’t need it. To make it easy to insert this tag into Drupal content, you can install the ckeditor_abbreviation module. This is a simple but effective module that adds a button to the ckeditor WYSIWYG editing panel that enables users to add an &amp;lt;abbr&amp;gt; without touching any code. The interface fits in seamlessly, with a similar experience to adding a link.&quot;
    },{
      "url": &quot;/2022/github-gitlab-diagrams-markdown/&quot;,
      "title": &quot;GitHub and GitLab: Diagrams in Markdown&quot;,
      "content": &quot;GitHub recently announced a great new feature that allows for generating diagrams within markdown. I tested this out and it also works in GitLab. If you’re writing a lot of documentation in markdown, as you probably are if you’re using GitHub or GitLab, this can be really nice when you need diagrams, like mapping out your CI/CD system or server maps. You can build diagrams much faster than putting something together in Photoshop or some other diagramming tool that exports an image which you then need to upload. This is all it takes, building off the standard code markdown “` markers and using arrows to indicate links between objects. mermaid graph TD; A--&gt;B; A--&gt;C; B--&gt;D; C--&gt;D;&quot;
    },{
      "url": &quot;/2022/free-as-in-cats/&quot;,
      "title": &quot;Free as in Cats&quot;,
      "content": &quot;I once heard the phrase that most open source projects (the context was specifically Drupal) was not free as in beer but was free as in cats. What he meant by that is that you may not pay for the code up front, but you probably do have to pay to keep it running and you need to give it your attention if you really want to get the best of it. It’s not something you can enjoy with no responsibility. Cats need food, attention (sometimes while you’re trying to sleep or use your computer), clean litter, play time, and so on. Open source technology needs code maintenance and infrastructure to run it. This is a conversation that came up a lot in a previous job that included consulting with non-profit clients on Constituent Relationship Management (CRM) systems. They were often very excited that we could help them get set up with CiviCRM, an open source platform, in part because it was “free.” We made a habit of explaining that the code may be free, but there would still be costs in other ways. There were costs paid to us to help them configure it properly. There were costs paid to us to apply software maintenance updates on a regular basis. There were costs for a web server to host the site, and CiviCRM did need meaningfully more power – which meant more expensive – than a simple WordPress shared host. There would often be the cost of bugs, both in terms of lost productivity while the bug was present and in terms of time/money to get the bug fixed. There were costs to develop any custom extensions. They had to know all this before they signed off on a project. Not always, but sometimes – especially with non-profit deals available with services like Salesforce and Dynamics 365 – it would actually be cheaper in both time and money to use a paid closed source hosted platform than to use the “free” CiviCRM. That depends on a few variables including number of users and how many customizations would be needed. And of course, there were other variables as well unrelated to cost (CiviCRM is built for non-profits first). Cost isn’t the only factor, but it often is the first one that comes to mind for non-profits on a tight budget. Open source is great. I’m thankful for many great open source systems including CiviCRM and Drupal. Just know that “free” is “as in cats” rather than “as in beer.”&quot;
    },{
      "url": &quot;/2022/php-xdebug-vs-code/&quot;,
      "title": &quot;PHP: XDebug in VS Code&quot;,
      "content": &quot;XDebug is a helpful tool when debugging PHP code. XDebug is a step debugging tools that helps you inspect functionality of PHP code in a cleaner way than regularly writing dump/log statements. I will not get into details here about setting up XDebug on a server, but you can find that with some Internet searching. Instead, this post is about how to use XDebug within Visual Studio Code. Install the Extension First, install the extension PHP Debug (xdebug.php-debug) from the Visual Studio Code extension directory. Start the Listener To start the XDebug listener: Go to the “Run and Debug” tab (Ctrl + Shift + D) in the main app sidebar. Click the play button beside the XDebug listener. Depending on your developer environment, you may have other listeners as well. A small window will be added to your workspace with a few buttons. While everything is running smoothly, it will be a pause, a stop, and a restart button. While stopped because of a breakpoint, it will provide other options to continue, step over, step into, and step out. Monitor Notices, Warnings, etc You can monitor for any notices, warnings, errors, or exceptions that occur while the listener is running. Go to the “Run and Debug” tab (Ctrl + Shift + D) in the main app sidebar. If it is collapsed, expand the “Breakpoints” accordion section. Select under what conditions you want to break and show you results, e.g. Notices, Warnings. Add a Breakpoint You can add a breakpoint at a line in the code and have XDebug report the value of variables as of the start of that line. In the VS Code editor for the code you want to check, scroll to the left of the line of code you want to break. This will cause a little red dot to appear. Click on that red dot to create the breakpoint. Carry out your test scenario (e.g. load the page that calls that PHP in your browser). See the results with the values of each variable at the breakpoint, within the Run and Debug window in VS Code.&quot;
    },{
      "url": &quot;/2022/drupal-docker-devcontainer/&quot;,
      "title": &quot;Drupal Docker: devcontainer&quot;,
      "content": &quot;I have previously shared setting up local development environments using vagrant and GitPod in Drupal friendly ways. This post will start a new mini-series on how I built a Docker Desktop setup for a Drupal-friendly environment that is (mostly) based on Oracle Linux 8. Code for this is found in my GitHub. Overview There will need to be several files to make this work: devcontainer.json for how to integrate with VS Code Docker-compose.yml for how to tie together all the different containers web.Dockerfile for the primary Apache container (Oracle Linux based) php.Dockerfile for the PHP-FPM container (Oracle Linux based) db.Dockerfile for the MariaDB database container (a simpler image from an official MariaDB image) Devcontainer.json I’ll start at the first file which gets called when you open VS Code: the .devcontainer/devcontainer.json. The most important pieces are near the top. This includes: Where to find the Docker composer file: &#92;&quot;dockerComposeFile&#92;&quot;: &#92;&quot;../docker-compose.yml&#92;&quot;, What user to connect as: &#92;&quot;remoteUser&#92;&quot;: &#92;&quot;drupal&#92;&quot;, What service to connect to and what other services to start up in the background: &#92;&quot;service&#92;&quot;: &#92;&quot;web&#92;&quot;, &#92;&quot;runServices&#92;&quot;: [&#92;&quot;web&#92;&quot;, &#92;&quot;php&#92;&quot;, &#92;&quot;db&#92;&quot;], In my case, my primary is called web for the main Apache container, with two other services for PHP and DB (MariaDB). What location on the container should be your workspace after connecting: &#92;&quot;workspaceFolder&#92;&quot;: &#92;&quot;/var/www/html&#92;&quot;, You can also define what extensions and settings should be installed in this container. Unlike the GitPod equivalent, this can be any VS Code extension, not only the Open VSX ones. I’ve written some of my favourite extensions in the past, but this is a good sample of some I use specifically with Drupal: &#92;&quot;extensions&#92;&quot;: [ &#92;&quot;gruntfuggly.todo-tree&#92;&quot;, &#92;&quot;eamodio.gitlens&#92;&quot;, &#92;&quot;gitkraken.gitkraken-authentication&#92;&quot;, &#92;&quot;cweijan.vscode-mysql-client2&#92;&quot;, &#92;&quot;esbenp.prettier-vscode&#92;&quot;, &#92;&quot;whatwedo.twig&#92;&quot;, &#92;&quot;marcostazi.vs-code-drupal&#92;&quot;, &#92;&quot;github.copilot&#92;&quot;, &#92;&quot;vscode-icons-team.vscode-icons&#92;&quot;, &#92;&quot;xdebug.php-debug&#92;&quot;, ], Scripts You can trigger other scripts to run at various points in the process. The primary one I made use of is postCreateCommand. This will run only once when the container is first built. This one is much more complicated building the entire site. I’ll have another post dedicated to that script. &#92;&quot;postCreateCommand&#92;&quot;: &#92;&quot;/bin/bash -c &#92;&#92;&#92;&quot;/postCreateCommand.sh&#92;&#92;&#92;&quot;&#92;&quot;, I also have a much simpler postAttachCommand. This only sources the ~/.bashrc to ensure the user knows where to find commands like drush and composer, which were set up by the postCreateCommand script. &#92;&quot;postAttachCommand&#92;&quot;: &#92;&quot;/bin/bash -c &#92;&#92;&#92;&quot;source ~/.bashrc&#92;&#92;&#92;&quot;&#92;&quot;,&quot;
    },{
      "url": &quot;/2022/people-lookup/&quot;,
      "title": &quot;Power Apps: People Lookup&quot;,
      "content": &quot;Background A while ago I started a post in the series on SharePoint site provisioning, unpacking some of the problems I’ve faced and overcome in building SharePoint site provisioning solutions. Back then, it was much more complicated to add a user from your Azure Active Directory as a field stored on an item. It&#39;s trivial when you&#39;re using a SharePoint list as the back-end for the data storage, since there are people lookup fields in SharePoint lists. But it&#39;s more complicated if you want to put it into DataVerse or not store it anywhere - only send it directly to a Power Automate Flow. The latter is what I wanted to do with the SharePoint site provisioning. I wanted to accept a user on the Power Apps form and then the Flow would assign permissions based on that. I did not successfully finish writing that up, so I&#39;ll only include my old incomplete notes below. Good News I finish the post now because there&#39;s great news: Microsoft has now made that much easier out of the box. That post includes lots of information for how to use this new tool. Old Way Here&#39;s the video from the old way of doing it that I was starting to implement, but you can now safely ignore that and use the new way instead.&quot;
    },{
      "url": &quot;/2022/tel-links/&quot;,
      "title": &quot;Tel Links&quot;,
      "content": &quot;For many years there has been a standard for making links that open phone apps to dial a phone number. This is really great. Imagine you&#39;re browsing a website for a pizza place and you want to start a phone call to place the order. Without a tel link, especially on mobile, you&#39;re stuck memorizing the number and copying it over to your phone app. With a tel link, you quickly click on the link and your phone app is opened with the number in the dialpad. The problem comes with extensions. How do you tell the phone app to dial an extension after the base number? Some quick searching of the Internet seems to point toward the use of w (for wait) or p (for pause) as the marker between the number and the extension, e.g. 555-555-5555w555 or 555-555-5555p555. I tested these options on my Android phone and on Skype on a Windows 10 machine (the version from the Store). They did not work. Some more research found that another option was a comma (,). This one does work on both iOS and Android. It does not work with desktop Skype. It also might not cover absolutely everything that has to be pressed. For example, in the system I tested in, it required pressing # after the extension. It did not work to add # to the end of the link, e.g. tel:+1555-555-5555,555# Now What? So, what should you do? It&#39;s not very consistent, unfortunately. If you know your audience is solely or mostly iOS and Android, you can proceed with the comma approach. But that will fail completely - not gracefully - in other uncommon circumstances like Skype. That might be an acceptable trade most of the time, because how many people are really clicking on a phone link on something other than iOS or Android at this point? A middle ground approach is to link the base phone number without the extension. This will work on any phone app that supports the tel protocol. This leaves some minor room for confusion, though: somebody clicks the link, it launches the phone app, then you hear it asking for an extension, so you have to switch back to the browser to read the extension, and switch back to the app to enter it. That&#39;s a bit of an annoying surprise, but I&#39;m in the camp that it is still less annoying than having no link at all. And of course, that&#39;s the third option: don&#39;t link anything. This would be the philosophy that it&#39;s better to do nothing if you can&#39;t do it perfect. In my context, we went with the comma. We know the audience is going to be almost if not entirely iOS and Android wanting that functionality. But which one you go with may depend on your context, because unfortunately it is not a completely universal standard.&quot;
    },{
      "url": &quot;/2022/building-jekyll/&quot;,
      "title": &quot;GitHub Pages: Building a Jekyll Site&quot;,
      "content": &quot;I recently decided that I wanted to try out hosting this site on GitHub Pages instead of WordPress that I had been using for over a decade. The reason why is simple: it’s free as in cats and it’s easier to maintain after the initial work of setting it up, with no regular maintenance required. Here’s a high level look at how I did it, and the code is available in my GitHub. The main branch is everything I control, then the GitHub Actions generate the site on the gh-pages branch. Chirpy Theme I looked around for a bit and chose to start from the Chirpy theme. I like its simple design while still supporting the key blog features: categories, tags, archive timeline. And the biggest selling point was the switch between dark and light modes. That’s a nice feature to offer visitors. It does have a few things I wish were better. The navigation sidebar is not in a nav element, so that&#39;s a bit less than ideal accessibility. It also leans heavily on using ID for every element instead of class, which means my Visual Studio Code is constantly warning me that this is a fragile way to build a site. And the default colour contrasts are not strong enough in several places, but that can be overridden with CSS (see below). Knowing myself, I&#39;ll probably try to find a different theme in 1-2 years, but for now, this will do. Config The next step was to create the general config of the site and any pages outside of the blog posts like the About page. Config provided several options including where to find the profile photo, which social network links to show in the sidebar and as share buttons on posts, whether to default to light or dark mode. About is a simple markdown file with some front matter, and placed within the _tabs folder which will add it to the main navigation. Adding more pages to this _tabs folder will add them to the main navigation as well. Assets The largest piece of work was editing the assets, specifically the CSS stylesheet. The key idea for this is that anything I put in a folder called “assets” would add to and override the default assets folder on the site. This includes images and stylesheets. This stylesheet behaviour is similar to WordPress where you can have a child theme build on and override certain pieces of the stylesheet. Posts I chose to use a WordPress plugin to export everything on my WordPress site into Jekyll format. It worked pretty well, but did leave a few pieces I wanted to clean up myself. Some of the front matter it included was not relevant to my theme, so I removed those. The image in the front matter was not the right syntax, so I had to adjust each of those. And it wrapped every image in a figure tag with a caption, while this theme supports a simpler syntax for images, so I also replaced those. Some of the code around images and YouTube video embeds were messier than they needed to be. I&#39;ll have more in a future post about how I write and publish posts. Domain What I have not done yet is assigning a domain. I will move my ryanrobinson.techology domain to here, but haven&#39;t yet. It seems like a pretty straightforward process.&quot;
    },{
      "url": &quot;/2022/wordpress-jekyll/&quot;,
      "title": &quot;WordPress vs Jekyll&quot;,
      "content": &quot;I recently switched from running this blog off a WordPress site to running it off a GitHub Pages site built on a Jekyll theme. Here&#39;s a few thoughts on the pro&#39;s and con&#39;s of each. Users WordPress offers support for multiple users with different permissions. You can have some admins with full site control, some editors to oversee content, some authors who can only write new content. Jekyll does not, except inasmuch as you have multiple contributors to the GitHub project. You can set a different author name for a post in the front matter, but there&#39;s no permission level for ongoing content management. Ease of Building Somebody inexperienced with website building can still have a pretty good WordPress ssite ready in half an hour, especially on WordPress.com but even on many self-hosted WordPress sites since many hosts provide tools to quickly install it in a few clicks. Jekyll takes more expertise. Even when using an existing theme, you still need to learn a few things about GitHub, edit configuration in files (not interfaces), and handle tasks like resizing image assets and cleaning up code yourself. Cost GitHub Pages is free as in cats. WordPress will usually cost at least a little in hosting, unless you do the free ad-supported WordPress.com instead, but that has other limitations. Maintenance With a WordPress self-hosted site you need to update WordPress, plugins, and server software like PHP on a regular basis or you&#39;ll have exposed security vulnerabilities. It&#39;s not hard most of the time - unless you hit a situation like your theme not working with secure versions of PHP - but it is something you need to remember. There&#39;s none of that with a Jekyll GitHub Pages site. Just don&#39;t delete your GitHub project. There is no PHP, no server maintenance, no juggling of a dozen contributed modules that may or may not be maintained. Formatting WordPress offers a lot of built-in editing and formatting tools, well beyond the basics of HTML tags. It has a lot of little features that add up. A couple of my favourites include searching for other pages on your site to add links, which means you seldom end up with a bad link, and an image upload interface paired with resizing tools to always show at the right size. With Jekyll the sites are written in markdown. It&#39;s a bit faster if you know what you&#39;re doing but has a lesser set of design options, such as no columns. Depending on your editor, you may also need to memorize the markdown syntax for each format you want, without a WYSIWYG. It also doesn&#39;t have those other helpful checks like confirming that a link really does exist on the site before you publish it. So which is faster? Depends on the complexity of your posts. If you&#39;re mostly writing text with some headers, like this post, markdown can be pretty quick and easy on any device. But if you want columns, easy images, links to other pages on your site... WordPress may end up faster. More Features WordPress also includes other features like: Scheduling to publish at a certain time in the future - useful to optimize your release window for maximum engagement Post to your social media when it does publish Formulaically generate the permalink for the page, with options to define that formula Jekyll doesn&#39;t offer any of that. That does mean more overhead to do these things manually.&quot;
    },{
      "url": &quot;/2022/email-suspected-spam/&quot;,
      "title": &quot;Microsoft 365: Email Suspected of Spam&quot;,
      "content": &quot;What’s the Error Message A scenario I encountered recently was a user getting this error message: Your message couldn&#39;t be delivered because you weren&#39;t recognized as a valid sender. The most common reason for this is that your email address is suspected of sending spam and it&#39;s no longer allowed to send messages outside of your organization. Contact your email admin for assistance. This happened with every email they tried to send. Why it Happened They were not actually sending spam in the truest sense of that word. But they were sending bulk emails out of Outlook like any other email, and possibly without some of the requirements we have in Canada for anti-spam legislation (e.g. there has to be a link to unsubcribe from any bulk mailing list). So, even if it wasn&#39;t truly spam, it had some of the characteristics of spam: sending a lot at once, with the same generic message, without the ability to unsubscribe. Microsoft&#39;s email service concluded that this was spam, probably because you got hacked, and thus shut down the account until an administrator could investigate. The valuable thing to remember here is that Microsoft 365 email mailboxes are not intended for bulk mail distribution. There are specific services like MailChimp that are designed to handle all those extra factors in a way that a standard email doesn&#39;t. They will send out in staggered time, with multiple servers. They&#39;ll have templates that ensure you aren&#39;t missing any legal requirements. So, don&#39;t use a standard email - Microsoft or otherwise - to send bulk mail. It will catch up to you eventually. How to Fix It After this error occurs, an administrator with the necessary permissions will have to unblock the user&#39;s account. Don&#39;t do this until you&#39;re confident that you&#39;ve dealt with the root issue, i.e. that the sender is not sending bulk email anymore. Once you&#39;re ready: Go to the restrictured users section in the security centre. Follow instructions available in official Microsoft documentation.&quot;
    },{
      "url": &quot;/2022/gh-pages-mobile-authoring/&quot;,
      "title": &quot;GitHub Pages: Mobile Authoring&quot;,
      "content": &quot;I’ve recently switched from hosting these tech posts on a WordPress site running on a shared host I pay a small fee for, over to GitHub Pages for free. It’s a great system and might be all you need if you’re a developer wanting to write and publish some content quickly. One thing I was worried about, though, is writing from my phone. My habit has often been that I write rough notes on my phone, then finalize it on a computer and hit publish. I didn’t want to lose that. So here’s what I did. Mobile App First I needed an app on my Android phone to edit the Markdown files. I tried a few and settled on Markor. It can stay opened to an entire folder, can create files, edit files, delete files. You can write in markdown and preview what it looks like (without anything like CSS styling from the site, of course). It does have some modes that are unnecessary to me - to do and quick notes - and no way to turn them off in the interface. The runner up was WriterPlus. It was a cleaner interface. It&#39;s more of a pure distraction-free writing app. But it did not have syntax highlighting, WYSIWYG buttons, or a preview mode. Those aren&#39;t big losses for my purposes mostly of writing notes on the phone to be finalized on a computer, but I decided Markor was worth the trade. Sync The next problem is getting those markdown files to the GitHub repositories. One app, GitJournal, did promise to do this directly from the phone, although I did not go all the way through tests for that. But I wasn&#39;t looking for that. As I said in the intro, I usually like to write from my phone but then finalize it from a computer. So I was happy with a middle step of getting my notes to a computer and then using the computer to get it to GitHub. That brought me to the OneSync app. This lets me sync the folder of the whole project to my OneDrive. I can write on my phone and then pick up later on a computer or vice versa. Commit to GitHub After that is straightforward. The files are on my computer in a project folder via OneDrive. Now I can open that folder in Visual Studio Code, finalize the changes, and commit to GitHub which will rebuild the site based on the latest files. Publishing only one article at a time is simple enough if you’re familiar with git: you can stage and commit that file while leaving others uncommitted as “drafts.” or I can keep drafts in a different folder, in which case they can be committed without publishing. I&#39;ve gone as far as having separate folders depending on the stage of what I&#39;m writing: there are ideas for new drafts, there are drafts where I have started but not finished the necessary demos and tests, there are drafts where everything is ready but not finished writing, and there are posts that can be published. All but posts go in the gitignore so they stay synced in my OneDrive but not in my GitHub.&quot;
    },{
      "url": &quot;/2022/drupal-sync-content/&quot;,
      "title": &quot;Drupal: Sync Content&quot;,
      "content": &quot;I have previously covered syncing configuration across Drupal. But configuration is not everything in the database. The other half of the picture is the content: nodes, custom blocks, menu items, files, etc. There is an extra module available called content_sync. Unfortunately, it is not a very reliable extra module, at least in my configuration. With that said, I will still share the main extra consideration to take into your plans. The caveat for this is that it gets a bit more complicated in most regular usage than configuration, because content changes can happen in both directions. Some changes will happen in the same direction as code changes and most configuration changes, especially in the early stages of the project before production is launched: first it gets changed on the local development system, then pushed through the stages to get to production. After a site is launched, often the changes in content are being made by regular users on the production site. Maybe in your context the only content editors are also the website administrators, in which case this isn’t a concern, but often you’ll need to navigate how to make sure you don’t get conflicts. In our case, in the early stage I did have content changes going the same direction as configuration: Add content on my local Docker container Export the content Push the content sync files through to GitLab, which continues the CI/CD processes to the next server The CI/CD imports the content on the next server It started to break down around the same time that we let beta testers onto the dev server. I did warn the beta testers that this is a playground server to try things and try to find bugs, not a server to finalize content, with it in mind that I would likely be overriding content with my usual daily changes. But content_sync completely stopped working around the same time, so I made the call that I would stop using it instead. I am still loosely keeping an eye on that module&#39;s development. I would like to have it for the occasional manual sync requirements between servers.&quot;
    },{
      "url": &quot;/2022/drupal-dockerfiles/&quot;,
      "title": &quot;Drupal Docker: The Dockerfiles&quot;,
      "content": &quot;This continues a mini-series describing how I set up a Drupal development environment using Docker Desktop and the VS Code devcontainer functionality. The full code is available in my GitHub. This post takes a look at the three Dockerfiles required: MariaDB Let’s do the simple one first: the MariaDB container for the database. This is using a general official MariaDB image and I’m not even being picky about version. This is the entire file: # Use the default MariaDB image, not a specific Oracle Linux one FROM mariadb:latest # Expose the MySQL port to be accessible to the web container. EXPOSE 3306 PHP This image starts from Oracle Linux, a requirement of this particular hosting environment: FROM oraclelinux:8 USER root SHELL [&#92;&quot;/bin/bash&#92;&quot;, &#92;&quot;-c&#92;&quot;] It adds some other useful utilities, although they may not really be needed here since I won’t be accessing this container directly: # Install other useful packages RUN dnf install -y curl wget zip It creates a directory that is necessary for running PHP-FPM: # Create required directory RUN mkdir -p /run/php-fpm It installs PHP 8.0, which requires a different repository (at least so far): # Install PHP 8.0, which needs a different repository RUN dnf -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm RUN dnf -y install https://rpms.remirepo.net/enterprise/remi-release-8.rpm RUN dnf -y module reset php RUN dnf -y module enable php:remi-8.0 RUN dnf -y install php It adds several PHP extensions, including a few that are recommended by not required by Drupal (apcu and uploadprogress): # Install PHP extensions, including PECL with APCU and UploadProgress extensions, recommended by Drupal RUN dnf install -y php-gd php-pdo php-zip php-mysqlnd gcc make php-devel php-pear php-pecl-apcu php-pecl-uploadprogress It installs XDebug, the PHP debugging tool: # Install XDebug RUN dnf install -y php-pecl-xdebug RUN touch /var/log/xdebug.log COPY /conf/xdebug.ini /etc/php.d/xdebug.ini It makes several changes to php.ini to increase performance: # Increase resources for PHP RUN sed -i &#92;&quot;s/max_execution_time = 30/max_execution_time = 300/g&#92;&quot; /etc/php.ini RUN sed -i &#92;&quot;s/max_input_time = 60/max_input_time = 600/g&#92;&quot; /etc/php.ini RUN sed -i &#92;&quot;s/memory_limit = 128M/memory_limit = 2048M/g&#92;&quot; /etc/php.ini RUN sed -i &#92;&quot;s/upload_max_filesize = 2M/upload_max_filesize = 128M/g&#92;&quot; /etc/php.ini RUN sed -i &#92;&quot;s/post_max_size = 8M/post_max_size = 0/g&#92;&quot; /etc/php.ini RUN sed -i &#92;&quot;s/display_errors = Off/display_errors = On/g&#92;&quot; /etc/php.ini RUN echo &#92;&quot;# Increase timeout&#92;&quot; &gt;&gt; /etc/httpd/conf.d/php.conf RUN echo &#92;&quot;Timeout 1200&#92;&quot; &gt;&gt; /etc/httpd/conf.d/php.conf RUN echo &#92;&quot;ProxyTimeout 1200&#92;&quot; &gt;&gt; /etc/httpd/conf.d/php.conf It updates the php-fpm configuration to allow listeners from other clients (the web container): # Update PHP-FPM configuration including allowing listeners from other clients, defining the OPCache, and listening on port 9000 RUN sed -i &#92;&quot;s/listen.allowed_clients = 127.0.0.1/;listen.allowed_clients = 127.0.0.1/g&#92;&quot; /etc/php-fpm.d/www.conf RUN sed -i &#92;&quot;s/;php_value[opcache.file_cache] = &#92;&#92;/var&#92;&#92;/lib&#92;&#92;/php&#92;&#92;/opcache/php_value[opcache.file_cache] = &#92;&#92;/var&#92;&#92;/lib&#92;&#92;/php&#92;&#92;/opcache/g&#92;&quot; /etc/php-fpm.d/www.conf RUN sed -i &#92;&quot;s/listen = &#92;&#92;/run&#92;&#92;/php-fpm&#92;&#92;/www.sock/listen = 9000/g&#92;&quot; /etc/php-fpm.d/www.conf And finally, it starts php-fpm as the service running in the foreground for this container: # Start PHP-FPM ENTRYPOINT [&#92;&quot;php-fpm&#92;&quot;] CMD [&#92;&quot;-F&#92;&quot;, &#92;&quot;-R&#92;&quot;] Apache The final container, web, is the main one that the devcontainer will connect to, running Apache as its main service. Much of this repeats the PHP container, so I won’t go through all that again. Here’s what’s different. It includes a few more useful packages: # Install other needed packages RUN dnf install -y curl wget git zip mod_ssl httpd php-gd openssl which mariadb sudo patch vim It configures Apache, including creating a self-signed SSL certificate for https browsing: # Apache configuration, including SSL certificates and logs RUN mkdir -p /etc/httpd/certs COPY /conf/openssl-config.txt /etc/httpd/certs/openssl-config.txt RUN openssl req -batch -newkey rsa:4096 -nodes -sha256 -keyout /etc/httpd/certs/local.drupal.com.key -x509 -days 3650 -out /etc/httpd/certs/local.drupal.com.crt -config /etc/httpd/certs/openssl-config.txt RUN openssl req -batch -newkey rsa:4096 -nodes -sha256 -keyout /etc/pki/tls/private/localhost.key -x509 -days 3650 -out /etc/pki/tls/certs/localhost.crt -config /etc/httpd/certs/openssl-config.txt RUN mkdir -p /var/log/local.drupal.com COPY /conf/local.drupal.com.conf /etc/httpd/conf.d/local.drupal.com.conf It adds the drupal user and alters some other permissions so it can edit the web folder directly without sudo, and can sudo when necessary: # Add drupal user within the apache group RUN useradd drupal RUN usermod -aG apache drupal # Change default permissions to 775 instead of 755, so that the drupal user can write to the web root RUN umask 0002 # Create sudo group, add drupal user to it, and allow those users to sudo without password RUN groupadd sudo RUN usermod -aG sudo drupal RUN sed -i &#92;&quot;s/# %wheel ALL=(ALL) NOPASSWD: ALL/%sudo ALL=(ALL) NOPASSWD: ALL/g&#92;&quot; /etc/sudoers It installs composer, an essential for Drupal management: # Install latest composer RUN php -r &#92;&quot;copy(&#39;https://getcomposer.org/installer&#39;, &#39;composer-setup.php&#39;);&#92;&quot; RUN php composer-setup.php --install-dir /usr/bin --filename composer RUN php -r &#92;&quot;unlink(&#39;composer-setup.php&#39;);&#92;&quot; ENV COMPOSER_PROCESS_TIMEOUT=9999 It installs pa11y for accessibility testing: # Install pa11y accessibility testing tool, including NodeJS RUN dnf install -y nodejs pango.x86_64 libXcomposite.x86_64 libXdamage.x86_64 libXext.x86_64 libXi.x86_64 libXtst.x86_64 cups-libs.x86_64 libXScrnSaver.x86_64 libXrandr.x86_64 GConf2.x86_64 alsa-lib.x86_64 atk.x86_64 gtk3.x86_64 nss libdrm libgbm xorg-x11-fonts-100dpi xorg-x11-fonts-75dpi xorg-x11-utils xorg-x11-fonts-cyrillic xorg-x11-fonts-Type1 xorg-x11-fonts-misc libxshmfence RUN npm install pa11y -g --unsafe-perm=true --allow-root It adds executable permissions to the postCreateCommand script which I will detail in a later post: # Scripts for further actions to take on creation and attachment COPY ./scripts/postCreateCommand.sh /postCreateCommand.sh RUN [&#92;&quot;chmod&#92;&quot;, &#92;&quot;+x&#92;&quot;, &#92;&quot;/postCreateCommand.sh&#92;&quot;] And finally, it runs Apache as its primary foreground process: # Start Apache ENTRYPOINT [&#92;&quot;/usr/sbin/httpd&#92;&quot;] CMD [&#92;&quot;-D&#92;&quot;, &#92;&quot;FOREGROUND&#92;&quot;]&quot;
    },{
      "url": &quot;/2022/git-local-workflows/&quot;,
      "title": &quot;GitLab DevOps: Local Workflows&quot;,
      "content": &quot;This continues a series setting up a GitLab DevOps pipeline through local virtual machine / container, GitLab, a dev server, a staging server, and a production server. This post assumes you already have a local VM or container, whether that’s through vagrant, Docker Desktop, GitPod, or something else. Branch Diagram This diagram is meant to help explain the flow of code in this system. The development happens in your local branch. When you want to deploy to the dev server, request merging into the dev branch. When you want to deploy to the staging server, request merging into the main branch. There is a CI/CD job for deploying to production as well, but that’s a manual process – not automatic on a branch merge like the other two. The CI files for the deploying will be covered in a later post. The Editing and Browsing Workflow My preferred editor is Visual Studio Code. Whether the VM/container is vagrant, Docker Desktop, or GitPod, you can use VS Code with it. The best experience is with Docker, then GitPod, and finally vagrant will work but similar to any other server, with no extra special functionality like adding extensions. You could of course also use a different editor / terminal. Along with being able to edit, this is a website, so it’s pretty important to test out what the website looks like with the current code and database. Exactly what that looks like will depend on whether you’re using a GitPod, a vagrant, or a Docker. I won’t detail all the Apache configuration needed here, since they’ll vary for each system. They are covered more in the posts about having a Drupal-friendly environment in each of the respective virtualization/containerization systems: Vagrant VM GitPod Docker Desktop Commit to GitLab In this setup, we’ll have one branch of the GitLab repository for each user. That way each user will not interfere with any work done by other users on other VMs. To start a new branch and switch to it, I run this: git checkout -b ryan To add changes to and then commit to the ryan branch, I run this: git add [file to add] git commit -m &#92;&quot;[description of what changed]&#92;&quot; Merge Request When I think it’s ready to be merged into the main branch, I go into the GitLab interface and put in a merge request from my ryan branch into whatever other branch I’m trying to merge it into (either dev or main in this scenario). There are a handful of good options available including who the merge is assigned to, who should do an extra code review first, and what project milestone it should be associated with. GitLab has good documentation on merge requests, so I will not detail them any further here.&quot;
    },{
      "url": &quot;/2022/drupal-docker-config-script/&quot;,
      "title": &quot;Drupal Docker: The First-Run Script&quot;,
      "content": &quot;This continues a mini-series describing how I set up a Drupal development environment using Docker Desktop and the VS Code devcontainer functionality. The full code is available in my GitHub. This post will look at the configuration script that runs on the initial creation of the containers. It will handle the majority of the Drupal-specific functionality, roughly equivalent to the GitPod.yml file of the GitPod series. This is a bash script that runs in the container once it is first built, so start by defining it as a bash script: #!/bin/bash Enforce permissions on the SSH keys, needed for the git SSH connection to work: sudo chown -R drupal:drupal /home/drupal/.ssh chmod -R 700 ~/.ssh Provide ownership of entire web root to drupal user: sudo chown -R drupal:apache /var/www/html Move to codebase: cd /var/www/html/local.drupal.com Add local domain to hosts file (useful for tools like pa11y being able to browse the site): if [ ! -z $(grep &#92;&quot;127.0.0.1 local.drupal.com&#92;&quot; /etc/hosts) ]; then echo &#92;&quot;127.0.0.1 local.drupal.com&#92;&quot; &gt;&gt; /etc/hosts fi Create database and grant all privileges to drupal user. In this case I&#39;m putting it in the script explicitly because I want to share an out-of-the-box functional system. If you&#39;re going to do this with a real site, make sure you aren&#39;t using the same password on servers that anybody else can access, unlike Docker on your computer. Also it should be a stronger password than &amp;quot;root.&amp;quot; mysql --host=&#92;&quot;db&#92;&quot; -e &#92;&quot;CREATE DATABASE IF NOT EXISTS drupal;&#92;&quot; -u root --password=&#92;&quot;root&#92;&quot; mysql --host=&#92;&quot;db&#92;&quot; -e &#92;&quot;GRANT ALL PRIVILEGES ON drupal.* TO &#39;drupal&#39;@&#39;%&#39;;&#92;&quot; -u root --password=&#92;&quot;root&#92;&quot; Install site&#39;s contributed code base from composer: composer install Add drush alias to PATH for easier use: echo &#92;&quot;alias drush=&#92;&#92;&#92;&quot;/var/www/html/local.drupal.com/vendor/drush/drush/drush&#92;&#92;&#92;&quot;&#92;&quot; &gt;&gt; ~/.bashrc Copy the Drupal configuration files: if [[ -f ./.devcontainer/conf/drupal.settings.php ]] then sudo cp .devcontainer/conf/drupal.settings.php web/sites/default/settings.php fi if [[ -f ./.devcontainer/conf/local.services.yml ]] then sudo cp .devcontainer/conf/local.services.yml web/sites/local.services.yml fi if [[ ! -d private ]] then mkdir private chmod 755 private fi if [[ ! -d sync/config ]] then mkdir -p sync/config fi Import the existing configuration. Note that there is a drush command to build a new site from the existing configuration, but it did not work reliably in this context. vendor/drush/drush/drush site-install -y minimal vendor/drush/drush/drush cset -y system.site uuid &#92;&quot;3d9878de-3355-4510-af4d-575deb24055f&#92;&quot; vendor/drush/drush/drush config-import -y Flush generated images, which helps if configuration changes since last time including changes to the media formats. This isn&#39;t really necessary when you aren&#39;t also using the content_sync module, which I no longer am in this demo, since without that there are no images to regenerate. But I kept the image flush anyway to demonstrate how that works. vendor/drush/drush/drush image-flush --all Sets admin password. The same note applies as with the MySQL passwords above. vendor/drush/drush/drush user:password admin &#92;&quot;ZNB*ufm1tyz4rwc@yzk&#92;&quot; Rebuild the node access caches. This isn&#39;t a huge issue, but does save clearing a warning that appears in Drupal. vendor/drush/drush/drush php-eval &#39;node_access_rebuild();&#39; Set the environment indicator. This is a Drupal module that shows the site toolbar and favicon with different colours. It&#39;s very helpful to visually cue your brain when you&#39;re in Docker vs dev vs staging vs production. For this to work, you&#39;ll have to include the environment indicator module in your composer.json file. vendor/drush/drush/drush cset -y environment_indicator.indicator name &#92;&quot;Local Docker&#92;&quot; vendor/drush/drush/drush cset -y environment_indicator.indicator fg_color &#92;&quot;#ffffff&#92;&quot; vendor/drush/drush/drush cset -y environment_indicator.indicator bg_color &#92;&quot;#000000&#92;&quot; Finally, the all-important clearing of the caches: vendor/drush/drush/drush cr&quot;
    },{
      "url": &quot;/2022/local-development-options/&quot;,
      "title": &quot;Local Development: Vagrant, Docker, and GitPod&quot;,
      "content": &quot;I&#39;ve now broken down some of what I&#39;ve learned about different virtualization/containerization options: Vagrant Docker GitPod I also learned that each of these options have some pro&#39;s and con&#39;s attached to consider. Vagrant Pro&#39;s Vagrant is likely to be the most intuitive to develop since it maps the closest to building a server (other than building a server), if you are used to building servers. It&#39;s not a whole new syntax and way of thinking to learn. Vagrant Con&#39;s It is very slow to build the VM, when it works at all. Fully building the VM and starting the Drupal 9 website I was using this for could take 6-8 hours. It continues to be very slow in usage afterward: browsing the site, running composer updates from the terminal, etc. It can be very buggy, with issues like arbitrarily changing permissions on a file so that you can no longer delete them from your computer even with admin access. I&#39;ve got files on my computer that as far as I can tell there is no way to delete. Eventually it stopped working for me entirely. There are no extra tools for integration with Visual Studio Code, although you can ssh to a vagrant VM the same way you can to any other server. Docker Desktop Pro&#39;s Compared to Vagrant, Docker is much faster to build the image, at about 1-2 hours for the same Drupal 9 site. It has great integration with Visual Studio Code via the .devcontainer functionality, including installation of any extensions and pre-defining settings for all users. I have detailed much of my setup here in my Drupal Docker series. It has a strong community to draw on for support and for images. It&#39;s the most popular option now and that means you&#39;ll be able to find what you need more easily than for other options. Docker Desktop Con&#39;s It is less intuitive to develop the images if you haven&#39;t used Docker before. Learning the new syntaxes (Dockerfile, Docker compose) is straightforward, but there is a shift in thinking to containers. It is for the most part about as slow as vagrant to operate after the site is built, although you can improve that a bit by editing your wslconfig file to allot more RAM. GitPod Pro&#39;s GitPod is much faster, both to build and to use afterward, although the build time does sometimes vary wildly and stall out. Even the odd slow time is at least as fast as Docker, but the inconsistency can be disruptive. It&#39;s possible that this is not a serious problem and it only happens in my stress test scenarios like constantly building new containers. The pre-build functionality allows for the build to happen before you actually need it. It does facilitate keeping everybody on the same code base since it builds directly from the GitLab repository each time. There&#39;s little concern about getting a commit behind. The Git integration is the strongest. Since it is already tied to your GitLab/GitHub/Bitbucket account, the push and pull does not require extra configuration like syncing an SSH key file and entering a passphrase each time. It allows for defining what to do when each port opens, e.g. open in the browser when Apache starts up so you can browse the site. This is a small time saver. It does not tie up your computer&#39;s resources and does not require at least 32GB of RAM like the locally-hosted options do. It does not need your computer to support virtual machines / containers (i.e. Windows 10/11 Pro). GitPod Con&#39;s It&#39;s great when you&#39;re using a Debian-based image, but does not support other Linux variants or multiple containers. Data for their SaaS offering is held in the US and EU. Those are the only data residency options. Canada where I am is not an option. If you&#39;re concerned about data staying in your country and you&#39;re not in the US or EU, this is a problem. Otherwise, they do offer self-hosting on Azure/AWS/Google Cloud where you can control where it gets stored, but if you&#39;re going that far, is it that much better than other options? It does not automatically open VS Code Desktop. It will open in the browser that looks exactly like VS Code, then you can prompt it to open VS Code desktop, which will take clearing a couple other permission prompts. This isn&#39;t a big deal if you&#39;re opening VS Code once per day, but if you&#39;re in and out for a lot of quick changes, that does add up. In my context, it also cannot open VS Code at all if I was on my work network, leaving me to only edit in the browser. It did work early on, and maybe it will work again. I also have no idea to what degree this is on GitPod&#39;s end vs our end, so I don&#39;t say this as much to negatively review GitPod as to say it is another factor to consider. It does not support all Visual Studio Code extensions - only those which are in the Open VSX directory - which means a few absences like GitHub Copilot. Local Server Pro&#39;s You could sidestep all of these questions about local development options and build your own servers. This makes it easiest to guarantee a match with your production servers. There shouldn&#39;t be any performance issues or use up any of your computer&#39;s resources. Local Server Con&#39;s This approach doesn&#39;t scale very well if you work on a team or on multiple projects. You would need at least one virtual server per person per project. All of that scale is being paid for while not being used most of the time. Depending on your organization structures, you may need to work across different departments to maintain the servers. There are no extra tools for integration with Visual Studio Code, although you can ssh to it the same way you can to any other server. Summary In my day job context, we settled on using Docker Desktop. It&#39;s not as fast as GitPod, but it&#39;s close enough and it was worth the trade to be able to build containers exactly as we want and with no data residency concerns because nothing leaves our devices. In my experimental and demo stuff that often gets profiled here, I&#39;m often using GitPod. It&#39;s fast, I can run it from any device, and it&#39;s great for demonstration purposes since I can set it up so that anybody can have a site running my code within a couple minutes.&quot;
    },{
      "url": &quot;/2022/profile-title-override/&quot;,
      "title": &quot;Drupal: Override Title Tag of Profiles&quot;,
      "content": &quot;The profile module is a nice tool to have on a Drupal site if you&#39;re looking to create public-facing profiles about your users (e.g. staff). But it has a few weak spots including being unable to change the URL alias - it can only be /profile/id - or the page&#39;s title which shows up as [Profile Name] #[id], e.g. &amp;quot;Staff Profile #1&amp;quot;. That&#39;s not very helpful. There are two places that the profile title needs to be overridden: what appears in the main body of the page and the title tag for the page which you&#39;ll see in your browser address bar. In my case I wanted to show the first name and last name instead, so I started by creating those fields on the profile as standard text fields. With the fields in place, this post, and accompanying GitHub code and configuration, is how I worked around those issues. The URL Alias Like other nodes, the URL can be changed using the pathauto module to generate based on a pattern. Note that this takes a change in the pathauto configuration, which might not be obvious if like me you did the initial round of pathauto configuration long before adding profiles. Here&#39;s a screenshot of the settings page, which can be found at /admin/config/search/path/settings: Once the ability to set paths for profiles is turned on, you can switch over to the Patterns tab and create the pattern. I made mine /staff/[profile:field_first_name]-[profile:field_last_name]. The Page Title I fixed the page title displayed within the body with a view. I also altered the display of a profile to not show the default title and not show the first name and last name otherwise. I won&#39;t break down every setting, but here&#39;s a screenshot of the view configuration: You can also see this in configuration YML form in the GitHub project&#39;s /sync/config/views.view.profile.yml file. Once the view is ready, add the block to the correct place in your theme, and turn off the standard Page Title block for those pages (based on the URL). The Browser Title The second one required a custom module, albeit a relatively simple one. This is the key part: /** * Implements hook_preprocess_html * * Overrides the &#92;&quot;Public Profile #[ID]&#92;&quot; title with the first name and last name of the profiled staff member instead */ function profile_title_preprocess_html(&amp;amp;$variables) { if (stripos($variables[&#39;head_title&#39;][&#39;title&#39;],&#92;&quot;Staff Profile&#92;&quot;) !== false) { //Get the ID from the original title to be replaced $profile_id = substr($variables[&#39;head_title&#39;][&#39;title&#39;],stripos($variables[&#39;head_title&#39;][&#39;title&#39;],&#92;&quot;#&#92;&quot;) + 1); if (isset($profile_id)) { //Load the profile $profile = &#92;&#92;Drupal::entityTypeManager()-&gt;getStorage(&#39;profile&#39;)-&gt;load($profile_id); if (isset($profile)) { $first_name = $profile-&gt;get(&#39;field_first_name&#39;)-&gt;getString(); $last_name = $profile-&gt;get(&#39;field_last_name&#39;)-&gt;getString(); if (isset($first_name) &amp;amp;&amp;amp; isset($last_name)) { //Change the title to first name and last name $variables[&#39;head_title&#39;][&#39;title&#39;] = &#92;&quot;$first_name $last_name&#92;&quot;; } } } } } Note: with PHP 8+ I used str_starts_with instead of stripos, but this works just as well for this purpose. Along with overriding the title, it also provides a warning to users. Because it is relying on overriding only when the current title follows a certain pattern, it is a little bit fragile in that changing the display title of the profile will result in the code no longer being activated. It won&#39;t break the site or anything, but will return to showing the default unhelpful title. Here&#39;s that code. It&#39;s fairly simple, firing on the hook for a profile edit form and then displaying a standard warning. /** * Implements hook_form_FORM_ID_alter * * Adds a warning to the admin page for the profile, to advise against changing the title */ function profile_title_form_profile_type_edit_form_alter(&amp;amp;$form, &#92;&#92;Drupal&#92;&#92;Core&#92;&#92;Form&#92;&#92;FormStateInterface $form_state, $form_id) { $message = [ &#39;#type&#39; =&gt; &#39;container&#39;, &#39;#markup&#39; =&gt; &#39;&amp;lt;p&gt;Warning: Do not change the display label of the staff public profile without altering the corresponding code in the custom module profile_title.&amp;lt;/p&gt; &amp;lt;p&gt;Failing to do so will result in the title of the profile page reverting back to showing the generic profile name instead of the staff member name.&amp;lt;/p&gt; &#39;, ]; &#92;&#92;Drupal::messenger()-&gt;addWarning($message); }&quot;
    },{
      "url": &quot;/2022/drupal-profile-block-hidden/&quot;,
      "title": &quot;Drupal: Hide Block on Own Profile&quot;,
      "content": &quot;Recently I was building a Drupal 9 site that included staff profiles as well as contact forms for those staff members. I wanted to add a block on the profile pages that included a button to link to the contact form for this contact. Here&#39;s the wrinkle in the default behaviour: it won&#39;t give you a contact link when you&#39;re viewing your own content, but it also isn&#39;t returning as nothing at all, so the title for the block appeared with nothing beneath it. The key to solve this is the &amp;quot;exclude&amp;quot; option on contextual filters. There&#39;s one contextual filter for the user ID associated to the profile that you&#39;re viewing. To add this: Add a contextual filter searching for the User ID Set the default value to &amp;quot;User ID from profile route match&amp;quot; I needed another contextual filter for the user ID of the currently logged in user, and mark that one as excluded. To add this: Add a contextual filter searching for the User ID Set the default value to &amp;quot;User ID from logged in user&amp;quot; Under the &amp;quot;more&amp;quot; section, check the &amp;quot;exclude&amp;quot; option Finally, change the option on the view in the Advanced -&amp;gt; Other section for &amp;quot;Hide block if the view output is empty&amp;quot; to Yes. This will turn off the block entirely when there is no results, such as if you&#39;re viewing your own profile when that contextual filter is in place. That did the trick! The block will now appear on any profile where the associated contact has a personal contact form, unless you&#39;re viewing your own profile.&quot;
    },{
      "url": &quot;/2022/drupal-css-structures/&quot;,
      "title": &quot;Drupal: CSS Structure&quot;,
      "content": &quot;Specificity rules for CSS priority in Drupal are determined on two axes: Active custom subthemes take priority over their parent theme. Stylesheets are split into priority groups as described here. Most of the times the descriptions for each category will hold accurate. However, you may encounter, for example, that a parent theme or a module put something in &amp;quot;theme&amp;quot; when it really is better categorized as a &amp;quot;component.&amp;quot; In the scenario that you need to override that style, you&#39;ll have to also put yours at the &amp;quot;theme&amp;quot; level or it will not take priority. The end result may be something like this in the theme&#39;s *.libraries.yml file: global-styling: version: 1.x js: js/navigation.js: {} js/search.js: {} css: base: css/base/elements.css: {} css/base/variables.css: {} layout: css/layout/containers.css: {} css/layout/footer.css: {} css/layout/header.css: {} css/layout/sidebar.css: {} component: css/components/buttons.css: {} css/components/details.css: {} css/components/field.css: {} css/components/forms.css: {} css/components/images.css: {} css/components/lists.css: {} css/components/tables.css: {} css/components/tabs.css: {} css/components/views.css: {} theme: css/theme/admin.toolbar.css: {} css/theme/search-results.css: {} //use.fontawesome.com/releases/v6.1.2/css/all.css: { type: external } Note that theme in this example also includes an imported stylesheet from elsewhere, in this case fontawesome for the social media icons. This works great, but you may want to remember to periodically updated for new versions.&quot;
    },{
      "url": &quot;/2022/word-inclusive-writing/&quot;,
      "title": &quot;Microsoft Word: Inclusive Writing Options&quot;,
      "content": &quot;I recently learned that Word has a lot of options for what kinds of prompts the spelling and grammar check will give you. One category of these options is inclusivity, helping you write in a way that better includes everyone. Many of these also conveniently double as being more concise, e.g. &amp;quot;child&amp;quot; instead of &amp;quot;boy or girl&amp;quot; or &amp;quot;spouse&amp;quot; instead of &amp;quot;husband or wife.&amp;quot; To turn these on (using the latest Windows version of Word): Click on &amp;quot;File&amp;quot; from the main menu Click on &amp;quot;Options&amp;quot; at the bottom Click on the &amp;quot;Proofing&amp;quot; tab Under &amp;quot;When correcting spelling and grammar in Word,&amp;quot; select the &amp;quot;Settings&amp;quot; button beside &amp;quot;Writing Style&amp;quot; This will open a pane with a long list of options, including the inclusive writing tools. Check the box beside each option you want to turn on and apply your choices. The option categories are: Age bias Cultural bias Ethnic slurs Gender bias Gender-neutral pronouns Gender-specific language Racial bias Sexual orientation bias Socioeconomic bias If you want to be more inclusive in your writing, this is a helpful tool to turn on and have in your toolbox.&quot;
    },{
      "url": &quot;/2022/drupal-security-audit/&quot;,
      "title": &quot;Drupal: Security Audit&quot;,
      "content": &quot;About a year ago, I (virtually) attended the DrupalGovCon 2021 conference. The highlight for me was the session on improving security of your Drupal website. I’ve embedded the video below and summarized some of the major points. Security Modules Review settings of security modules including: [ ] jsonapi_extras [ ] flood_control [ ] login_security [ ] username_enumeration_prevention [ ] honeypot [ ] reCAPTCHA [ ] seckit Dev Modules Ensure none of your development modules are on production: field_ui views_ui devel anything migration related stage_file_proxy I don&#39;t have a perfect solution in place for this yet. I did try the config_split module for a time, allowing me to have separation with some development-related modules on our local development containers but not on production. Unfortunately it was becoming buggy. Config Cleanup [ ] Remove unused views [ ] Check that there is correct access control on all views [ ] Review permission roles, especially anonymous and authenticated [ ] Confirm that user registration is limited to admins [ ] Confirm that there are no test user accounts in production [ ] Confirm that file size upload limits are reasonable [ ] Confirm that private and public file directories are used properly Custom Modules and Themes [ ] Grep for any standard debugging phrases like var_dump which may still be left in your custom code [ ] Check code in an editor like VS Code that warns you of unsanitary code [ ] Look for any cases where you missed using the t function to make text translatable. Even if your site is entirely in one language, it&#39;s still good practice. [ ] Review any TODO / FIXME etc in the code Users [ ] Review users. Anybody, e.g. former staff, still have active accounts that they shouldn&#39;t? Does anybody have more permissions than they need? [ ] Confirm that the superuser account is blocked and that nobody else has the superuser role in production.&quot;
    },{
      "url": &quot;/2022/twitter-mastodon-coso/&quot;,
      "title": &quot;Twitter, Mastodon, and CounterSocial&quot;,
      "content": &quot;If you&#39;re on this site, you&#39;re probably aware enough of the chaos around Twitter. I started to write a post about why I was leaving Twitter, narrowing in on the various terrible management practices (printing off code, firing based on lines committed, losing entire security teams, etc.) at the time when everybody was focussed on the verification disaster. Then all the stuff I was worried about completely exploded so now everybody is well aware of that, too, and it is likely a matter of days to weeks before Twitter completely fails. So there didn&#39;t seem to be much point there. I also started a post about how I left Twitter. I was going to detail how I repeatedly notified everybody there while I started moving more and more to using Mastodon and CounterSocial instead. Then after the latest round of Twitter employees being fired, I realized about 7% of my Twitter follows were already on Mastodon. I&#39;m not sure that many people really need to hear about how I migrated as if that was unusual. So, how about this post idea instead: a quick rundown of Mastodon and CounterSocial and how they compare to Twitter. For the most part, Mastodon and CounterSocial line up with each other, as CounterSocial forked off of Mastodon in the past over some licensing disagreements (I have no interest in picking a side), but there are a couple big differences. The Algorithm The biggest difference from Twitter to the others is that Twitter relies heavily on the algorithm to determine what gets on your feed. Their business model - like all the other major social networks - is based on selling as many of the most targeted ads as possible. You are the product and advertisers are the customers. To do that, they have to collect as much data about you as possible. To do that, they need to keep you engaged as much as possible. To do that, they develop algorithms to target content to you, regardless of whether you actually chose to see that content, so that you can never be &amp;quot;caught up&amp;quot; and return to your normal life without fear of missing out. Much has been said about the dangers of these kinds of algorithms. They prey on some of our worst impulses, feeding us things that make us angry. Many right-wing personalities have caught on to this, realizing they can make a big following by gaming the algorithm: they say something that gets people mad, everybody shares it either in support or to dunk on it, and all of that adds up to exponential growth in their audience as the algorithm boosts it even more. The end result is more hate speech, more anger, and more polarization. They also need to collect massive amounts of data in order to categorize us effectively for advertisers. While Twitter had a pretty good security record, this does open up the potential for huge data leaks. I don&#39;t expect that pretty good record to last much longer, which is why I&#39;m inclined to delete my Twitter instead of merely deactivating or stopping use of it. There certainly are some positives as well. They are designed to give you what you will respond to, so sometimes that&#39;s really good, with it helping surface people you want to follow who may not be big enough to trend in general but is exactly the kind of person you want to know about (especially when dealing with marginalized groups), or making you aware of breaking news very quickly. There&#39;s also the simple reality that many people are happy with this trade, giving up loads of data rather than needing to pay for a service. It lowers the barrier to entry. Mastodon and CounterSocial do not use an algorithm. You see what you chose to see. You can follow accounts and hashtags and those can boost (retweet) from other accounts into your home timeline. You can also check in on your local timeline (both Mastodon and CounterSocial) or global federated timeline (Mastodon only, since CounterSocial does not federate with anybody else). But you&#39;ll never get recommendations decided by a mysterious machine learning algorithm. The speed of discovery is limited to human speed, not machine speed. Verification Verification - at least before the recent changes - was the largest advantage Twitter had over Mastodon and CounterSocial. Trust is important in an information ecosystem to know that people are who they say they are. Mastodon and CounterSocial do have a form of verification, but it&#39;s a verification of links, not of identity. It is a system that allows you to set up a link on your website to Mastodon/CoSo, which confirms that the website owner and the account owner are the same person. It does not necessarily confirm that the account is who they say they are. Somebody could impersonate me by setting up a fake website to go alongside the fake account. That it is a lot more work, so it might happen in targeted ways of public figures, but it won&#39;t happen as much in casual mass bot attacks. It also doesn&#39;t give a &amp;quot;blue check&amp;quot; equivalent, so it isn&#39;t obvious at a glance whether somebody is who they say they are. As more public figures and journalists move to Mastodon and to a lesser extent CounterSocial, this will be really interesting to watch whether they need to come up with some kind of system, and in the case of Mastodon, whether that would even be possible in a decentralized system. Servers and Onboarding Here&#39;s where CounterSocial aligns with Twitter instead of Mastodon. For each of those, you sign up and you&#39;re in. It&#39;s pretty straightforward, not much different than any other website. Mastodon gets more complicated. It&#39;s a protocol of federated servers called instances, not a &amp;quot;platform.&amp;quot; This means upfront that you have a question to figure out: what instance should I join? Veterans will think this is no big deal, but if you&#39;ve never dealt with something like this before, staring down hundreds of servers is not an obvious choice. There isn&#39;t much in the way of advice for how to decide, or tools to filter a list. Do you care about the data residency of the server? Do you care about how the admins make decisions? Do you care about what rules it has, or what other instances it has blocked? Do you want general topic or a specific interest or identity group? I landed on the largest Canadian server, mstdn.ca, but only after trying four others over four days. A couple were tech specific. A couple were big general ones. Once I found this one, I was happy for Canadian data residency, general topics, an admin who has so far done pretty well keeping up with the demand, and reasonable rules. If you pick a bad server, you may end up with some problems. Maybe that server doesn&#39;t moderate well and there&#39;s hate speech (or spam, or porn). This means you get a lot of hate speech in your local feed, but it also means other instances might block your instance so that the hate speech doesn&#39;t spread. That leaves you isolated to your server, completely losing out on the advantages of the whole federated concept. You could end up spending a couple hours getting set up only to have a miserable experience on a bad server and conclude Mastodon is horrible, when really that one server was horrible. In my opinion it&#39;s worth it in the long run, but it&#39;s a bit of a mess to sort out. User Interface Twitter has polished their interface over the years. There may be things you don&#39;t like, but they know what they&#39;re doing. Mastodon depends wildly on which app you use. I&#39;m on Tusky on Android, which is pretty good but does have some weird holes like searching for users on other instances without entering the whole instance address. But it is attractive, easy to use, and has a good range of options. CounterSocial is the definite loser here. It isn&#39;t hard to use but it doesn&#39;t fit the Android design language at all, it only has a pseudo dark mode - no light mode and no true dark mode - and is weirdly small on the screen. Features A couple significant features are not in Mastodon or CounterSocial that you might have gotten used to on Twitter. There is no quote tweet. This is intentional as it is a feature that is most often used for piling on harassment. But there are lots of positive uses for it as well, so the loss will be felt by many. I used it a lot to comment on a news story as I spread it onto my feed. Search on Twitter indexes everything, all the text of every tweet. Mastodon and CounterSocial do not. They only index profiles, hashtags, and posts that you have already interacted with. This is intentional as it gives those posting some control: if you want it to be searchable, add the hashtags you want it to be searchable by, but if you want to be able to keep it a little more private, you can. This limits harassment of mass searching for text and attacking anybody you find. There are no circles, although if you were intense enough you could replicate a less flexible version of it by making your own instance and recruiting all your closest friends to it. You can choose to share only to your followers, only to your instance, or available to be federated globally, so that is really nice. If your instance was effectively your circle, problem solved, but that&#39;s not something that most people would achieve. Mastodon allows you create bots, but you can flag those bots as bots and it will be obvious when viewing. This is great for unofficial bots like news organizations to know when this account is not being actively monitored. The latest version of Mastodon allows editing your posts. This isn&#39;t in the Tusky app on Android that I use, so I cannot comment on it. Prior to this, both Mastodon and CounterSocial had a &amp;quot;delete and re-draft&amp;quot; which was also better than nothing. Content warnings or content wrappers are available. The consensus at least for Mastodon (not as much CounterSocial) is to use these quite liberally. They aren&#39;t only for potentially sensitive content but are really any kind of subject header for the post that allows people to decide whether they want to read more or not. This is possibly my favourite feature and I&#39;ve used it for anything from Ontario politics to talking about Twitter to spoilers of Black Panther Wakanda Forever. Mastodon and CounterSocial can schedule posts in advance, which is nice. Twitter could to that with some third party tools but not in the official apps. Mastodon and CounterSocial both allow you to auto-delete posts after a certain period of time. This is also great if like me you appreciate when it feels more like an ephemeral conversation than a permanent archive of everything I ever said. Mastodon and CounterSocial allow you to mute somebody for a shorter period of time, not always an all or nothing permanent decision. This is great if you just need to tune out somebody for a bit but you&#39;re happy to see them again later. Many other features are comparable, including muting, blocking, and adding image descriptions for accessibility. Overall, in raw features, Mastodon wins out. Losing circles is unfortunate, but some of these other tools truly are great to have and easy to use. Payment As discussed above, Twitter is mostly funded by mining data for targeted ads. They also added Twitter Blue as a freemium style plan to get more features, which the Elon Musk era is leaning into. CounterSocial is freemium. You can do all the basics for free, but you&#39;ll need a Pro account to do several things including some of the features I&#39;ve named above. Mastodon is donation based. The instance admins pay the costs and hope that they get enough donations to cover it. So if you are joining Mastodon, have found a good instance, and have some room in the budget, consider kicking a few dollars to your admin. Culture Twitter had a famously unique culture. It was the place where news broke. It was the place where movements started and spread to the mainstream, including both movements for justice and movements of hate. There were plenty of niche corners for your favourite topics, but you would also inevitably get caught up in the big news of the day. It&#39;s hard to completely judge Mastodon and CounterSocial since they have been overrun by Twitter exiles. But what I can mostly say is this: CounterSocial seems to be much more the home for those who want to talk US left-wing politics. That&#39;s a good portion of the firehose. As a Canadian, that isn&#39;t a great selling point to me. Mastodon certainly has those corners, but it&#39;s also easier to avoid them and to fully delve in completely different topics instead, especially technology. Both do suffer from a bit of arrogance. They&#39;re proud of the fact they built these networks without big money algorithms, and they should be. But sometimes they also come across as looking down on those who are new and confused why things are different. I&#39;ve seen this more with CounterSocial which loves to set itself in opposition to Mastodon as well as in opposition to Twitter, but I have seen it in Mastodon as well. Both have stronger stances on paper against hate speech and trolls than Twitter. But moderation is a lot easier said than done. Ultimately this all depends on moderator abilities to keep up and to act consistently to avoid frustrations. It will be interesting to see what happens here as Mastodon in particular is growing so rapidly and will become a target. CounterSocial is a bit more &amp;quot;extroverted&amp;quot; by which I mean that it is more normal to get replies from (mostly friendly) strangers. Conclusion I have just completed deactivating my Twitter. I am really finding a new online home on Mastodon. It&#39;s where the most people are headed now and has a great range of features, topics discussed, and it has a friendly interface. It&#39;s really just the hurdle of picking an instance you&#39;ll need to get over first, and that is understandably a larger hurdle than is worth it for some people. CounterSocial will be a great option for a lot of people. It&#39;s a lot faster to get an accurate gauge of whether it&#39;s good for you or not, without the instance decisions, so I would say to go ahead and check it out if everything I&#39;ve written makes it sound appealing to you. It may not be my long-term option, aside from the fact that a few people I know are on there and I would like to keep up with them.&quot;
    },{
      "url": &quot;/2022/power-platform-certificate/&quot;,
      "title": &quot;New Certificate: Power Platform App Maker&quot;,
      "content": &quot;This week I completed a new certification! I am now Microsoft certified as a Power Platform App Maker Associate,, having completed the PL-100 exam. This covers a lot of the introductory understanding of the Power Platform including Power Apps, Power Automate, Power BI, and Power Virtual Agent. There&#39;s a lot of great tools in here and I&#39;m happy that I&#39;ve gotten a good introductory grasp of them. It is listed on the about page along with my other certifications.&quot;
    },{
      "url": &quot;/2022/active-links-views/&quot;,
      "title": &quot;Drupal: Active Links in Views&quot;,
      "content": &quot;Drupal 9 provides a different style for active links - i.e. a link that goes to the same page that you&#39;re already on - in most contexts. This helps identify for users when it would be redundant to select it again. There&#39;s one exception where it doesn&#39;t do this out of the box, however: within views. Here&#39;s how I got around that. First add this file to the theme&#39;s /js folder, in a new file called active-links.js. (function (Drupal) { var viewLinks = document.querySelectorAll(&#39;.view a[href=&#92;&quot;&#39;+window.location.pathname+&#39;&#92;&quot;]&#39;); for (let i = 0; i &amp;lt; viewLinks.length; i++) { viewLinks[i].classList.add(&#92;&quot;is-active&#92;&quot;); } })(Drupal); This is a simple function, checking to see if there are any links within views that point to the same address as the current page, and if it finds any, it adds the standard is-active class to it. As with any other new JavaScript or CSS file, you&#39;ll also need to tell Drupal to include this library if you want it to show up. Adding it to the global-styling for the theme will load it on every page. You can do that by adding a new line to the libraries.yml file, within the js section under global-styling: global-styling: js: js/active-links.js: {} This could also be structured as a module instead of dropping it into the theme. That would make it more easily sharable for others to include it on their sites instead of needing to adapt it to their themes. But it is also a very small piece of code and that may be more hassle than its worth, so I am not currently planning on doing that. Note: you will probably need to clear caches, close your browser, and open a new one, before you&#39;ll notice the change in place. JavaScript will be cached so you might be tempted to think it isn&#39;t working when it really just needs to reload.&quot;
    },{
      "url": &quot;/2022/gitlab-devops-deploy-server/&quot;,
      "title": &quot;GitLab DevOps: Deploy to Server&quot;,
      "content": &quot;This continues with the long-stalled GitLab DevOps series. An essential piece of any CI/CD system is deploying to another server. You do not want to have to sign in to each server separately and carry out 15 deployment steps to take backups, pull the updated code, import new configuration, update database schemas, clear caches, etc. You want to hit a button and it rolls out everywhere. You also want to have the ability to easily roll back if you realize a mistake was made. You will first need a GitLab runner on the server you want to deploy to. As with the earlier post on PHP linting within GitLab CI/CD, in a real scenario this would be split into two projects. In my real usage, I have one project with the general CI/CD functionality, then the specific website project can extend from those functions to push out to the server. In this demo scenario, it is all in one project of my GitHub. Deploy.yml The deploy file provides you a baseline template for deploying code changes, which is in the general CI/CD functionality project. Here&#39;s what that might look like: ## Deploy Jobs ## variables: ENVIRONMENT_NAME: &#92;&quot;dev&#92;&quot; SERVER_URL: &#92;&quot;&#92;&quot; WEB_ROOT: &#92;&quot;/opt/www/html&#92;&quot; ### Generic deploy to specified server ### .deploy_template: stage: deploy environment: name: $ENVIRONMENT_NAME url: $SERVER_URL script: - echo &#92;&quot;Deploying to server at $SERVER_URL&#92;&quot; - cd $WEB_ROOT - git fetch - git reset --hard - git stash - git pull Project .gitlab-ci.yml The other key component is the project&#39;s file to extend this deploy job. Start the project&#39;s .gitlab-ci.yml file with including the deploy file from the other project. In a scenario where that is a different project but accessible to the same GitLab user, it would look like this, assuming the project is called gitlab-ci: # Includes general CI jobs include: - project: &#92;&quot;[group path to project]/gitlab-ci&#92;&quot; ref: main file: deploy.yml This will now give you the freedom to extend jobs found in the deploy.yml file of the main branch on the gitlab-ci project. Add deploy as a stage for your project: stages: - test - deploy Finally, define the job(s) that use(s) the general deploy functionality, passing in the needed variables: ## Deploy Jobs ## ### Deploys to dev server only when changes are made to the dev branch ### deploy_dev: extends: - .deploy_template variables: ENVIRONMENT_NAME: Development SERVER_URL: [dev url] WEB_ROOT: /opt/www/html tags: - [dev GitLab runner tag] only: refs: - dev ### Deploys to staging server only when changes are made to the main branch ### deploy_staging: extends: - .deploy_template variables: ENVIRONMENT_NAME: Development SERVER_URL: [dev url] WEB_ROOT: /opt/www/html tags: - [staging GitLab runner tag] only: refs: - main ### Deploys to production only when main branch and manually triggered ### deploy_prod1: extends: - .deploy_template variables: ENVIRONMENT_NAME: Production 1 SERVER_URL: [production 1 URL] WEB_ROOT: /opt/www/html tags: - [prod 1 GitLab runner tag] only: refs: - main when: manual deploy_prod2: extends: - .deploy_template variables: ENVIRONMENT_NAME: Production 2 SERVER_URL: [production 2 URL] WEB_ROOT: /opt/www/html tags: - [prod 2 GitLab runner tag] only: refs: - main when: manual You will need one of these types of jobs for each server to deploy to, with each&#39;s corresponding variables. This demo includes a dev server, a staging server, and two production servers. Project Access Token For this to work smoothly, you can use a project access token. This is a unique token that can be set up on a project – essentially a service account – rather than the connection being tied to a particular user. This is helpful to avoid problem scenarios like a sudden change of staff when the deploy was set up using the former staff&#39;s account that no longer exists. Create a project token by going to the project -&amp;gt; Settings -&amp;gt; Project Access Tokens. Name the token something descriptive for the server that will be using it, e.g. Dev Server. Specify the permissions for that token. Within this workflow I&#39;ve been walking through here, that is only read access so it can pull within the deployment job. It does not need to be able to write back to the GitLab project, since code changes will always start at local and push through, never the other way around. Now when you clone the project or add remote for the new server, add it using a variation on the https format rather than the SSH format: https://oauth2:[token]@[GitLab project address] Next: Drupal Configuration This deploys code changes to the servers. You quite possibly need more actions to take place, like database updates, depending on what platform you&#39;re using. In my context of developing this for Drupal, there are several more steps needed. Those will be covered in the next post in this series.&quot;
    },{
      "url": &quot;/2023/drupal-deploy/&quot;,
      "title": &quot;GitLab DevOps: Drupal Deployment&quot;,
      "content": &quot;This continues where the previous post in the GitLab DevOps series left off. We can now deploy code changes to the new server, and that&#39;s great for generic deployments. Drupal adds a few extra components when it comes to configuration sync and the database. Dev vs Production The biggest challenge with this topic is navigating that there should be some things different from dev and staging servers compared to production. Some of these are cases where the module or core configuration is on each server, but with different options (e.g. environment indicator) and others are a case of modules that are only needed on dev and staging and therefore shouldn&#39;t be on production at all (e.g. maillog and stage_file_proxy). Composer offers a good way to handle the code package building distinction. You can require a package, or require-dev a package. Then you can composer install or composer install --no-dev depending on the server. That part is straightforward. The hard part is the distinctions in configuration. Much of this can be handled using the config_ignore module, which tells the configuration imports and exports to not include that configuration file, which means it doesn&#39;t get synced across the servers. You have to set those ignored configurations up separately on every server, which means if there&#39;s a change, you need to remember to do it on all environments where it would be relevant. But then there&#39;s other configurations like modules that shouldn&#39;t be active at all on production but is on dev and staging. You can&#39;t simply ignore syncing the installed modules configuration, or else you&#39;d have a mess every time you added or removed a module. Sometimes those modules create new configuration schemas, or require you to change another configuration schema to take advantage of it. Maillog is the best example of this. Maillog is a module that should install on dev/staging but not production, as it is a debugging tool for logging outgoing mail rather than sending it. It can be separated in composer as a dev module so that the code doesn&#39;t install on production. But then it also changes the modules configuration when it is installed, as well as the site&#39;s mail settings configuration. Ideally these servers are still managed through the configuration, with differences in dev/staging compared to production. That rules out using config_ignore. The other imperfect option is changing those configurations as part of the CI/CD processes. It also has a couple of problems: There might be a few seconds where the configuration is wrong, e.g. if you import the synced configuration but then force it back to the desired value with a follow-up command. Often this doesn&#39;t matter, like a module being installed and then being promptly uninstalled again, but in other cases like sending email, it might be that the site tries to send an email in those few seconds, fails, and nobody notices that somebody should have gotten an email but didn&#39;t. So you&#39;d have to be very careful about this technique in production. It relies on keeping the CI/CD up to date. If something changes with the site configuration, you better remember to account for how that configuration will be the same or different on all servers and if the CI/CD needs any changes to prepare for it. This makes it more prone to something breaking when you forget or can&#39;t adequately test production behaviour until it&#39;s too late, already on production. Note: I did try the config_split module, which sounds like it should help but did not work reliably. There is an update that came out since my last times, so maybe it will be worth trying it again. For the simplicity of the rest of this blog post, I&#39;ve decided not to include my specific examples here. Hopefully my thoughts on how I&#39;ve negotiated the options for these kinds of scenarios is more valuable than any specific conclusions that we have in place right now. The Jobs These jobs are added to the general function deploy.yml, which in my real-world scenario is in a different project but could be in the same project. Here&#39;s what the extendable jobs look like: ### Generic deploy to specified server ### .deploy_template: stage: deploy environment: name: $ENVIRONMENT_NAME url: $SERVER_URL before_script: - echo &#92;&quot;Deploying to server at $SERVER_URL&#92;&quot; - cd $WEB_ROOT - git fetch - git reset --hard - git stash - git pull ### Drupal install jobs ### .drupal_composer: stage: deploy environment: name: $ENVIRONMENT_NAME url: $SERVER_URL script: - cd $WEB_ROOT - vendor/drush/drush/drush state:set system.maintenance_mode 1 --input-format=integer - composer install .drupal_composer_prod: stage: deploy environment: name: $ENVIRONMENT_NAME url: $SERVER_URL script: - cd $WEB_ROOT - vendor/drush/drush/drush state:set system.maintenance_mode 1 --input-format=integer - composer install --no-dev .drupal_config: stage: deploy environment: name: $ENVIRONMENT_NAME url: $SERVER_URL after_script: - cd $WEB_ROOT - vendor/drush/drush/drush state:set system.maintenance_mode 1 --input-format=integer - vendor/drush/drush/drush cr - vendor/drush/drush/drush config-import -y - vendor/drush/drush/drush cr - vendor/drush/drush/drush updb -y - vendor/drush/drush/drush state:set system.maintenance_mode 0 --input-format=integer - vendor/drush/drush/drush cr .drupal_cache: stage: deploy environment: name: $ENVIRONMENT_NAME url: $SERVER_URL tags: - prod2 after_script: - cd $WEB_ROOT - vendor/drush/drush/drush cr The first job is the one detailed in the previous post, except that it has now been moved from script to before-script to work better with the other pieces. The next component is composer. Drupal is built using composer packages. Any action that updates the composer.lock file - adding a new module, deleting a module, updating packages - will require this step. composer install ensures that it installs the exact same versions of the exact same packages as on your other servers. It might be tempting to use composer update instead to get the latest versions, but then you might end up with trying to install something that you haven&#39;t tested elsewhere yet. There are two variants of this job, one for dev servers that installs dev packages and one for production that does not. Secondly, the configuration and databases need to be updated. drush cr rebuilds your caches. There&#39;s one to start this section because it is often necessary for scenarios like a new module. The Drupal cache needs to know when there&#39;s a new module with the code in place, before you get to the config-import that will try to install that module. Otherwise you&#39;ll get an error. With that settled, you can import all the configuration changes, then clear the caches again to ensure the site is now reflecting those changes. Finally, update the site&#39;s database using drush updb. This is sometimes needed with new modules or updated modules that need to change the database schema. If you don&#39;t do this, you can end up with errors about missing columns in tables. The final job only clears the caches, without any of the other changes. The reason for these separations will become more clear in the site&#39;s jobs that extend from these. Site Jobs These are the jobs to execute each job on the relevant server, found in the .gitlab-ci.yml file for the project. ## Deploy Jobs ## ### Deploys to dev server only when changes are made to the dev branch ### deploy_dev: extends: - .deploy_template - .drupal_composer - .drupal_config variables: ENVIRONMENT_NAME: Development SERVER_URL: dev.demo.com WEB_ROOT: /opt/www/html tags: - dev only: refs: - dev ### Deploys to staging server only when changes are made to the main branch ### deploy_staging: extends: - .deploy_template - .drupal_composer - .drupal_config variables: ENVIRONMENT_NAME: Staging SERVER_URL: staging.demo.com WEB_ROOT: /opt/www/html tags: - staging only: refs: - main ### Deploys code to production only when main branch and manually triggered ### deploy_prod_1: extends: - .deploy_template - .drupal_composer variables: ENVIRONMENT_NAME: Prod 1 SERVER_URL: prod1.demo.com WEB_ROOT: /opt/www/html tags: - prod1 only: refs: - main when: manual deploy_prod_2: extends: - .deploy_template - .drupal_composer variables: ENVIRONMENT_NAME: Prod 2 SERVER_URL: prod2.demo.com WEB_ROOT: /opt/www/html tags: - prod2 only: refs: - main when: manual #### Deploys config updates to production. These need to wait for both servers to have the composer updated code. #### drupal_install_prod1: extends: .drupal_config variables: ENVIRONMENT_NAME: Production 1 SERVER_URL: prod1.demo.com WEB_ROOT: /opt/www/html tags: - prod1 only: refs: - main needs: [deploy_prod1, deploy_prod2] drupal_install_prod2: extends: .drupal_cache variables: ENVIRONMENT_NAME: Production 2 SERVER_URL: prod2.demo.com WEB_ROOT: /opt/www/html tags: - prod2 only: refs: - main needs: [deploy_prod1, deploy_prod2, drupal_install_prod1] Deploying to dev server can be done in a single job. The before_script deploys the code changes, the script installs from composer, and the after_script imports the configuration. It runs on any change to the dev branch. The idea is that we do work on separate issue branches, then when we need to test on dev, we merge it into the dev branch and it will deploy there. Deploying to staging server is the same, but with different variables for the server details and activating on merges to main instead of to dev. Merges to main should only happen when its a release candidate ready for final testing before deployment. Deploying to production adds a couple more layers to consider. There are two servers load balanced, with one database on a different dedicated server. Composer needs to install the updates on both servers. But the configuration only needs to be imported on one, since it&#39;s the same database - it wouldn&#39;t break anything to do the config on both, but would be an unnecessary extra few minutes of the site being in maintenance mode to run the same job all over again. The second server does still need caches cleared, though, or it might take some time for the changes to be reflected there, which could mean errors in the meantime. The other factor to consider is that the configuration import should not run until the code update has been deployed on both servers. If the order of operations was to install code updates on prod 1, then run config import, it would be trying to make updates on the shared database when only one of the servers has the code ready for the updates. That&#39;s why production can&#39;t collapse to one job per environment like dev and staging do. The resulting order of operation that you need is: Deploy code changes, with composer install, to production 1. Deploy code changes, with composer install, to production 2. Import configuration changes to production 1. Clear the caches on production 2.&quot;
    },{
      "url": &quot;/2023/gitlab-ide/&quot;,
      "title": &quot;GitLab&#39;s New Web IDE&quot;,
      "content": &quot;I discovered a few days ago that GitLab has a new web IDE that is based on Visual Studio Code. The blog about this new editor highlights a few advantages this gives compared to their previous editor, including: Flexible layouts, with the possibility for multiple panes open at once. Drag and drop support in the file browser. Find and replace across all open files. 80% reduction in memory usage. I&#39;ll also add that it&#39;s nice to have consistency between my desktop editor and the web IDE. Whether I&#39;m editing code from the desktop app or the occasional quick fix directly in GitLab from a browser, it is essentially the same experience (aside from things like what plugins are installed). This is similar to what is available on their big rival GitHub. They&#39;ve had a web IDE based on VS Code for a while, which you can access by substituting GitHub.com in the URL of your project with GitHub.dev instead, or by typing the . (period) key while viewing your GitHub project. It&#39;s less obvious that it is there compared to GitLab which has always had a big button for their web IDE, but works great once you know about it. See documentation about GitHub&#39;s editor. Tangentially, this is also a big vote of confidence in VS Code. GitHub being built on VS Code isn&#39;t surprising as they&#39;re both Microsoft products. But GitLab also using it, as well as other services like GitPod, is a significant indicator that it is becoming the industry default editor.&quot;
    },{
      "url": &quot;/2023/importing-multiple-paragraphs/&quot;,
      "title": &quot;Drupal: Importing Multiple Paragraphs&quot;,
      "content": &quot;A few months ago I was part of migrating content from an old Drupal 7 website to a new Drupal 9 website. Much about the data structures were similar, but some components were not. The biggest one was a field on the old site using field collections while the new site needed the newer paragraphs module instead. Each field collection/paragraph contains two fields, one for a title and one for a longer description. That already gives us one complication in a migration strategy, because those (collection or paragraph) are separate entities which are linked to the main node, so it is a bit harder to create those and link them in one smooth import. A second complication comes from it being possible to have multiple field collections on the old site and also multiple paragraphs on the new site, so we&#39;ll need to navigate collapsing multiple entities with two fields each alongside the related node into one export, then import that to again have multiple paragraphs with two fields each connected to the correct node. This post may not be as precise as I&#39;d usually like because it was a while ago and I can&#39;t easily recreate all of it, but hopefully it will be close enough that a Drupal admin would be able to fill in the gaps sufficiently. Drupal 7 Export Step one is setting up the export from the old site. This relies on the views_data_export module to help generate a csv export. The view should be based on the main content type node that you&#39;ll be exporting. I added a relationship between the node and the field collection. I added each of the field collection&#39;s fields to the Fields of the view, to be included in the export. There were other fields to also consider in the export, as well as other options around formatting, but those are fairly standard views behaviour and not related to this problem, so I won&#39;t detail them. The end result should be that you can get multiple rows for each node with field collection combination, e.g.: Node Title,Field Collection Title,Field Collection Description Node 1,Field 1,Description 1 Node 1,Field 2,Description 2 Node 2,Field 3,Description 3 Data Cleanup in Excel I now needed to collapse each node into one line. I did this somewhat manually in Excel, copying field data into each row but separated by double colons (::). You can choose any character separator as long as it won&#39;t occur naturally within the field, so you probably don&#39;t want as simple as a single : or ;. In my scenario we were dealing with about 1000 nodes, about 100 of which had multiple field collections from this field. If you have much more than that, you probably want to script something to combine the rows for you. Node Title,Field Collection Title,Field Collection Description Node 1,Field 1::Field 2,Description 1::Description 2 Node 2,Field 3,Description 3 Note: there may be other complications here like what text encoding to use, depending on the characters in your data. Similarly you may have to navigate whether each column should be separated by a comma (,) or something else. I won&#39;t get into that here. Drupal 9 Feeds Configuration The data is now ready to get into the new site. This import will be handled by the feeds module, with help from feeds_tamper. I created a new feeds type from Structure -&amp;gt; Feed types -&amp;gt; Add feed type. Fill in the options. The most important is the second group, the basic options of how to grab the data and what to do with it: Fetcher: Upload file Parser: CSV Processor: Node Content type: [Your content type, in this case it was Resources] On the mapping screen, I added a straightforward mapping between the Field Collection Title in your csv file and a temporary text column (allowing multiple values) created in your content type of the new site. After this, the import will be able to bring in data and put it in new nodes of the content type, but the text in the temporary fields are still separated by the :: if there are multiple. Now I went over to the Tamper tab and found the field you need to split up and added an &amp;quot;Explode&amp;quot; tamper plugin for that field, specifying the string separator and a label. I repeated this on each these temporary multiple fields. This will now leave us with multiple entries in the temporary field directly on the node. We still need to turn those into paragraphs and associate those paragraphs with the node, which is where we run out of options within Feeds to handle it on its own. Custom Module The final piece is to convert the temporary text fields into multiple associated paragraphs instead. This will be handled with a custom module. I&#39;ve created a generic demo of this available in my GitHub. This is the main PHP file. &amp;lt;?php use Drupal&#92;&#92;Core&#92;&#92;Entity&#92;&#92;EntityInterface; use Drupal&#92;&#92;paragraphs&#92;&#92;Entity&#92;&#92;Paragraph; use Drupal&#92;&#92;user&#92;&#92;Entity&#92;&#92;User; use Drupal&#92;&#92;node&#92;&#92;Entity&#92;&#92;Node; /** * Implements hook_ENTITY_TYPE_insert() for &#39;nodes&#39; * When a node is created, with content in the temporary field, create the associated paragraph instead */ function paragraph_split_node_insert(EntityInterface $node) { //If node content type is node_type if ($node-&gt;getType() == &#39;node_type&#39;) { if (!$node-&gt;get(&#39;field_title&#39;)-&gt;isEmpty) { $new_paragraphs = array(); $count = count($node-&gt;get(&#39;field_title&#39;)); for ($k = 0; $k &amp;lt; $count; $k++) { //This is needed to handle when there is a title without a matching description $desc = (empty($node-&gt;get(&#39;field_description&#39;)[$k])) ? &#39;&#39; : $node-&gt;get(&#39;field_description&#39;)[$k]-&gt;getString(); //Create new paragraph entity $paragraph = Paragraph::create([ &#39;type&#39; =&gt; &#39;paragraph_type&#39;, &#39;field_title&#39; =&gt; $node-&gt;get(&#39;field_title&#39;)[$k], &#39;field_description&#39; =&gt; $desc, &#39;uid&#39; =&gt; 1, ]); $paragraph-&gt;save(); //Add the new paragraph to the array array_push($new_paragraphs,array( &#39;target_id&#39; =&gt; $paragraph-&gt;id(), &#39;target_revision_id&#39; =&gt; $paragraph-&gt;getRevisionId(), )); } //Overrides if there was already one in place, which there shouldn&#39;t be in this context of an import $node-&gt;field_paragraph_reference = $new_paragraphs; } $node-&gt;save(); } } The module logic is relatively straightforward: Hook is on the node being created but it will only continue on a specific node type. Everything else is irrelevant in this scenario. If the temporary title field is provided, continue. Otherwise there&#39;s no extra handling needed. In this context there was never a description without a title, so this was safe. You may need more handling for those scenarios. Loop on every match found, since there can be multiple in the temporary title field. If there was a title but not a description - which did happen in our context, unlike the reverse - then the description, replace with a blank string. Create a new paragraph entity and save it. Create a new entry in the field of the node that links to the paragraph, with the paragraph&#39;s ID and revision ID. Save the node when done. Note: another approach here that would also involve custom module code would be to build this handling directly into the feeds import instead of hooking on the node being created. That&#39;s probably a bit more precise to run only in this one context. This hook on node insert does run more often, which is both less efficient and could be extra safety in case somebody accidentally put data in those temporary fields manually instead of creating the paragraphs. In any case, this was a quicker one for us to prepare, which was the priority with getting a one-time import done. When the module is ready (including an info file), enable it and give it a test run.&quot;
    },{
      "url": &quot;/2023/drupal-disable-pw-reset/&quot;,
      "title": &quot;Drupal: Disable Password Reset&quot;,
      "content": &quot;In one particular site, we chose to block the user registration and user password reset options. The site is using SSO login anyway, so this would really only impact our development team&#39;s superuser admin account (user 1) that is typically blocked outside of an emergency, so this seals up a possible attack vector with no user impact. User registration can also be blocked with site configuration, so I wouldn&#39;t have bothered with custom code for only that part. But since I was doing it for the password reset anyway, I decided to go ahead with the extra layer of protection on the registration route as well. I achieved this with a simple custom module which you can view in full in my GitHub. Here&#39;s the key part, blocking access to the routes: protected function alterRoutes(RouteCollection $collection) { // Always deny access to unwanted routes. $disallow_routes = [ &#39;user.register&#39;, &#39;user.pass&#39;, ]; foreach ($disallow_routes as $disallow_route) { if ($route = $collection-&gt;get($disallow_route)) { $route-&gt;setRequirement(&#39;_access&#39;, &#39;FALSE&#39;); } } }&quot;
    },{
      "url": &quot;/2023/brandwood/&quot;,
      "title": &quot;Brandwood A11y Checker&quot;,
      "content": &quot;Today I learned about an interesting tool: the Brandwood Accessibility Checker. There are lots of tools out there for testing colour contrast and whether it meets the necessary levels to meet accessibility requirements. But in most of those, you enter one colour for the foreground and one for the background and it gives you the contrast ratio and whether that passes. This one goes a step further and lets you see contrast of text in front of images. This is great for scenarios like a homepage slideshow (slideshows often aren&#39;t great for accessibility in other ways, but we&#39;ll ignore that for now) where there is text of a title that shows up layered in front of an image. Whether the text is readable with enough contrast might depend on what part of the image the text appears over, so this tool lets you specify your image, your text colour, and lets you move the text around to see what the contrast is against the image at that specific point. Note that depending on your mobile responsiveness handling and the possibility for other tools like browsers increasing or decreasing the font size, this might still not be a perfect conclusion. The simplest solution when possible is to put the text above or below the image instead of layered on top of it. But if you need to do it, or if you&#39;re looking for a tool to help with a more fixed design scenario - like a PDF document or printed poster - this is a nice tool to have in your belt.&quot;
    },{
      "url": &quot;/2023/cognitive-ease/&quot;,
      "title": &quot;A11yTalks: Cognitive Ease&quot;,
      "content": &quot;A11yTalks had a great presentation this week about designing for cognitive load. It has a lot of valuable insights about ways that we can do much better with website design that goes beyond simply meeting the WCAG checklists. Check it out below: As a tangent, I know this site has a lot of weaknesses in its semantic design and I&#39;ve largely ignored them because this is mostly my little blog to myself that few if any will see, but I know I should look into forking from this theme to create a more accessible version.&quot;
    },{
      "url": &quot;/2023/mastodon-apps-android/&quot;,
      "title": &quot;Mastodon Apps on Android&quot;,
      "content": &quot;I&#39;ve been enjoying my time on Mastodon since the recent migration from Twitter. Of course, I&#39;m far from the only one to make this migration, and that includes a lot of developers, so we&#39;re starting to see a lot of better apps for Mastodon and the wider fediverse popping up. That has led me to an experience that has been very rare since about 2012: testing out and researching a variety of independent apps to meet a need. For years I&#39;ve mostly just used whatever the official app is for whatever service (including Twitter), with really the only recent exception being when I spent some time testing out alternative email apps. Before writing this post, I put the question out on my Mastodon to see what factors others have considered in picking their preferred Android app. I&#39;ll incorporate a couple of those answers here, but first, one shared a very helpful spreadsheet breaking down all the options (not only Android). This is great if you need a high level overview of features available. This post will try to narrow down some of what matters most to me and to others when I put the question out for feedback. Mastodon (official app) As I&#39;m sure with most people, I first tried the official app simply called Mastodon. This was the time of the great migration and a significant portion of posts I saw were advice about Mastodon, including a lot of posts saying not to use the official app. But it&#39;s not as bad as that makes it sound. It has Trending topics. It has home and local feeds. It&#39;s a fairly clean and friendly design. But yes, as many point out, it is missing several other features that you might want. Check the spreadsheet linked above for the full list, but here are a few that that matter to me: You can&#39;t read threads in continuity, which makes it prone to reading posts without recognizing there&#39;s context. There&#39;s no federated timeline. You can&#39;t manage lists at all. You can&#39;t see ALT text, which does matter even for those of us who are sighted because descriptions can still help us identify things, and it might help us identify that maybe we don&#39;t want to boost a post that is missing ALT text. It doesn&#39;t support unlisted posts, or scheduled posts, or the delete and redraft option, or simultaneous posting of a thread, or multiple choice selection polls. It&#39;s fair to say that none of those are core features, but there&#39;s enough of them that there&#39;s probably at least one you&#39;d be disappointed to be missing. Tusky When I was seeing posts about not using the official Mastodon app, it was usually tied with the recommendation to use Tusky instead (sometimes Fedilab). I tried that and kept it as my main app for a couple of months. It does a few things very well: Very clean design that feels at home on Android, especially with the truly black theme and options for the main navigation to be either at the top or bottom A good array of options including being able to do most of the things that the official Mastodon app cannot. My question on Mastodon also got this negative response about some of Tusky&#39;s design choices: I’ve been a mobile developer for 15 years (mostly android) and used Tusky before I got my iPhone. It wasn’t my favorite tbh. my gripes with Tusky are all around data and state handling. For instance the UI has very few “in progress” states, so for instance boosting a toot can momentarily show the button in the wrong state if the request is slow. Also the caching for viewed toots is very rudimentary, so lots of toots (in conversations esp.) are needlessly reloaded while I stare at a blank screen. Source: https://tech.lgbt/@em_ad/109855207946318853 Fedilab I then tried Fedilab, the next I saw recommended a lot. It has a small one time cost for the app, but if you&#39;re already settled on using Mastodon, it will probably be worth it to you to try it out. Positives As the spreadsheet backs up, it is the most featured app by a significant margin. I&#39;ll highlight a couple big selling points that the others don&#39;t have: It allows following other instances, which is a great way to monitor niche topic communities even if you&#39;re not in them. For example, there is an instance for Drupal developers at drupal.community. I don&#39;t really want that to be my primary home, since Drupal is a relatively small portion of what I post. But I do want to easily keep an eye on conversations happening there since they will have a high rate of being relevant to me. It allows following Twitter accounts via Nitter, although there are some drawbacks to how it is implemented. You can&#39;t edit the list of who you are following easily. There&#39;s no follow/unfollow buttons. There isn&#39;t even a way to view the list of everyone you currently follow. So you need to maintain a list somewhere else, in the right format (separated by a space, not a new line) so that you can copy and paste it in each time you want to add or delete one. It also may have a limit on how many total it can follow? I&#39;m not sure about that, but I found that when I tried to add more, it broke, with a generic error that didn&#39;t explain anything. And finally, a lot of the time the links it shows for a quote tweet simply don&#39;t work. I often try clicking the link several times in a row to see what is being quoted. It allows creating simultaneous posts in a thread, which none of the others do. I always appreciate this feature as I don&#39;t like getting caught having written up half a thread. This is even more true on Mastodon than on Twitter because it is best practice to keep all posts in a thread after the first as unlisted. But when there are long gaps between the start and end of the thread, I might end up with nobody even seeing that you have added more to it. Being able to dump the whole thread at once optimizes that best practice. It is one of the few apps that is built for all Fediverse services, not only Mastodon. That means if I get a Pixelfed account some day, I could do it from the same app. A Mastodon user highlighted another feature that isn&#39;t a big deal to me but might be to you: I read the posts from oldest to newest. At the moment Fedilab seems to work best for this reading order, as it can continuously update the timeline so there are no breaks. Tusky is second best, you can set the &amp;quot;load more&amp;quot; button to fill the timeline above the break, not below. Still no app seems to allow reverse order timeline (oldest posts first), which was a deal breaker for me when selecting my Twitter app. Source: https://mastodonsweden.se/@kallekn/109856154392108362 Negatives Most of my negatives have to do with the user interface and I&#39;ll focus on two: I don&#39;t like the pill button design. It feels out of place on Android. It feels dated, like a few years ago when there was a lot less consistency in app design on Android, while now most comply with the recommended design language. I don&#39;t like the fragmented placement of timelines. In the bottom toolbar you can have the home, local, federated, notifications. That&#39;s great. Then in the top go any other instances you follow, lists, and Nitter, in this weird ticker style interface. Then in the sidebar under the profile photo mostly makes sense but also has Trending in there. I&#39;d love to have these consolidated to one navigation area, in the bottom, where I can pick the four that are highest priority to stay with easy access, then the rest in an easy to access overflow menu. There is an option to combine the top and bottom, but only to put them all in that weird top ticker style, not in the bottom as I would want. And there&#39;s no option to add Trending there. Megalodon This is the newest discovery for me. It is not heavily featured - a little more than the official Mastodon app, less than the others - but it looks fantastic, probably my favourite design of the bunch. Conclusion For now, I&#39;m sticking with Fedilab. I really like some of those features it does better than the rest, enough that I&#39;m willing to sacrifice some annoyance at the design style. But I am going to keep the others around and check in on them occasionally. It&#39;s early enough in the life of most of these that they may catch up in the features while maintaining a nicer design.&quot;
    },{
      "url": &quot;/2023/include-default-view-custom-module/&quot;,
      "title": &quot;Drupal: Include Default View in Module&quot;,
      "content": &quot;You may be working on a Drupal module and want to include a default view. Install This is actually quite simple: put the configuration file in the config/install folder within the module, with the proper configuration name, and it will install. This would be the full configuration file, the same as you would generate using the configuration sync tools. You can write this file from scratch, especially if they&#39;re hidden options. If it is something that has a user interface, you can do what I tend to do: determine all your preferred settings and test it out within the interface, then export the configuration from either the UI (/admin/config/development/configuration/single/export) or drush (drush config-export but that does all configuration, not only the specific one you want). Uninstall What about uninstalling the configuration you&#39;ve installed with the module? This will depend on the context of your module, but you may want to also uninstall this configuration when the module is uninstalled. The configuration might no longer make sense without the module, or you might create conflicts if you installed it once, uninstalled the module but left the configuration, then tried to install it again. To do that, add a few lines like this to the uninstall hook of the .module file, for example with a config file named webform.webform.event_registration: /** * Implements hook_uninstall * */ function module_name_uninstall() { if (!empty(&#92;&#92;Drupal::configFactory()-&gt;getEditable(&#39;webform.webform.event_registration&#39;))) { &#92;&#92;Drupal::configFactory()-&gt;getEditable(&#39;webform.webform.event_registration&#39;)-&gt;delete(); } }&quot;
    },{
      "url": &quot;/2023/algorithms/&quot;,
      "title": &quot;On Algorithms and Section 230&quot;,
      "content": &quot;For those paying attention, there are a couple big cases in front of the US Supreme Court that could shape the Internet as we know it. I&#39;m Canadian, but the (English) Internet is largely determined by what is allowed in the U.S., so yes, I&#39;m still talking about it. Context Essentially, the plaintiffs want to hold Google legally responsible that its recommendation algorithm sometimes promotes ISIS recruitment videos. The concept could apply to other harmful content as well. There is no question that Google is protected in the harmful videos existing on YouTube. It would be impossible for the modern Internet to exist if every piece of content had to be moderated and considered by a team of lawyers before it could go online. That is clearly covered by section 230. It&#39;s the recommendation algorithm that complicates things, because that gives an impression of an endorsement from Google. Google claims they are protected under section 230, which says they are not responsible for content posted by users and the algorithm&#39;s results is effecctively user-generated. The plaintiffs say that Google is not protected because the algorithm for recommendations means Google is acting as a publisher. Legal Eagle provides a good breakdown on Nebula or (appropriately) YouTube. He does a great job explaining why it would be terrible to remove section 230 entirely. The Internet would divide into either no moderation or extremely strict and expensive moderation. So let&#39;s rule out that extreme immediately. But here&#39;s where it gets really interesting to me. If not the status quo, and not the opposite extreme, what else is possible in between? Another distinction he mentions that SCOTUS may try to make is that between recommendation engines and search results, allowing the latter but making the platform provider responsible for the former. He says this would be less of a disaster, but there isn&#39;t that big of a philosophical difference between the two, so it might be hard to hold up. Personalization Another distinction even more interesting to me, that he doesn&#39;t mention, is personalized recommendations and search result algorithms. The Impact on the Business Model This might seem like a smaller change but it hits at the heart of big tech. The basic business of most big tech is this: They make most of their money by selling ads Ads are more effective if they are personalized In order to personalize, they need you to give them lots of data about yourself The best way for them to get you to give up lots of data is for them to keep feeding you things that you want to engage with If the recommendations and/or search results were not personalized, you probably spend less time on their service and give them less data. They can still sell ads, but they won&#39;t be as effective and they make less money. The big established platforms could still make a profit, I&#39;m sure, but it would be a radical shake up of the entire business model. The Impact on Privacy In terms of privacy, this would be good for users. There would be a lot less incentive for big tech to know everything about us, which could result in unintended consequences like security breaches. The Impact on AI Research In terms of AI, it would significantly slow down development. AI needs lots of data, which big tech already has. If we stop giving as much data, there&#39;s a lot less training material. Maybe at this point it wouldn&#39;t matter much, since they&#39;re already collecting all the data. Hate and Progress Movements What does it mean in terms of social consequences, which is an important question not only because that&#39;s related to the cause of the lawsuit in the first place, but also because isn&#39;t it really always the big question? If recommendations and search results were not personalized, they could still be weighted in other ways, like prioritizing content that already has lots of interaction or newer content. It just wouldn&#39;t be able to prioritize and give different results based on what the algorithm has determined that you individually will engage with. That means that everyone sees the same results from the same input. We would lose a lot of the &amp;quot;echo chambers&amp;quot; where you only see things that you want to see, at least not without consciously entering that echo chamber (searching a hashtag, deciding who to follow, etc.) There wouldn&#39;t be an algorithmic push to extremes. This applies to all extremes, not just social/political ones. We&#39;d see a lot of content that is broadly palatable to almost everyone. Think of what network TV was like 20 years ago, when each household had one TV so if you wanted viewership you wanted a show that the entire family could watch together. If you watch some of that content now, it will mostly seem boring, because you&#39;re used to media that is much more niche targeted to your interests. The core argument in favour of this kind of approach is that algorithms stop promoting hatred. It would break the very successful business model of right wing grifters who make lots of money by generating lots of engagement spewing hatred and fear. That certainly sounds good. But that&#39;s not the only group that would be impacted. It would also break the reach of anybody else who doesn&#39;t already have a critical mass of followers, like activists. You wouldn&#39;t hear from those profiting from spreading transphobic lies, but you also wouldn&#39;t hear a lot of important work from trans people telling their stories. You&#39;d see less of the crafted manipulation of authoritarians but you&#39;d also see a lot less of those fighting back. Algorithmic social media is often step one in organizing resistance and a big part of that is because in order to keep you there, they give you something that&#39;s a little more of what they think you want. So if you express a little interest in anti-racism work, it&#39;s going to help you find more anti-racism, which is great. This is the area that is the most complicated. Do you consider that a fair trade, even if it were practically possible? And it is also ideological, not only practical: is that a level of regulation that you want your governments to be able to exert - not stifling free speech, but stifling reach? Smaller Sites These kinds of proposals will have 95% of the conversation focused on big tech, but if the rule is established for YouTube, it would apply for everybody else, too. My day job is for a university library, primarily the website. There is currently no recommendation interface. There are search results, which are not at all personalized, but I&#39;ve had the serious thought that maybe they should be in a very minimal way. There&#39;s a lot of content on the site and a lot of it using similar keywords around databases and resources and citations, so if you use keywords like that, you get a bunch of different pages spanning all of the subjects, likely burying the results most relevant to you. Even one piece of personalized information would really improve search results: what subjects are you studying? Wouldn&#39;t the search be better if we knew that and prioritized responses accordingly, giving computer science results first if you&#39;re a computer science student? That&#39;s a small example. It doesn&#39;t require big machine learning algorithms monitoring your every move on the Internet, but it does still require personalization. Maybe that&#39;s the distinction instead: allow personalized results based on information explicitly given by the user, but don&#39;t allow engagement data mining. But who defines what is engagement data mining and what is essential information to provide the service, or engagement data mining for telemetry, for the developers to know what improvements they need to maks? My example may have a clearer line, but that certainly won&#39;t always be the case. Conclusion There are a lot of complicated factors here and I&#39;m thankful for channels like Legal Eagle that are helping to unpack the potential consequences. Let&#39;s hope SCOTUS thinks this through carefully and rules in the best interests of a healthy role for the Internet in society.&quot;
    },{
      "url": &quot;/2023/drupal-recycle-bin/&quot;,
      "title": &quot;Drupal: Recycle Bin&quot;,
      "content": &quot;Drupal does not come with any kind of recycle bin / trash system out of the box, unlike the other technologies I have worked with most (WordPress, SharePoint). There were a few modules available at the time I looked into this (admittedly over a year ago now), but they were all rough, either old or barely developed. None were reliable enough to count on. In this post I’ll describe a simple halfway approach that did not require any extra code. There is a demo of this available on my GitHub, with a GitPod container to test it out. Permissions: Only Admins Can Delete Check permissions for each content type to confirm that only admins can delete anything. Everybody else will only be able to use the flag field we&#39;re about to create to request deletion, not directly delete anything. Go to the node permissions screen at /admin/people/permissions. Look for the permissions of each content type that are &amp;quot;Delete own content&amp;quot; or &amp;quot;Delete any content.&amp;quot; For the purposes of this demo, only the admin users should have these permissions. Your permission structure may be more complicated. Field: Flag for Delete Proceed to the Manage Fields page of the first content type that you want to have this feature on. For example, if my content type is recyclable_content_type, I would go to /admin/structure/types/manage/recyclable_content_type_/fields. Create a boolean field. Limit the field to only allow one value at a time. The final screen for creating the field allows you to add more options that are mostly about helping your content creators understand what this is doing. I&#39;ll add some help text as well as change the On and Off labels to be more clear of what they mean. You now have the field on this content type ready to use. You may also want to go into &amp;quot;Manage form display&amp;quot; and &amp;quot;Manage display&amp;quot; to customize how this appears for your users. In my case, on the form I put it inside a Details sidebar paired with the published option. I then used a module like condition_field to only show that option when the node is already marked as unpublished, so that somebody cannot accidentally mark something for deletion without unpublishing it first. Finally, on the display I hid it entirely because the public should never see that field - it is only for back-end processes. If you have more than one content type you want to deploy this feature on, go to the next content type and add a field again. This time, select from the dropdown of existing fields instead of creating a new one. The final steps for help text, form display, and public display remain the same. View: Ready for Deletion Finally, you&#39;re going to want a way that admins can review the nodes flagged for delete and go ahead with permanently deleting after the set amount of time. This is a great problem to be solved by views. You can either create a new view or add a new display to an existing view. I&#39;m going to start by building on top of the default admin view named &amp;quot;Content&amp;quot; since I want something that is basically the same but with extra filters. This is at /admin/structure/views/view/content. Either add a new page from the Add button, or duplicate the existing page with the Duplicate Page option in the dropdown near the right side of the screen. If there was only one display which is also the default, the result would be essentially the same. I put it in the menu under Content of the Admin menu, and with the path /admin/content/delete. Flag for Delete filter Add a filter to the view, overriding this page only, on the Flag for Delete field. Published filter The view I duplicated from already had a Published filter, but it was exposed to allow users to select. So I&#39;m going to edit that one to only show unpublished content. I only want unpublished content, because if somebody has flagged for delete but it is still published, that suggests they might have made a mistake. Date Updated filter Finally, in my case, I also wanted to filter on the date updated, so that only content that hasn&#39;t been updated within the past 30 days would show up. Anything less than that suggests that the content creator may be having second thoughts and isn&#39;t really ready to delete it yet. In your context, you may not want this delay at all, or you may want a much longer delay. Now admins can periodically check and bulk delete any content that is flagged for delete, unpublished, and last updated more than 30 days ago. You can also do any of the usual views changes from here as you desire, including adding views, changing text to show when there are no results, adding CSS classes, etc.&quot;
    },{
      "url": &quot;/2023/paragraphs-sidebar/&quot;,
      "title": &quot;Drupal: Paragraphs Sidebar&quot;,
      "content": &quot;Suppose you want to be able to specify a contact person for most nodes of content on a site, which will then show up in the sidebar with a label and some information about the contact person. Each node could have zero contacts, could have one contact, or could have several contacts. I solved this using a combination of the Paragraph module, the Profile module, content type fields, and views. A demo of this is available in my GitHub that you can load up using GitPod. Create the paragraph and its fields First, create the paragraph and its corresponding fields. In my case, I wanted two fields: a label that will be displayed, and a user reference to connect to a staff user account. I created the paragraph type and called it “Contact.” Next, add a field for the label, as a text select to choose from a few pre-defined options including &amp;quot;Primary Contact,&amp;quot; &amp;quot;Secondary Contact,&amp;quot; etc. Only allow one label per paragraph, which is different from where we&#39;ll later allow multiple paragraphs per node. Finally, add a field for the user reference. Set this up to show up as an autocomplete, filtering to only show those who are of the Staff role. Assign to a content type Add a field on the content type that references this paragraph. The field type is going to be an entity reference revision. In this case, since it&#39;s going to display it in the sidebar using a view, also remove it from the display for the content type. I won&#39;t get into details of designing the page on this post. This step can be repeated to add the field to multiple content types, so that you can have the consistent design on all of them. Create the profile For the sake of this demo, I&#39;ll only add a couple of fields to the profile - a name, a phone number, and an email - and I won&#39;t do any of the design of the profile page, just creating the fields for the purpose of showing it in the sidebar of the content. Create sidebar view That&#39;s all the underlying structure. We can now proceed to creating the view itself. This will be a view of content, because while it will be showing paragraphs, it will be showing those paragraphs based on the content being viewed. It will be a block, since we&#39;re going to want blocks that can be placed in theme block regions. Next, make a contextual filter so it will only show content related to the current content on display. This will be on the node ID of the page being viewed, so that only paragraphs related to the current node will come up as results. Create a filter for content ID and set the default to be &amp;quot;Content ID from URL.&amp;quot; Now we need to create the link between the page being viewed and its contact paragraphs. This can be done with the relationships functionality of views. Under Relationships, click on Add, then select &amp;quot;Paragraph referenced from field_contact.&amp;quot; We then need two more relationships: From the contact field of the paragraph back to the user: &amp;quot;User referenced from field_contact_user.&amp;quot; From the user to the profile, which will have the majority of the fields we want to display: &amp;quot;Profile.&amp;quot; The last setting we&#39;ll change in the Advanced section is near the bottom: set &amp;quot;Hide block if the view output is empty&amp;quot; to Yes. We don&#39;t want it to show up at all if there isn&#39;t a contact for the current content. Now that we&#39;ve built the complexities of the query, we can return to the more common configuration in the left pane to determine what is shown. I set: Display for the view to an unformatted list of fields. The fields are the contact name, the contact label, the phone number, and the email address, all from the associated profile. Each field is set to be hidden if there is no result, in case a profile is created with some information but not others, e.g. somebody offers their email but doesn&#39;t want to share their phone. You can also add more styling around how these get displayed (labels, links, etc). Filter on the paragraph type to be &amp;quot;Contact,&amp;quot; on the paragraph to be published, and for the user to be active. This helps avoid unexpected results when a user is later blocked, or somehow another paragraph type is associated to the content. Sort by the contact label and then by contact name, which will be more intuitive to users than sorting on something else like authored date. We can go back to the settings on the Unformmated list to group results by the label, so that the label will show up as a header. If you want more precise control over what HTML tags are used for which elements, semantic views is another great module. Place block view in sidebar Finally, add the block to show in that sidebar for all relevant content types. Of course this will depend on your theme having a sidebar region, but most have at least one. Final result Here&#39;s an example of what all of this looks like, at its most basic level without any extra CSS styles added:&quot;
    },{
      "url": &quot;/2023/drupal-openai/&quot;,
      "title": &quot;Drupal: OpenAI Module&quot;,
      "content": &quot;AI, or more specifically large language models (LMMs) are all the rage these days, so it shouldn&#39;t come as too much of a surprise that there is now a Drupal module that integrates OpenAI. It is only an alpha release but includes several submodules to leverage OpenAI tools within a Drupal site in various ways: Generate content for you in any content type Speech to text Interaction with the ChatGPT prompt endpoint A button in CKEditor to send a prompt to OpenAI and get text back Assistance in the content editing process, such as adjusting your tone and summarizing body text Analyzes logs to summarize likely problems I haven&#39;t given it a try yet, mainly because I do not have a paid OpenAI API key and I assume I cannot use this without one, but it definitely could be worthwhile in certain Drupal site contexts.&quot;
    },{
      "url": &quot;/2023/descriptive-alt/&quot;,
      "title": &quot;Accessibility: Level of Descriptiveness&quot;,
      "content": &quot;There&#39;s a tension in discussing accessibility for screen reader users that comes up sometimes. Most recently and perhaps most commonly is about ALT text, but it can also apply to other things, too. The main conflict is between: You should provide lots of detail in order to make sure a blind user has access to as much detail as a sighted user can. If you&#39;ve got a picture of a cute cat, you provide all the details about the cat: colour, size, facial expression, posture in the shot, anything that a sighted user would be able to see. Or: You should provide only the core details so that the screen reader user can get to the important point as fast as possible. It might be tempting, especially if you&#39;re in the first camp, to frame this as something like equality vs efficiency. But it&#39;s really two different definitions of equality: is it equality in detail or equality in function. Another way to think about it is in terms of cognitive processing time. When an image has more details, it doesn&#39;t really add to the time that a sighted user needs to process it all, unless it&#39;s a Where&#39;s Waldo kind of scenario that is intentionally hiding details in a way that is not available at a glance. On the other hand, adding more text description that needs to be spoken out loud is a linear function. If you have twice as much descriptive content, that&#39;s twice as much time to listen to it. Maybe you structure it in such a way that they can get the basics out of the way and then have the longer description in a different spot that they can easily skip over, giving them a choice, but that&#39;s not always going to be easy. So what&#39;s the answer? How much description should be added for screenreader users? Like many things in life, I think the answer is &amp;quot;it depends.&amp;quot; Here&#39;s one scenario: you&#39;re posting on Mastodon about how cute your cat is. In that kind of scenario, the point is the details. You probably should have a description of the cat and what makes it cute. If somebody is choosing to listen to the description of the image attached to a post about how cute your cat is, it&#39;s probably fair to assume that they want the details even if that takes a minute or two. A chart of data may vary on the context. In a quick casual social media post, it&#39;s probably better to get to the point. Imagine a chart that shows the increase of CO2 emissions over time, spanning several years with lots of data points, but the clear purpose of the chart is to show how badly things are increased. Somebody probably doesn&#39;t want to be scrolling social media and get bogged down in a 5 minute narration of data points naming every year plus exact numbers for that year that all blend together and are hard to interpret. That&#39;s probably not what they&#39;re there for. But what if it&#39;s the same chart but as an academic resource? If somebody is doing research, they might need to know the exact data points, so the ALT text (or corresponding text nearby) needs to be quite detailed. Side note: I once heard about efforts to aid screen reader users encountering a graph to get the point better than a list of numbers, that relies on sounds. If the data is going up, it makes a higher pitch sound. If the data is going down, it makes a lower pitch sound. It will be interesting to see if something like that ever becomes standardized enough to work. This exact same distinction can also apply to other context like menu items: do you want to add extensive aria-labels to force verbose experiences on blind users? The title attribute is a nice balance in that example. At least in Jaws, there is a user setting for whether to read out title text. This allows developers to empower the blind users to decide for themselves whether they want the verbose description or not. And it&#39;s always better to empower users to have control over their own experience whenever possible. Provide title attributes and if they want it, they&#39;ll have it; if they don&#39;t want it, it won&#39;t slow them down. In any case, the main point is that ALT text and accessibility in general is not always an obvious formula. Sometimes there are tensions to navigate. The most important part is that you are thinking about it, and when possible listening to the real experiences of users to steadily improve how to navigate those tensions.&quot;
    },{
      "url": &quot;/2023/drupal-subprojects/&quot;,
      "title": &quot;Composer: Options for a Submodule&quot;,
      "content": &quot;Suppose you want to tie in another private git package, such as something that is not listed on the Drupal directory - as part of your standard build process. Here are two approaches to do that: Composer Drupal packages are managed with a composer file. Composer files can pull from many sources. Most commonly in the case of a Drupal project, that means pulling in modules and themes from the Drupal directory. But it also includes the ability to pull in projects as zip files or a git project hosted elsewhere, even a private one. Here&#39;s how: Create the module that you want to pull in and give it a composer.json file. Here&#39;s an example of that file: { &#92;&quot;name&#92;&quot;: &#92;&quot;ryan-robinson/demo-module&#92;&quot;, &#92;&quot;type&#92;&quot;: &#92;&quot;drupal-module&#92;&quot;, &#92;&quot;description&#92;&quot;: &#92;&quot;Demo module&#92;&quot;, &#92;&quot;keywords&#92;&quot;: [ &#92;&quot;Drupal&#92;&quot;, &#92;&quot;module&#92;&quot; ], &#92;&quot;license&#92;&quot;: &#92;&quot;GPL-2.0+&#92;&quot;, &#92;&quot;require&#92;&quot;: { &#92;&quot;drupal/core&#92;&quot;: &#92;&quot;^9.0 || ^10&#92;&quot; }, &#92;&quot;config&#92;&quot;: { &#92;&quot;platform&#92;&quot;: { &#92;&quot;php&#92;&quot;: &#92;&quot;8.1&#92;&quot; } } } Add two things to the composer.json of your Drupal project: One, you need to define the source for where to get the project: &#92;&quot;repositories&#92;&quot;: [ { &#92;&quot;type&#92;&quot;: &#92;&quot;vcs&#92;&quot;, &#92;&quot;url&#92;&quot;: &#92;&quot;git@gitlab.com:ryan-robinson/demo-module.git&#92;&quot;, &#92;&quot;branch&#92;&quot;: &#92;&quot;main&#92;&quot; }, ] Second, you need to add it to the requirements list to pull in: &#92;&quot;require&#92;&quot;: { &#92;&quot;ryan-robinson/demo-module&#92;&quot;: &#92;&quot;dev-main&#92;&quot; } If the repository is private, you&#39;ll also need an auth.json file with your personal access token. Here&#39;s an example that would work for a GitLab project: { &#92;&quot;gitlab-token&#92;&quot;: { &#92;&quot;gitlab.com&#92;&quot;: &#92;&quot;token value&#92;&quot; } } Git Submodule The other approach is to make it a git submodule. The advantage of the submodule approach is if you want to be able to continue to contribute to that. You can work on the parent project and the submodule at the same time, committing them both separately. Of course that only matters I&#39;d you have permission to do so. The context I&#39;ve found myself using this: when I&#39;m developing a custom Drupal module, I need a Docker devcontainer environment to work within for testing. That&#39;s one git repository. I also need a repository for the module itself, so that it will be ready to share with other Drupal projects (often using the composer technique above). This allows me to load the devcontainer, work on the module, and commit back to the module. This can be done by running the git submodule add command, e.g.: git submodule add [URL to module] This generates a .gitmodules file at the project root that looks something like this: [submodule &amp;quot;web/modules/custom/demo-module&amp;quot;] path = web/modules/custom/demo-module url = [URL to module]&quot;
    },{
      "url": &quot;/2023/future-microblogging/&quot;,
      "title": &quot;Future of Microblogging Social Media&quot;,
      "content": &quot;Twitter, Mastodon, Calckey, Bluesky, Threads... And a whole bunch more that aren&#39;t even worth mentioning in my opinion (I did try a couple of them). It&#39;s an interesting time for social media, especially the microblogging format. Part of what makes it so interesting is that there are real differences in philosophies of what the Internet - and therefore, society at large - should be, not only a difference in which billionaire gets to sell all our data. So, after a particularly tumultous week, I decided now is a good time to check in with how I feel about the current options and where I see things going from here. Twitter Twitter is dying. It may not fully shut down, but there&#39;s no trust of anything you read, there&#39;s lots of harassment, you&#39;re limited in how much you can even use it, there&#39;s lots of security failures and privacy violations, occasionally it goes down for several hours. They&#39;re even now bribing far right influencers to keep people around spewing hate speech so yeah, if you&#39;re still viewing Twitter, or still buying ads on Twitter, you&#39;re directly funding hate speech. It&#39;s on life support and it&#39;s hard to see it lasting for much longer as anything more than Musk&#39;s vanity project. Threads Threads from Meta is the other big corporate player, trying to claim that spot as the default microblogging platform. One big difference is that they want it to be more of a happy fun place that advertisers want to be with minimal moderation efforts required, so they&#39;re downplaying talk of real life issues. I have not personally used it. I don&#39;t use anything owned by Meta. From what I&#39;ve heard, it&#39;s a very rough start in terms of features, but as you expect from Meta&#39;s expertise, it&#39;s easy to get started and it got a huge start from brands, journalists, and influencers - the kinds that drive the network effect - in addition to everybody else. The feed is heavily algorithmic, forcefeeding you a lot of brand content instead of who you follow. Where it gets really interesting is their promise that it will be federated with the ActivityPub protocol, meaning it will not be a complete walled garden. Why are they doing that is hard to say. Maybe they think it will appease regulators. Maybe some engineer who does believe in federation proposed it without really thinking it would be approved and it got through. In any event, it&#39;s great to see. I&#39;ll respond to more of that below. Bluesky Bluesky is the most interesting, somewhere in the middle of the spectrum. It&#39;s federated, sort of. They&#39;re promising their protocol will be available to others. But it isn&#39;t yet. And it&#39;s a unique protocol, not tying in to ActivityPub, the established standard. Which means is still exists on its own, like Twitter, at least until the protocol is done and they manage to sell others on it. In my time there, I&#39;ve struggled to see a unique selling proposition that can keep it going. It has some nice features: the options for content filters, being able to subscribe to someone else&#39;s mute list which can shut down problem accounts faster, making your username a domain you own, some degree of customizing your feeds. But as I learned back in the Google+ days, features alone do not a social network make, especially once the established players start copying the best ideas. The unique selling proposition so far seems to be that it is not as much of a cesspool as Twitter, not as confusing to sign up as Mastodon, and not controlled by an evil megacorp like Threads. That comes from the exclusivity of being invite only at this point: most of the people who are there are more or less on the same page with what they want from this network. But the seams are showing with each time it fails to moderate well - they don&#39;t even have a trust and safety leadership structure - and as the user base steadily grows. As eventually happened with Facebook, exclusivity isn&#39;t much of a selling point when it will get more inclusive over time. Facebook survived because there wasn&#39;t much in the way of competition. But for Bluesky? There&#39;s plenty fighting for this space right now. And more to the point, most of the people I see there I don&#39;t think really want exclusivity forever. A lot are socially conscious &amp;quot;shitposters&amp;quot; and journalists and authors. I was for a while referring to the first group mostly as activists, but I&#39;ve realized that isn&#39;t quite right. They&#39;re more like we saw a lot on Twitter, people who compete for the most snarky response to whatever the story of the day is that we&#39;re all angry about, then move on to the next thing tomorrow. That&#39;s not so much activism, which is usually more focused on long-term efforts toward a specific cause. But in any case, wherever it falls in the activism work vs shitposting spectrum, it can only go so far in a closed bubble. Most of them are still on Twitter and/or on Threads as well for that reason. So how long do they want to spend also focusing efforts on Bluesky? Or do they keep it as their secondary place of rest, to recover from the more frontline work elsewhere? I see that fading out as the exclusivity fades and the lack of trust and safety becomes more obvious, and I&#39;m not sure they&#39;ll have any other selling point to replace it. Bluesky seems to think the selling point is their protocol, but it&#39;s not even done yet and most people don&#39;t care about federation. Those who do are already in the fediverse. Maybe I&#39;m wrong. Maybe they hire a big moderation team that sets the new gold standard for protecting marginalized people. That would certainly be a selling point. But they aren&#39;t doing that so far and that makes me think they don&#39;t have much of a future. The Open Fediverse: Mastodon and Calckey Mastodon is where I have mostly found my home (@Ryan@mstdn.ca). It&#39;s got years of stability in its extensive features - some would argue too many features that make it harder to use. It&#39;s on the ActivityPub protocol so it connects with other fediverse projects, including eventually Threads. The complaints are mostly that it is hard to use, especially signing up and having to pick a server. Calckey is another in the fediverse that addresses some of the concerns many have with Mastodon, like full text search and quote posts. There are lots more that I won&#39;t get into: Pixelfed for photos (think Instagram), Bookwyrm for book reviews (think Goodreads), Lemmy or Kbin for forum-like conversations (Reddit)@. They can all talk to each other. That&#39;s a pretty promising future. Predicting the Future My guess of what happens is we end up like email. A few big corporate giants control most of the email: Google, Microsoft, etc. They offer the service for free in exchange for mining data and showing ads, or in some cases also with a subscription offer that doesn&#39;t show ads for a monthly fee (Microsoft 365 Home includes this). It&#39;s a quality service, with many professionals working on problems like spam prevention and security patching. That&#39;s likely to be Threads the way things are currently going. But it is possible to run your own email server that can send messages to and receive messages from the big ones or other small email servers. It&#39;s not easy. You&#39;ll have to meet some fairly high security standards and spam prevention. But you can. That&#39;s a lot of the smaller cases of people running their own Mastodon or other servers for a few dollars a month. Or you can pay for a professional version of email, like Microsoft 365 Business. There&#39;s no ad scrapings, they&#39;ll take care of most of the security and maintenance, and you just need to pay a few dollars a month and configure it to your liking. That&#39;s a great option if you can afford it. In this comparison, that might be some of the bigger fediverse instances. Most of them operate on a donation basis. You don&#39;t have to support them financially, but somebody does have to or it won&#39;t keep going. I suspect that&#39;s where we get with microblogging, and I&#39;ll consider that a huge win. Threads may be the equivalent of Gmail, that many have. Most people will still opt for giving all their data to a billionaire for the right to look at ads, in exchange for all the ease of use that a company like Meta or Google can provide. But you will have some choice other than that and being a hermit, an option that has been increasingly tempting for me over the last 10 years. Maybe you host your own ActivityPub service. Maybe you pay somebody else. Maybe they implement ads but without the data mining. Maybe you stay on somebody else&#39;s without ever giving back or seeing ads. But I am happy that at least in this future there would be options.&quot;
    },{
      "url": &quot;/2023/role-file/&quot;,
      "title": &quot;Drupal: Add Role Based on File&quot;,
      "content": &quot;A while ago I wrote about, and shared sample code for, adding a role to a user when they log in if their name appears in a file on the server. That was a Drupal 7 sample. I&#39;ve now updated that for Drupal 9 and created a project in my GitHub. It&#39;s relatively straightforward so I won&#39;t provide a line by line breakdown again.&quot;
    },{
      "url": &quot;/2023/church-site-content/&quot;,
      "title": &quot;Church Website Content&quot;,
      "content": &quot;This is not my usual type of content for this site, but I was reminded of it from a Mastodon conversation and wanted to put it somewhere more or less permanent. Before building the latest iteration of my church&#39;s website several years ago, I put out the question on Twitter for what information people want before they would consider visiting. I sent those to the church leadership and still had the document, so I looked it up to summarize those questions again below: Sunday Worship When does the service begin and end? Is there a nursery for infants and toddlers? Something for children and youth? Are children welcome in the service, even if they make noise? Is there adult education? What&#39;s the (usually unspoken) dress code? Is there parking? Bike racks? Transit nearby? Where&#39;s the main entrance? Is there coffee available? Inclusivity Is the building accessible with a wheelchair? Is there assistance for those with hearing impairments? (You could add more specific disabilities) Are women or non-binary people involved in leadership? Could they be a pastor? Do you have a gender neutral washroom? Could an LGBTQ+ person be your pastor? Would your pastor marry a same-sex couple? How well does the racial demographics of the church match the wider community? Other Ministries Is there a youth group? What do they do? How often do they meet? What&#39;s your understanding of youth discipleship? What areas of social justice and community work do you do? What social groups can I join? What about Prayer groups? Or Bible study groups? Rental of Facilities Can I use the space for weddings or concerts? Do I have to be a member? How many does it seat? What does it cost? Is there any discount (or free) for indigenous groups whose land you are on? Theology Most did not want a full statement of faith on every possible question, but they wanted to know key values of the community that shape how the church tackles all those other questions. Are you starting from the love of God or from an inerrant Bible or something else? Is there a denominational affiliation? Other Can I donate online? How do I contact someone and who do I contact for what needs? Can I see a calendar of upcoming events?&quot;
    },{
      "url": &quot;/2024/text-position-underline/&quot;,
      "title": &quot;Text Underline Position&quot;,
      "content": &quot;This one is a quick accessibility CSS tip. I have long been a proponent of the old standard of underlining links, and never underlining anything else. It helps make it clear that it is a link. Add a hover and focus effect to make it more obvious when they are selecting it, too. But it can introduce a problem: for certain reading disabilities, the line being close up against the text can make it hard to read. Even for myself without one of those disabilities, once it was flagged for me I immediately saw it every time I looked at a link. So here&#39;s a quick CSS solution that significantly helps. Use text-underline-position: under to make links with underlines easier to read. It moves the underline down to the bottom of the hanging letters (g, y, j) rather than right up tight against the bottom of other letters and cutting right through the middle of the hanging letters. It may not quite be perfect - even better might be a few pixels below the hanging letters to get space there as well - but it solves the majority of the problem with one quick CSS change.&quot;
    },{
      "url": &quot;/2024/vs-code-extensions-workflows/&quot;,
      "title": &quot;VS Code Extensions: My Workflows&quot;,
      "content": &quot;When it comes to my VS Code extensions, I have mine set up a few different ways depending on what kind of project I&#39;m working on: Default These are the default plugins I have locally installed, for every profile and every project container. GitLens Playwright Tests for VS code Prettier Todo Tree tl;dr pages vscode-icons Static Sites These are additional plugins which come in handy with building static sites like this one and ryanrobinson.ca: Markdown All in One Markdown lint Front Matter CMS Containers/Remote (WordPress/Drupal) This profile includes the basic requirements to connect to containers and/or remote: Dev Containers Remote SSH Each container, using the devcontainer.json functionality, can also define new extensions for that container. So here&#39;s an example of doing that for a Drupal site: MySQL Drupal Twig PHPTools&quot;
    },{
      "url": &quot;/2024/dark-content-mode/&quot;,
      "title": &quot;Dark Content Mode&quot;,
      "content": &quot;In case you haven&#39;t caught it yet, there&#39;s new functionality coming to browsers for dark mode applied to content. This could come as a surprise to web developers who did not plan for dark mode in their design, but is overall a win for those of us who prefer dark mode on all the things all the time. It&#39;s already become my default on Android Edge, inheriting that I have my device in dark mode. I haven&#39;t found it to be default on any other browser (including desktop Edge). Chrome has it as an experiment features flag in chrome://flags, so I did most of my testing by enabling that. Issues Found The biggest problem I found in my main work site was with images that were previously assuming white backgrounds. I found the nicest solution to that was using an SVG instead of a PNG file, which the dark content mode adapted to invert the colours quite nicely. There were also some insufficient contrast I discovered, like grey borders around form text boxes, that were maybe a little weak in light mode but were barely visible in dark mode. Many of these were quickly solvable by making it a stronger contrast. For example, instead of a grey border against the white background, I made it into a black border. It content dark mode, it would flip to a white border on black background. Both modes came out looking better. I identified a few other contrast elements that were not quite ideal. But attempting a similar approach did not work. There were a few variations where I had black against the default light theme and trying it with the dark content mode turned it into a grey instead of a white - far from invisible, but not as strong of a contrast as I wanted. This is a new feature, not enabled by default other than that Android Edge that few people use, so I&#39;m hoping this gets cleaned up on the browser end and there isn&#39;t much else I need to do. No Way to Target? I tried the media query for prefers-color-scheme but it doesn&#39;t help for this. That detects if the device prefers dark mode, not if the browser&#39;s dark content mode is on. It&#39;s nice if you want to specifically define what everything should look like in dark vs light modes. But as far as I could find, there&#39;s no way to specifically target content dark mode at this time.&quot;
    },{
      "url": &quot;/2024/open-menu/&quot;,
      "title": &quot;Drupal: Open Menu&quot;,
      "content": &quot;In a previous related post, I wrote about some lessons in a co-design project of an open menu, with a video where we presented about it to the IDRC. That was mostly about the process and this is about the result: a header navigation page on a Drupal 9 site. The goal was to provide an overview of the same information that’s in the main navigation, but in a more accessible way for screen readers and with descriptions that provide more information about the page without having to load the page. It was not a complete index of every page on the site; it was only an alternate format for the menu. As mentioned in that previous post, an important requirement is that it must update programmatically; we cannot leave it as a separate manually-updated page, because realistically it would only be a matter of time before somebody updated the menu but not the header navigation page. Here&#39;s an example screenshot of the open menu structure: Menu Entity Index If you’re familiar with Drupal, you likely read the introduction and immediately thought of views. Views are the perfect solution whenever you are looking at a scenario involving one data source displayed in multiple ways. Drupal 9 does not expose everything I needed to views on its own, but there is a module that fills in the gap: Menu Entity Index. Once that is installed, you’ll need to configure it. I did not find the settings page intuitive at all, but the first block of settings is which menus need to be indexed and the second block is which types of target to index. In my case, I only want to index content in the main navigation. Notably, it cannot index views pages, so for this scheme to work, you can’t use views pages in the menu. Fortunately you can still make a views block, put that on a page, and put that page in the menu. Configure the View The configuration for the view itself won&#39;t be too much of a surprise for anybody who has configured views before. It has a grouped block for each second-level menu, where the header had a programmatically-set ID so that they can be used for links (both from that top block and from the top level of the main menu itself). There&#39;s a lot of mundane configuration here so I won&#39;t break it all down, but here&#39;s the configuration file if you want to import to your site, or you can check it out using the GitPod demo and going to Structure -&amp;gt; Views -&amp;gt; Open Menu. Overriding the View Template I still had one more problem. The view involved used the grouping function to bunch together the results. I wanted the group label to be an h2 and have an id on it, so I did that using the rewrite the results functions. That worked with no issues. But I noticed it was also supplying an empty h3 immediately before my h2. That doesn’t hurt sighted users, but it does complicate the experience for screen reader users. It took some time to realize the h3 was coming from a template file in the views core module: views-view-unformatted. Once I knew the source of the original file, I could edit it directly, but that would be lost as soon as there was an update to the theme. The more permanent answer is a subtheme with an override of that file. That seems a little excessive in the case of this demo, but in a real site you&#39;ll probably want a custom subtheme for all your CSS and templates anyway. That file is also in the GitHub, but it was a pretty tiny change removing the h3 wrapper from the grouping. Caveats There are a few caveats to this system: There&#39;s no handling for menu items more than 2 levels deep. That may well be possible, but we didn&#39;t need it, so I haven&#39;t included it in this demo. No views pages in the menu will show up in the open menu. But view blocks inserted into a page, where the page is in the menu, will show up. Be very careful changing the module settings. Perhaps this is fixed two years later, but at the time, attempting to index certain entity types would result in a white screen of death on the next cron run. We&#39;ve never had problems with indexing content. You could add some built-in warnings to the admin interface, or even override admin forms, to try to suppress the possibility of those caveats causing a problem, but this blog post was long enough so I&#39;ll leave it here.&quot;
    },{
      "url": &quot;/2024/idrc-presentation/&quot;,
      "title": &quot;IDRC Presentation on Open Menu&quot;,
      "content": &quot;I recently joined the Inclusive Design Research Centre to present along with a couple of colleagues how we designed an &amp;quot;open menu&amp;quot;, a header-focused main menu navigation that works much better for screen reader users and also provides some other benefits like a Ctrl+F find. The video is available on the IDRC wiki. I don&#39;t have a transcription of the entire presentation, but here are some of my key notes from the perspective of the Drupal developer. The Process The co-design process loop often went something like: Ashley (accessibility consultant) described what worked and what didn&#39;t about her experience as a screen reader user, Mark (digital experiences librarian) made an HTML mockup of what we should try instead, and I figured out how to make Drupal do that in the best and most sustainable way. Repeat as necessary until we&#39;re all happy with the results. I had to consider several issues, not only accessibility. For example: Timeline: One bit of important context was the launch timeline. I had just been hired and I was learning my way around the old site at the same time I was beginning the new site. We had a deadline for the new site to launch, what was at the time expected to be the Drupal 7 end of life (it got extended again after that). These menu considerations came as part of this rebuild process. That meant there was some balancing between what is ideal and what can be done in time. Security: Failure to meet that timeline would have security implications if we didn&#39;t meet it. We could have launched the open menu after launching the new site, in order to hit the end of life deadlines, but it would have been awkward at best and required restructuring the main visual menu at worst. Visuals: While not the main focus of this feature, there is visual design for sighted users to consider. We want it to still be attractive and easy to use for sighted users as long as it doesn&#39;t come at the cost of screen reader users. Maintainability: How do we ensure we keep this working and accurate? A major important point to me was that there needed to be one source of information that fed both the collapsed menu and the open menu. Otherwise, there&#39;s a high likelihood that at some point the two will get out of sync with each other, where somebody adds to or rearranges the collapsible menu without considering the open menu or vice versa. Sometimes an inaccurate accessibility feature is worse than no accessibility feature at all, because people wouldn&#39;t think to go try the less-ideal collapsible menu if they&#39;ve been assured they have all the same content. Open source models: What can be done in Drupal core, what in a contributed theme, what in a contributed module, what in custom code? If a module is close to what we need but not quite there, how do we navigate what we report and try to get the maintainer to change vs what we should rebuild ourselves? Lessons This was a fascinating project for me in ways much deeper than the technical details. So here&#39;s a few things that stood out to me: Empathy vs checklist: The biggest lesson is that personal experiences of disabled people really matter. I&#39;d worked with accessibility before, but mostly at the level of a WCAG (Web Content Accessibility Guidelines) checklist in a contract consultancy context where finishing the job under the budget was the main priority. This whole project was a great example of taking the time to get it truly right to real user experiences. Ashley explained that even though a standard Drupal menu passes the checklist, it can be really time-consuming for her. Hire someone if you can to fill that same role. It opened my eyes to a more emphathetic approach rather than a checklist approach. Maybe you can&#39;t hire somebody - most of us can&#39;t, and even our time with Ashley was limited - but at least take the time to sit and try to imagine what it would feel like to do this without sight, or without hearing, or without a mouse. Don&#39;t just hit the WAVE extension button in your browser and declare it done. Multiple solutions: Sometimes there isn&#39;t one solution that is perfect for everybody. We tried to figure out a way to make a menu that was optimal for all users. But the reality is that the standard collapsed navigation is ideal, tuned over decades of web development, for most sighted users who can skim at a glance without it taking much cognitive load. So we took an additive approach instead, keeping both versions of the menu. In hindsight this isn&#39;t really that shocking - we do the same for things like having a video and a transcript - but I hadn&#39;t really thought about it for something as central as the main navigation. Jargon: Getting the descriptions for each page was more challenging than expected and one of the reasons why were questions about jargon. Often in talking about accessibility, we talk about wanting the most generic phrasing that everybody understands, avoiding jargon. That&#39;s great for new users not familiar with the jargon. But there is a tradeoff for those who are familiar with it: that&#39;s probably what they&#39;re looking for. If they&#39;re looking for the jargon and it isn&#39;t in the description, it doesn&#39;t show up from a site search or in the open menu items page where a Ctrl+F might find it. Some of that could be mitigated by including the jargon and the broader definition, but that has a tradeoff of making the content longer to sift through, which is especially more time consuming on a screen reader. Making it more accessible to one group can make it less accessible for others. Coming Soon I have another post coming about how this was technically achieved, which was relatively straightforward. The bigger lessons were here, in the process, but I do want to share how it was done for those that are interested in replicating it.&quot;
    },{
      "url": &quot;/2024/dev-env-updated-dockerfiles/&quot;,
      "title": &quot;Drupal Dev Environment Updated: The Dockerfiles&quot;,
      "content": &quot;A while ago I wrote about building a Docker Desktop dev environment for Drupal. It was built on Oracle Linux, a requirement in that context, with three images for Apache, PHP, and MySQL. But there were some significant problems with it, first and foremost that it was very slow. So, this is another version, now based on one of the official Drupal images instead of the Oracle Linux ones. It can be seen on my GitHub. The Web Image This new setup has both Apache and PHP in the same container, is about 40% of the total image size compared to the previous one, and runs much faster, probably because Apache and PHP are in the same container instead of having to communicate across the network. Let&#39;s start with the basic start to the image and its shell. FROM drupal:php8.2-apache USER root SHELL [&#92;&quot;/bin/bash&#92;&quot;, &#92;&quot;-c&#92;&quot;] Install the extra useful packages - some of the most essential ones are already included in the Drupal image - and PHP development settings. The file copied from the PHP folder covers some of the XDebug configuration settings. # Install needed repositories and general packages, and put the php.ini in place RUN apt-get update -y &#92;&#92; &amp;&amp; apt-get install -y wget git zip which sudo vim locales default-mysql-client docker nodejs npm &#92;&#92; &amp;&amp; apt-get upgrade -y &#92;&#92; &amp;&amp; mv /usr/local/etc/php/php.ini-development /usr/local/etc/php/php.ini # Install PHP extensions, using PECL RUN pecl channel-update pecl.php.net &#92;&#92; &amp;&amp; pecl install apcu xdebug uploadprogress &#92;&#92; &amp;&amp; docker-php-ext-enable apcu &#92;&#92; &amp;&amp; echo &#92;&quot;apc.enable_cli=1&#92;&quot; &gt;&gt; /usr/local/etc/php/conf.d/docker-php-ext-apcu.ini &#92;&#92; &amp;&amp; docker-php-ext-enable xdebug &#92;&#92; &amp;&amp; touch /var/log/xdebug.log &#92;&#92; &amp;&amp; chown www-data:www-data /var/log/xdebug.log &#92;&#92; &amp;&amp; docker-php-ext-enable uploadprogress COPY /php /usr/local/etc/php/conf.d Let the default www-data user have sudo permission without passwords, which would not be the way to go in production but in a development environment is pretty useful in case you ever need to do things like install a new package without rebuilding the whole environment. # Add www-data user to sudo group, and allow those users to sudo without password RUN usermod -a -G sudo www-data &#92;&#92; &amp;&amp; usermod -d /user/www-data www-data &#92;&#92; &amp;&amp; mkdir -p /user/www-data/.vscode-server &#92;&#92; &amp;&amp; chown -R www-data:www-data /user/www-data &#92;&#92; &amp;&amp; mkdir -p /user/www-data/.ssh &#92;&#92; &amp;&amp; chown -R www-data:www-data /user/www-data/.ssh &#92;&#92; &amp;&amp; chmod 700 -R /user/www-data/.ssh &#92;&#92; &amp;&amp; ssh-keyscan -t rsa gitlab.com &gt;&gt; /user/www-data/.ssh/known_hosts &#92;&#92; &amp;&amp; sed -i &#92;&quot;s/%sudo ALL=(ALL:ALL) ALL/%sudo ALL=(ALL) NOPASSWD: ALL/g&#92;&quot; /etc/sudoers Fix a locale error that shows up once Apache is running, by specifying the locale it is running in. In my case, that&#39;s Canada, with Canadian English. # Fixes locale errors, must happen before Apache. This is using my locale of Canada RUN echo &#92;&quot;LC_ALL=en_CA.UTF-8&#92;&quot; &gt;&gt; /etc/environment &#92;&#92; &amp;&amp; echo &#92;&quot;en_CA.UTF-8 UTF-8&#92;&quot; &gt;&gt; /etc/locale.gen &#92;&#92; &amp;&amp; echo &#92;&quot;LANG=en_CA.UTF-8&#92;&quot; &gt; /etc/locale.conf &#92;&#92; &amp;&amp; locale-gen en_CA.UTF-8 Add a self-signed certificate for the Apache configuration, so that we&#39;ll be able to browse the site locally with HTTPS. # Apache configuration, including SSL certificates and logs COPY /apache /etc/apache2 RUN a2enmod ssl &#92;&#92; &amp;&amp; mkdir -p /etc/apache2/certs &#92;&#92; &amp;&amp; openssl req -batch -newkey rsa:4096 -nodes -sha256 -keyout /etc/apache2/certs/example.com.key -x509 -days 3650 -out /etc/apache2/certs/example.com.crt -config /etc/apache2/certs/openssl-config.txt &#92;&#92; &amp;&amp; chown -R root:www-data /etc/apache2 &#92;&#92; &amp;&amp; chmod 770 -R /etc/apache2/certs Increase some of the default resource limits for PHP. The defaults never seem to be nearly enough for Drupal. These go much larger, larger than you really need on production, but on local development it is easier to be safe than sorry. # Increase resources for PHP RUN sed -i &#92;&quot;s/max_execution_time = 30/max_execution_time = 300/g&#92;&quot; /usr/local/etc/php/php.ini &#92;&#92; &amp;&amp; sed -i &#92;&quot;s/max_input_time = 60/max_input_time = 600/g&#92;&quot; /usr/local/etc/php/php.ini &#92;&#92; &amp;&amp; sed -i &#92;&quot;s/memory_limit = 128M/memory_limit = 2048M/g&#92;&quot; /usr/local/etc/php/php.ini &#92;&#92; &amp;&amp; sed -i &#92;&quot;s/upload_max_filesize = 2M/upload_max_filesize = 128M/g&#92;&quot; /usr/local/etc/php/php.ini &#92;&#92; &amp;&amp; sed -i &#92;&quot;s/post_max_size = 8M/post_max_size = 256M/g&#92;&quot; /usr/local/etc/php/php.ini &#92;&#92; &amp;&amp; sed -i &#92;&quot;s/;max_input_vars = 1000/max_input_vars = 10000/g&#92;&quot; /usr/local/etc/php/php.ini This is a minor thing, but I like when I grep through code to see results with the colour highlights, making it much easier to read. That can be done by setting an environment variable and putting an alias for grep into a bashrc file. # Set up nicer grep results ENV GREP_COLORS=&#39;mt=1;37;41&#39; COPY .bashrc /user/www-data/.bashrc Line up the script that will be run after creating the images. That script will handle setting up the Drupal database, but I&#39;ll cover that more in the next post. # Scripts for further actions to take on creation and attachment COPY ./scripts/postCreateCommand.sh /postCreateCommand.sh Copy the Drupal settings file and the local services file. The former is the essential settings for any Drupal site, including being able to connect to the database. The latter is not essential but enables some of the development features that are ideal in local development, like seeing comments in the generated HTML code that shows you what template is being used to generate it. # Drupal configuration COPY /drupal /web/sites Finally, set the permissions on the main Drupal folder as well as on the script that we&#39;ll need to be able to run. RUN chown -R www-data:www-data /opt/drupal &#92;&#92; &amp;&amp; chown www-data:www-data /postCreateCommand.sh &#92;&#92; &amp;&amp; chmod 777 /postCreateCommand.sh Much of this is the same as the previous version: adding a self-signed SSL certificate, increasing PHP resources. But a few things changed: Many of the PHP extensions are already installed from the Drupal image so I didn&#39;t need to do them again in this Dockerfile. It needs a lot more overriding the permissions since that Drupal image already comes with a Drupal install in that /opt/drupal directory and with the www-data user, and with some as volumes which mean they immediately revert to root ownership when you attach to it. The Database Image Here&#39;s the entire Dockerfile: # Use the default MariaDB image FROM mariadb:latest ENV MARIADB_ROOT_PASSWORD=drupalroot ENV MARIADB_DATABASE=drupal ENV MARIADB_USER=drupal ENV MARIADB_PASSWORD=drupal # Expose the MySQL port to be accessible to the web container. EXPOSE 3306 This is essentially no change from the previous one, just setting the environment variables to define the database and its access. Docker Compose The docker compose file is not doing anything too unusual. The main thing of note is the volumes. I do not have the entire /opt/drupal set up as volumes. That&#39;s because when you attach to it, the entire area would get its permissions reassigned back to root, which causes issues. It&#39;s also not very efficient, since I really don&#39;t need things like the core files and contributed modules in the vendor folder to be persistent. So instead, it defines the pieces that are relevant to be able to build it, as well as the private and public folders where I might want to download a file for testing that the site can use. I also have it set up to share my user profile&#39;s SSH keys and preferred settings like the my commit name, so that I can connect to the repository without having to configure it all again with every new build. version: &#92;&quot;3.9&#92;&quot; services: web: hostname: &#92;&quot;web&#92;&quot; container_name: &#92;&quot;web&#92;&quot; build: context: &#92;&quot;.devcontainer&#92;&quot; dockerfile: &#92;&quot;web.Dockerfile&#92;&quot; ports: - &#92;&quot;443:443&#92;&quot; volumes: - &#92;&quot;./.devcontainer/:/opt/drupal/.devcontainer/&#92;&quot; - &#92;&quot;./.git/:/opt/drupal/.git/&#92;&quot; - &#92;&quot;./.gitignore:/opt/drupal/.gitignore&#92;&quot; - &#92;&quot;./.vscode/:/opt/drupal/.vscode/&#92;&quot; - &#92;&quot;./patches/:/opt/drupal/patches/&#92;&quot; - &#92;&quot;./private/:/opt/drupal/private/&#92;&quot; - &#92;&quot;./sync/:/opt/drupal/sync/&#92;&quot; - &#92;&quot;./web/sites/default/files/:/opt/drupal/web/sites/default/files/&#92;&quot; - &#92;&quot;./composer.json:/opt/drupal/composer.json&#92;&quot; - &#92;&quot;./composer.lock:/opt/drupal/composer.lock&#92;&quot; - &#92;&quot;./docker-compose.yml:/opt/drupal/docker-compose.yml&#92;&quot; - &#92;&quot;./README.md:/opt/drupal/README.md&#92;&quot; - &#92;&quot;${USERPROFILE}/.ssh/:/user/www-data/.ssh/&#92;&quot; - &#92;&quot;${USERPROFILE}/.gitconfig:/user/www-data/.gitconfig&#92;&quot; user: &#92;&quot;www-data:www-data&#92;&quot; depends_on: - db networks: - &#92;&quot;drupal&#92;&quot; db: hostname: &#92;&quot;db&#92;&quot; container_name: &#92;&quot;db&#92;&quot; build: context: &#92;&quot;.devcontainer&#92;&quot; dockerfile: &#92;&quot;db.Dockerfile&#92;&quot; networks: - &#92;&quot;drupal&#92;&quot; networks: drupal: Next Post In the next post, I&#39;ll mention some of the changes to the devcontainer.json file and to the postCreateCommand script, although there&#39;s nothing too drastic there.&quot;
    },{
      "url": &quot;/2024/dev-env-updated-devcontainer/&quot;,
      "title": &quot;Drupal Dev Environment Updated: The DevContainer&quot;,
      "content": &quot;This continues from a previous post about a new Drupal dev environment, a significant upgrade from some previous posts that you can find on the Drupal Docker tag. That covered the Dockerfiles, while this will cover changes to the Devcontainer.json and the postCreateCommand script. The whole project&#39;s code can be found on my GitHub. Devcontainer What changed in devcontainer from the previous setup? A few things: I reviewed several of the extensions and made some changes. The most significant are adding Playwright testing (maybe I&#39;ll be able to write more about that later), the PHP tools extension, and the Drupal extension. Between them I now have much stronger error detection, deprecation warnings, testing, and formatting to meet the Drupal standards. Some more notes about useful extensions are in my recent post about what extensions I use. The postAttachCommand has to set the permissions again with each new time attaching to it, so the files that are set up as volumes don&#39;t revert to root. postCreateCommand This is mostly the same, at least for now: it installs all composer packages and sets up the database on the first time creating these images. A few things were able to be removed to simplify this script, functions like creating the database that are handled more easily in other ways. What it still does: Installs composer packages. Puts the Drupal settings and local services files in place. This comes after the composer install so that the basic Drupal file structure is there already. Import the site&#39;s configuration using drush. Set the admin password. Rebuild the node access cache. There aren&#39;t any nodes yet, but there is a warning on the site status for a bit if this function isn&#39;t included, so it&#39;s one extra command to save some confusion. Sets the environment indicator theme. Clears the site&#39;s caches to reflect all changes. Next: GitLab/GitHub CI/CD Updates I&#39;m going to continue with this by now improving it a step farther: instead of building the images each time on local, have it build in GitHub or GitLab programmatically, and then saving it alongside the repository in those platforms&#39; respective container registries. Then when it is time to start a new local development environment, it only needs to pull the latest image, not build it again. That saves time, especially if you have to switch between branches and especially if you have multiple team members who would otherwise all be spending time building it individually. This will include moving most of the postCreateCommand functionality into the GitLab CI / GitHub Actions, for both the web and database containers. I already have this working in my real context using GitLab. I have less experience with GitHub Actions but am working on doing the same idea there for a more shareable version.&quot;
    },{
      "url": &quot;/2024/vs-code-university-colour-palettes/&quot;,
      "title": &quot;VS Code: University Colour Palettes&quot;,
      "content": &quot;I like changing colour palettes on my various workspaces. It’s especially nice when in situations like jumping between personal and work spaces of the same app, or multiple clients, or dev vs staging vs production servers. It&#39;s a great way to quickly adjust what I&#39;m thinking about, e.g. if I&#39;m seeing yellow or purple or red, it immediately slows me down to be a lot more careful about what I&#39;m changing. So, one of the things I’ve done is have different colour palettes in Visual Studio Code for different servers. One set of colours are for Wilfrid Laurier University, where I work. I personally use the green one for my primary development container, yellow for dev, purple for staging, and red for production (red because I shouldn’t ever actually be editing directly on production). I also use the pure black one with the brand colours as accents for when I am editing something local to my Windows host machine, not in any docker or remote SSH. And then, just for fun, I decided to do the same with my alma mater university, Queen’s. And then a couple of other nearby universities, Waterloo and Guelph. I haven&#39;t heavily tested those so maybe some of the fringe contrasts aren&#39;t quite right, and some of them (Waterloo) offers less range in colours than others. But they&#39;re at least a starting point. They are available in my GitHub. I&#39;d be happy for anybody who wants to include their own university to submit it as a merge request. I&#39;d also be happy if anybody wants to turn that into a VS Code extension.&quot;
    },{
      "url": &quot;/2024/switch-to-eleventy/&quot;,
      "title": &quot;Site Rebuilt in Eleventy&quot;,
      "content": &quot;I have rebuilt this site in Eleventy, replacing the previous Jekyll version. Here are some of my thoughts on the experience: Advantages It&#39;s got a couple advantages over my Jekyll experience. One that eleventy tends to brag about is that it builds very fast. This doesn&#39;t matter that much to me in the context of a simple site mostly hosting blogs on GitHub Pages, but it is nice to at least know when I commit something that it will be visible for testing quickly. The bigger one for me has been that I understand it more. I got the hang of the templating system pretty quickly, as its nunjucks is not far off from the twig I use for Drupal. With the old Jekyll site, it mostly felt like a black box that I didn&#39;t have enough control over. There probably was a way, but it wasn&#39;t obvious. Here I know what files are contributing to the markup, what files are contributing to the styles, and what extra JavaScript is running. Search Search is one of the more significant disadvantages. There is no default search module. I implemented PageFind. This mostly works well enough, and wasn&#39;t that hard to figure out, but unlike the big advantage noted above with eleventy templates, there isn&#39;t a huge amount of control over the UI of the end result. The largest problem I have with that is I am unsure it is the most accessible possible, e.g. it does not create an aria-live region when the content visible on the page has been changed, so screen reader users may not even know the new content is being displayed. Taxonomy Pages The other big-ish technical piece that I have not solved yet is having pagination on the term pages like the one for accessibility. This is because pagination is itself used to create those taxonomy pages programmatically, rather than needing to set up each tag&#39;s page manually, and there is no support for another level of pagination within those. This isn&#39;t a huge problem for me at the moment, since my most-populated tag is at 49 posts and that isn&#39;t unreasonable to show on one page. But it could become a more serious problem eventually. Post tl;dr Because there is relatively easy flexibility of what metadata you track for posts and how those appear in templates, I could do things like add a post tl;dr that shows as the summary on the posts lists, as well as the top of the individual post display. This is intended to be a quick summary when you visit a page, to understand if this is going to answer your question or whether you should look somewhere else. That came out of some consultation with a screen reader user, who expressed how it can be hard to get to a page and have to listen to the whole thing only to realize that she is on the wrong page. Series I created another taxonomy for the series of a post. Now, if a post is part of a series, that whole series will appear in the sidebar of that post, making it easier to jump to other posts that you might need to read first or want to read next. Post Changes A lot of the metadata in my posts needed to change to support this new structure instead of Jekyll&#39;s, including: Add a description for each post to serve as the tldr. Remove the category field which is no longer relevant. Change the filename to what I want the permalink to be, and remove the permalink from the front matter. It is possible to set the permalink through front matter to override the file name, but at least for now I&#39;ve gone with simplicity. Update any links and image references, since those are all in new locations now. Domain I have also changed the domain. Instead of ryanrobinson.technology, I&#39;m using a subdomain, tech.ryanrobinson.ca. This is primarily an effort to simplify my web presence around the one main domain that hosts a broad landing page about me. Now that I have a better idea of how to build a site with eleventy, and have a theme I&#39;m mostly happy with (some improvements noted below notwithstanding), I will likely make more subdomains off of ryanrobinson.ca for other types of blog content. Still To Do While I am declaring this good enough to phase out the old site and get back to some writing, there are a few things I hope to still do as of writing this post: As mentioned above, I don&#39;t think the search functionality is as accessible as it can be. I may have to make a copy of the PageFind JavaScript UI in order to rework some of it, while keeping PageFind&#39;s handling of other aspects like building the index. The styles are not all great. I think they&#39;re readable, but there are things like the black of the dark mode probably still being too black. I have done some passes to review the posts, but it might not all be there yet. I need to review all the descriptions, tags, and series.&quot;
    },{
      "url": &quot;/2024/accessible-fonts/&quot;,
      "title": &quot;Accessible Fonts&quot;,
      "content": &quot;I recently did some research on the accessibility of a variety of fonts. It&#39;s an interesting area, and it is one of those areas of accessibility where there is no one solution that is absolutely perfect for everybody. A font that is great for one particular reading disability may be a big problem for others. In other words, there are two big things to keep in mind, as always in accessibility: Know your audience. If you&#39;re specifically targeting people with dyslexia, your decision will be different than for the general public. Pursue an additive approach where possible. If one font can&#39;t help everyone, set a default to be something that is best for as many as possible, then have something like the Fluid UI framework for Drupal which can allow users to select something else that might be better for them. With that said, here are some of the things I learned to consider when looking at fonts: Testing How did I test these? With most fonts, I could generate samples using Google Fonts and/or Adobe Fonts. I typed in the scenario I was trying to test and took notes on how well they do in each of the categories below, slowly cutting down which fonts were serious contenders. As I got down to only a few options, I also tried putting them into the site I was considering fonts for so I could see it within our specific context. Doubles Doubles are pairs of characters that look too similar to be able to tell about, at least without other context. In some fonts with really bad doubles, this doesn&#39;t even require any extra reading disability - it applies to all of us. One cluster of these are the characters that are mostly straight lines in some fonts. This site won&#39;t demonstrate the problem, but in a lot of fonts, try l (lowercase letter L), the number 1, uppercase letter I, and the pipe |. There have been a handful of times in the past couple of years that I&#39;ve seen so many people talking about AI, then somebody mentions a guy named Al and I am very confused because in the font of wherever I saw it, I could not tell them apart. When I did the testing, I often found that most fonts were good at distinguishing two or three of those four from each other, but not that many of them were obvious enough for all four characters. Another very common double is the uppercase O with the number 0. In a lot of fonts, the letter is wider, but otherwise they look very similar. If it&#39;s not in the middle of a different number, you can&#39;t tell it apart. How many times you have seen an alphanumeric code like a password printed on something and not been able to tell whether it was a 0 or an O? Other than being a little skinnier for the number, the most obvious way to differentiate is to put a line through the 0, as this site&#39;s font does. Depending on the context, that might be more annoying to read than it is helpful to differentiate, but it should be something you consider. Mirrors Mirrors are a little different, the pairs of letters that are distinct from each other but very similar if you flip one of them, which can be a problem for dyslexia. The two common examples are: pq and db Look at them side-by-side. Can you see what&#39;s different, maybe an extra serif on one that&#39;s not on the other to tell them apart? Spacing This one isn&#39;t quite as easy to say pass or fail, but I tried to judge how readable the spacing was between letters. I found when fonts were too skinny they became hard to read, with the letters blurring together. But too far apart might also be hard to read, and also increases the pages size which results in more scrolling and that&#39;s a cognitive load in a different way. There is some sweet spot, which might be different for all of us. Monospace Most fonts allow different letters to have different widths, which is another way to help them stand out from each other. There&#39;s a reason why an &amp;quot;em&amp;quot; is a unit of measurement in HTML/CSS: because the letter m is the widest letter, so it is a way of saying &amp;quot;I want to fit at least this many letters here.&amp;quot; The exception where monospace fonts - those with the same width for all letters - are often preferred is for coding. Knowing that each letter is the same width is very helpful for keeping the code blocks lined and being able to judge how many characters are there. So if you&#39;re including code snippets on your site, you might want a monospace font for those, but otherwise you probably don&#39;t. Conclusions So what did I land on? The Braille Institute&#39;s Atkinson Hyperlegible was the best across all those tests, which makes sense given that it was very intentionally designed for accessibility. I&#39;m using it for this site for that reason. The 0 with the strike through is a bit jarring for some, as I mentioned above, but I am starting to really like it. I&#39;ve even started using it in other apps that offer it (e.g. Pachli, the Mastodon app for Android). Tahoma scored well on most fronts, other than the 0 and O mirror. Of the major fonts by default available on most systems already, this came out as my winner. It is the most friendly to the most number of people, maybe scoring a little lower than Atkinson Hyperlegible on some of the mirrors and doubles, but quite well on everything else, and without the line through the 0 that is a distraction to a fair number of people. In a professional context with a general audience, there&#39;s a good case to take this over Atkinson Hyperlegible. Comic Sans, mocked by much of the Internet, is considered one of the best fonts for dyslexia, because it doesn&#39;t have mirrors. Every letter is easy to differentiate. But I think it is valid to point out that it can cause readability problems for most others, so if your site is general purpose rather than targeting the dyslexic community specifically, it might not be a great choice. There is also an Open Dyslexic that is similar and particularly targeted for dyslexic users. I&#39;ll note some others that scored generally ok: Aptos, the new Microsoft 365 default font; Arial; and Verdana (much like Tahoma but a bit wider).&quot;
    },{
      "url": &quot;/2024/managed-mastodon/&quot;,
      "title": &quot;A Brief Experiment in a Mastodon Self-Managed Instance&quot;,
      "content": &quot;I took advantage of a Black Friday deal from fedihost.co to try out using a managed host for a Mastodon account, as compared to the large Canadian instance mstdn.ca that I have been using since my early days of joining Mastodon. It was $1/month for the lowest plan for a year, which is pretty solidly in impulse purchase territory, so it was a worthwhile idea. To clarify, this is not an experiment in self-hosting in the strictest sense. I&#39;m not doing any of the server or software maintenance myself. It&#39;s more like self-administering. Some professionals I&#39;m paying did all the server and software work. I just administered users, which in this case was just me, and general settings. Anyway, here&#39;s how that experiment went. You might already have some ideas from words I&#39;ve used like &amp;quot;brief&amp;quot; and how a Black Friday deal experiment is now all past tense. Signup The process with fedihost was easy. It gave me the DNS record to create first, then I came back the next day to confirm it propagated, and signed up including a payment method. Soon after, I got an email that everything was ready, with a link to the dashboard. That dashboard is minimal but had a few options for managing services like how many days of cache to keep, which would quickly become an essential option (see under Storage below). It also showed me a randomly generated admin password I could use to sign in. Domain The biggest benefit of this option may be that I could use a subdomain of my main domain, creating mstdn.ryanrobinson.ca. If you really care about your branding, that&#39;s a big selling point, plus as Bluesky has demonstrated, it&#39;s a form of verification that you are the same person who owns that website. Mastodon does have a more subtle version already, where you can put a link on your site coded in the correct way, then put that website URL in your profile and it will show with a check mark. But that&#39;s fairly hidden on a user profile, not directly in a handle or anywhere close to the now-infamous blue check on Twitter. The downside of this, at least in my case, was that now my handle became much more unwieldy. I have a pretty great handle at mstdn.ca, simply @Ryan@mstdn.ca. That&#39;s rare with my very common name to get that good of a handle. It&#39;s clean, easy, not taking up much space. With the managed hosting, I got @Ryan@mstdn.ryanrobinson.ca, which gets to use my domain but looks much clunkier. Trending If I had thought it through, I would have predicted that trending would be less useful. With less wide range of federation, less tags and links will come in. What I never would have thought of was that I would need to approve each tag and each link before it can even show up within the typical user app. I get why that&#39;s an admin moderation function, to not let hate speech or whatever else you don&#39;t want to be visible for other users. But when you&#39;re the only user, it means you can&#39;t see anything trending in the app until you&#39;ve first already seen it in the admin panel, for which I get a few emails a day to review. Those emails will decrease quickly because once I&#39;ve approved it once, I don&#39;t need to again, but that is an annoying extra step with zero benefit when it&#39;s a single-user instance. Search Search is also less powerful with less federation, although that was less of a problem in my few days of testing. It&#39;s not that often that I really want to search for a topic. At this point I mostly follow who I follow, not going looking for what strangers are saying about something. Moderation Moderation is the area I can probably say with least confidence because I was doing it for such a short period of time. I didn&#39;t have a lot of connections, and I did not end up doing any of my own moderation. That&#39;s the upside of the limited federation. This was the thing that I was worried would be the biggest extra piece of work, because I really have no desire to squash a bunch of crypto scammers or hate speech on my own. Maybe it would have become more of a problem on a longer scale. Markdown This managed hosting instance did not support markdown as mstdn.ca does. This isn&#39;t a huge deal, but I do appreciate having it, especially for links since there isn&#39;t a guarantee that every app has link previews so I want to at least give it readable link text. Local Feed I rarely use the one on mstdn.ca, but the entire concept of a local feed is meaningless on a single-user instance. It&#39;s just me. I may as well look at my profile. Storage The cheap plan, the one that was so cheap on Black Friday as to spark an impulse purchase, comes with 10GB. That feels like it should be a lot for one social media account with auto-deleting posts. After initial install I was using about 2.5GB. By the second day, I had a warning email letting me know that I was over 80% and if I fill it up, I will be automatically upgraded to the next plan which does not have the Black Friday bargain prices. So I went in to the dashboard and cut the caching option from keeping 10 days down to only 1. The next morning I saw I was back down to about 65% used. I still got another warning email the next day. Today I decided that there was nothing left to learn in this experiment and decided to delete. I was at 97% of my storage even though I had barely logged in today and had the 1 day cache. It was inevitable I would have been paying more for it soon, which takes it not only past the $1/month bargain but also past the $6/month regular price for that plan that is roughly equivalent to what I am donating to mstdn.ca. Conclusion Overall, it&#39;s a fine option. It&#39;s reliable, affordable, not a lot of extra work, and has some benefits if you really want to be able to shape it to be exactly what you want from moderation decisions to the domain. But I think it&#39;s a step backward in my case. It&#39;s still some more time required, a clunky handle, and almost certainly would have ended up paying a fair bit more to support more storage than I am donating to mstdn.ca. The storage challenge helped rush the decision, but really, it was already becoming clear that there was nothing else left I might learn to convince me it would be better off than what I&#39;m using now. I&#39;m going back to mstdn.ca full time.&quot;
    },{
      "url": &quot;/2025/podcasts/&quot;,
      "title": &quot;The State of Podcasts 2025&quot;,
      "content": &quot;It&#39;s a new year (happy 2025) and a good chance to review the state of my podcast feeds. I&#39;m a major podcast listener, with something running in the background a lot of the time that I&#39;m working or playing Xbox or cleaning or on a bus or a lot of other times. So here are a few scattered thoughts about podcasts: Is This Healthy? First it is fair to ask whether it is good for me that I listen to podcasts for large portions of the day. We also have a YouTube Music subscription. I could be listening to ad-free music instead, and I often think I should be doing that more, or even gasp allow myself some occasional silence. This is a subset of the general problem of &amp;quot;there&#39;s too much content&amp;quot; which also applies to other types of media. There just isn&#39;t nearly enough time in a day to listen to all the good podcasts and watch all the good shows and read the good books and listen to all the good audiobooks and play the good games. And I think I&#39;m starting to lean toward wanting to have less podcast content (and maybe less TV) but be more picky about it being the best, freeing up the other time for books or music or silence. Pricing Models That leads into revenue models, because I don&#39;t really want to be paying for a lot of stuff that I can&#39;t even keep up with anyway. So with that in mind, it got me thinking about the state of podcast pricing models, because there is some wild variety. There are three general ways to make money off podcasts that I have encountered, and they may be combined in various ways: Ads, in a couple of varieties: in the more user-friendly case, these are host-read ads. Those are context sensitive (e.g. if it&#39;s a tech podcast, it has ads that are relevant to tech professionals, in the same way that it is impossible to watch sports without getting sports gambling ads). I don&#39;t mind those most of the time, but they are getting less and less common. In the worse case, they are targeted surveillance ads, based on tracking you all over the Internet and inserted as you download/listen. They also tend to be precorded and inserted, which is more jarring than hearing the hosts read it. I really have a limit on how many surveillance ads I&#39;ll put up with. Paid subscription tiers: it&#39;s pretty common to have some content be free and then more bonus content be behind a subscription paywall. There&#39;s a spectrum here between how much they try to sell you on additional content vs how much it is treated more like a donation appeal. This is where some of the comparisons I&#39;ll make below get really interesting. Loss leader for something else: the podcast itself could essentially be an ad for a larger product that the podcast is associated with, like a newspaper&#39;s podcasts that are consistently if subtly pushing toward getting a newspaper subscription, or a university&#39;s podcasts that might subtly convince you to take classes there or donate. Of course there are also a handful of podcasts that are purely a labour of love and don&#39;t make any money, but that&#39;s tough to maintain because they can&#39;t even buy things like quality equipment. I don&#39;t think I have any of those anymore. So here&#39;s the breakdown of the pricing models of the podcasts I currently have in my queue. This Week in Tech I&#39;ll start with my tech podcasts since this a tech blog. The TWiT (This Week in Tech) network is a great bargain compared to anything else in my feeds, with their Club TWiT offering. It&#39;s only $7 (all prices USD) a month for a lot of good shows. Even before I joined the paid level, a couple of their shows - Windows Weekly and the flagship This Week in Tech - were the only tech shows I was still listening to. Everything else died for some reason or another; at least some were because they did not have sustainable revenue models. Before subscribing I only listened to those two. Now subscribed I&#39;m trying out at least a few more: This Week in Google (great host chemistry and not really that focused on Google), Tech News Weekly (very interesting so far), Security Now (great topics but a bit harder to listen to with only one speaker for the most part), Home Theatre Geeks (some episodes are really interesting to me, but not others). The subscription unlocks a few things: Ad-free episodes. That opts out of both host read ads and surveillance ads, although they do interestingly provide an extra feed for the host read ads for those who do still want them, since they are actually relevant and screened by their experts. A Discord community with a channel for each show that allows things like submitting questions, as well as live chats while the shows are airing live. I usually listen after the fact via podcast, not live, so that isn&#39;t a huge selling point for me, but I did sign up. Some odds and ends of bonus content, like a book club and Ask Me Anything interviews. Some of these are interesting. In other words, that&#39;s a pretty good balance of carrot (some bonus content and community) and stick (ads you have to listen to if not). Nebula I&#39;ll reference Nebula in its own category. I follow three of their podcasts, using the Nebula exclusive feed. This means no ads. Nebula is an excellent deal at $30/year. For that, you get: Ad-free videos from some of the best educational content creators out there. Extended versions of those videos. Exclusive content from those creators, including some short movies they have produced. Ad-free on any of the podcasts. That&#39;s a pretty great deal, compared to YouTube Premium to avoid ads there, which is $14/month for an individual or $23/month for a family. Nebula is barely more for a year than YouTube Premium Family is for a month, and they have almost all of my favourite YouTube creators (we still have YouTube Premium as well, primarily for music). Movies and Comedy I listen to all five shows from the From Superheroes network. There is no network bundle price. Each show has their own Patreon, with different tiers. The tier to avoid ads and get bonus content, on the ones that offer that, are up around $10/month. Per show. They do have some other small perks as well, like being able to vote on audience choice episodes. I really like their shows, but they are the worst value for the money from my feed, and it feels even more ridiculous if you start comparing it to things like video streaming platforms. That&#39;s a high barrier of entry, so they have a lot less paid subscribers. The &amp;quot;You&#39;re Wrong About&amp;quot; Family You&#39;re Wrong About was the first podcast with Sarah Marshall and Michael Hobbes. Michael is no longer on that one, but hosts If Books Could Kill that I also listen to. Sarah also hosts You Are Good. All three of these follow the same model. They&#39;re ad-free for anybody. Then they have a Patreon that is mostly pitched as a way to support them, only $2-4 a month (depending on the podcast) to get bonus episodes. If Books Could Kill also has a couple higher tiers to join a Discord and vote on books. So they&#39;re leaning heavily toward the carrot, with no real penalty to listening for free, but a low cost to get bonus episodes, meaning they probably get by mostly on a massive number of subscribers at a small price per person. Christian Homebrewed Christianity and The Bible for Normal People have a similar model: the main podcast feed has ads - both host-read and surveillance - and there is also a paid subscription option. In both cases, you get a lot for the paid subscription: ad-free podcasts extensive classes taught by experts in theology/biblical studies online community extra written content (Homebrewed Christianity) TB4P is cheaper, at $12 USD per month, or $11 if you commit to a year up front, while Homebrewed is $25/month if you commit to a year. Homebrewed does also have a cheaper tier for just ad-free podcasts but not the classes. It&#39;s a little pricey but not so much when you compare to auditing a seminary course, which is basically what the classes are offering. Reclaiming My Theology and Inverse have a model similar to the &amp;quot;You&#39;re Wrong About&amp;quot; group, or somewhere between those and the two above. There are no ads for the general public feed, then they have Patreon options that include things like extra content and more direct connection to community. Then there are a few I listen to in the third category I mentioned above: Woodland Hills Church is a podcast sermon feed, so no ads there but they would take donations to the church from the virtual congregation. ReKnew&#39;s Apologies and Explanations come from roughly the same group of people, also ad-free but does accept donations. Yale Divinity School has an occasional podcast that is ad-free but definitely helps build good will to their seminary. Sports Sports skews the most heavily towards ads, mostly surveillance, being the only option. That is a big part of why I&#39;ve cut down my sports podcasts, along with there simply being so many, even though other than the ads they do tend to make great white noise while working. I have two from The Athletic: No Dunks primarily focused on men&#39;s basketball and The Athletic Women&#39;s Basketball Show as the name suggests primarily focused on women. They both have ads, both surveillance and host-read. When The Athletic was independent, you could subscribe and get the writing and the podcasts ad-free, but you had to use their app - there was no way to put the podcast into your regular feed - which meant other limitations like not being able to cast to a speaker or put in a queue with other episodes. I haven&#39;t tried subscribing again, but from what I have been able to read online, a subscription is now twice as expensive and doesn&#39;t avoid ads and I don&#39;t know if they solved the feed problem. The Pickup is a WNBA comedy podcast. That&#39;s all surveillance ads. The Raptors Show is some surveillance and some host ads. Raptors Republic was all surveillance ads. I might drop all of those from my rotation. Newly added to check out: Katie Nolan has a new podcast. I loved her old podcast with ESPN. I can guarantee that will be ad-based, some combination of host-read (she&#39;s a great comedic ad reader) and surveillance. Bringing Home the W is a very specific show about the start of the Toronto Tempo WNBA team, giving a look behind the scenes. This has no ads, because the entire thing is essentially an ad to be excited for the upcoming team (and I am). Conclusion So we&#39;ve got a wide range of pricing models. Some only give you options for ads, especially the sports ones, maybe because they know that they&#39;re more often white noise, unlike some of these other categories where you&#39;re more likely to be listening and taking notes. When you are listening and taking notes, the ads are more jarring and you&#39;re more invested so more willing to pay to get rid of them and/or to get bonus content. Some other are all-in with the bonus content paid model and no ads, which I love to see. Some combine the two, with a large difference in price between the two networks I have doing that, TWiT and From Superheroes. What Now I already added Club TWiT recently. That&#39;s a good deal for a lot of good podcasts. I&#39;m tempted by the &amp;quot;You&#39;re Wrong About&amp;quot; family, because they are so cheap to get the bonuses, and I do kind of want to encourage that model. I&#39;m likely going to subscribe for the extra classes from either Homebrewed Christianity or Bible for Normal People. But it&#39;s probably a bit too much (both in money and in time to keep up) to do both of them at the same time, so maybe I go for a year of B4NP and then a year of Homebrewed. I&#39;m likely going the other way on sports, cutting back significantly because there are just too many of them with too many surveillance ads, and if I have to cut something to make some time, those are the most obvious to go.&quot;
    },{
      "url": &quot;/2025/link-new-window/&quot;,
      "title": &quot;Opening Links in New Tabs&quot;,
      "content": &quot;Links can be set to open in new windows/tabs or (by default) not. You mostly shouldn&#39;t do that, but there are a few exceptions and a few ways to carry out those exceptions. Accessibility: When to Do It The decision for whether to open links in a new tab is not a simple matter of personal preference. You need to think about the experience for all users, including those relying on assistive technology such as screen readers. Most of the time, don&#39;t open in a new tab. This is because it can be confusing in a screen reader to be directed into a new tab without noticing, then being unable to use features they&#39;re used to like a back button. That&#39;s bad, so mostly don&#39;t do it. There is one exception category, however: if the user needs to reference the new tab while still doing something in the original tab. The most common scenario is a form that has a link to more information about something that might change you fill out the form. If you go to the extra information in the same tab, you lose all the work filling out the form so far, and there&#39;s no potential to go back and forth. In that case, a new tab is worthwhile. Accessibility: How to Do It With that said, if you do need to do it, it needs to be as clear as possible (without being unnecessarily annoying) to all of the users. Verbal Indicator It needs to be especially clear to those who cannot see that the new tab has been opened. To do this, make sure one way or another that it will be announced to the screen reader before they click. The easiest way to do that is to set the aria-label. For example, if the link&#39;s visible text said &amp;quot;privacy policy&amp;quot; then the aria-label would be &amp;quot;privacy policy, opens in new window.&amp;quot; That might not be the most obvious to do within a content management system context, but often there is a way; if you&#39;re a developer and not just a content creator, it&#39;s your job to figure out how to offer that. Visual Indicator While maybe not quite as urgent, similar to the point above, you do also want to notify sighted users that a new window is about to open. Sure, they can tell afterward because they can see the extra tab open, but it can still be an extra nuisance if it catches them off guard. So for that, I decided on a fairly common approach of adding an icon after any link that opens in new tabs. Here&#39;s a quick bit of CSS to help show an icon for any link set to open in a new tab/window. I grabbed an icon from Font Awesome that is a pretty standard one for this purpose. First, if you don&#39;t use it already, make sure the source of the icon is loaded in your site. In my case, I used the all CSS path. This link is for the current version 6.7.2. You may need to look up the latest version. You also might be happy with a smaller version for a subset of the icons; in my case I was using enough other ones in other contexts that it made sense to take it all. Now you can add a style like this to the appropriate place in your stylesheet: a[target=&#92;&quot;_blank&#92;&quot;]::after { display: inline-flex; margin-left: 5px; content: &#92;&quot;&#92;&#92;f08e&#92;&quot;; font-family: &#92;&quot;FontAwesome&#92;&quot;; } The display value of inline-flex has a minor advantage in my case compared to the default. My links are normally underlined. Without that inline-flex distinction, the icon would also be underlined, but that margin in between the text and icon would not be, which looks awkward. By switching the display value, it removed the underline from the icon, looking cleaner.&quot;
    },{
      "url": &quot;/2025/ecosia/&quot;,
      "title": &quot;Ecosia Browser and Search&quot;,
      "content": &quot;I recently stumbled my way into hearing about Ecosia. They&#39;re a search engine and browser backed by a non-profit where they turn any excess revenue into planting trees. That sounds much nicer than the usual, excess revenue turning into the constantly-ballooning demand for more shareholder value. I decided to do some rough tests of each. Browser The browser is about as close to a barebones Chromium as you can get. The mobile app doesn&#39;t allow you to change your search engine from their ecosia, as far as I can find, so if you don&#39;t want to use that for search, that will quickly wipe out any value for that browser. The desktop app does allow that change. The desktop app doesn&#39;t come with the ad blocker enabled by default, but it is there in the settings. The mobile app includes it by default. Otherwise, there&#39;s not a lot to talk about here that I can see. That&#39;s not necessarily a bad thing. It might be pretty good for you, because it is as lightweight and efficient as possible without all the extra layers that Google adds to Chrome or Microsoft adds to Edge or even some of the other smaller Chromium variants. Professionally, I use enough of the extra crap that Microsoft adds to Edge, including but not limited to vertical tabs and a lot of little ways that Microsoft 365 is built in, that I think it still makes sense to keep Edge my primary work browser. My personal mobile browser I&#39;ll stick with Firefox if only because I can change my search engine and I&#39;m not sold on ecosia for that (see below). There&#39;s maybe a bit more room for me to consider it instead of Firefox as my primary desktop personal browser. They&#39;re probably pretty close on privacy protection, maybe a little better extension support, ad blocker built in, and it also being Chromium does make it a little more natural for me to use it personally as I do Edge professionally. I&#39;ll still call that aspect a maybe, but there isn&#39;t much of a selling point and if I&#39;m sticking with Firefox on mobile, there are some benefits in synchronization that I take advantage of often. Search Engine The search engine is largely a skin of either Google or Bing. You can select which set of results you prefer, if you allow some more cookies - it is nice that they give that choice and actually explain it, rather than throwing up a prompt and hoping you click yes. There are also some other providers that add instant answers and other features, with a lot of the more interesting ones being those that are more about social and environmental responsibility. Most of the categories of results show up within their interface, but then Maps just redirects you entirely to Google Maps. See their documentation for more. If you&#39;re not regularly doing the kind of environmental research those extra features help with, I don&#39;t know if there&#39;s much of an advantage here but it may not be a meaningful step backwards either, while knowing you&#39;re helping plant trees instead of line investor accounts. It does also have an option for whether to have personalized search results, instead of assuming that obviously you want those to get slightly better results. I do think there are some cases where personalized results do help me. I do a lot of searches for Drupal things, and I might not always attach &amp;quot;Drupal&amp;quot; to the search terms, but my search engine with personalized results sometimes seems to skew the results toward it anyway. But it is still the right thing to allow people to decide whether that is worth more tracking. Also, as far as I can see, they don&#39;t try to do any tracking you anywhere else - just the searches that you make on their search engine if you enable it. Again, I&#39;m unlikely to change. I use Bing for a couple of reasons, one that is entirely selfish and one that is a bit more practical: Microsoft Rewards funds my Xbox habit. I haven&#39;t paid for a game in years. I just build up points and then buy old games on a $15 sale that I can play for 200 hours and by the end of that I&#39;ve got lots of points for more. Integrated SharePoint search. This is really useful if your organization does hold a lot of important information in SharePoint, as you can search it as fast as you could search the web. Ecosia isn&#39;t doing enough to make me want to replace those, as much as in principle I would love to support their cause. Maybe I could do like the browser: keep Bing for my work searches and switch to ecosia for personal searches. On personal searches the SharePoint aspect is a moot point, and I could probably still pick up enough reward points from professional searches that I could still keep my gaming backlog longer than I ever have much hope of catching up to. I&#39;ll keep considering that, maybe come back to see if it has evolved at all every few months.&quot;
    },{
      "url": &quot;/2025/comp-sci-ai/&quot;,
      "title": &quot;Computer Science Degrees in the Age of LLMs&quot;,
      "content": &quot;I recently was asked by a family member about the future of developer work in the age of large language models, more commonly referred to as &amp;quot;AI&amp;quot; but I dislike that label for misleading people into thinking it can do much more than it can. Her daughter is good at it and is interested, but they&#39;ve heard a lot of the hype that those jobs won&#39;t exist soon because an LLM can do it all. So, can it? Is the hype real? Are developer jobs in danger? LLMs Are Language Models, Not Thought Models Here&#39;s my general thinking on the matter: being a good developer takes a lot more than just writing semantically correct code. It means being able to think through questions like: What are we trying to accomplish? Often the proposed solution by somebody less informed about the technology and big picture is not really the best solution to the actual problem. Good developers will know to zoom out when they hear a request to make sure they know the real problem that they&#39;re trying to solve, not just writing code because somebody else said to. How does it fit in the context of everything else we&#39;re doing, technical systems or otherwise? What&#39;s the best user experience workflow? What are design principles we care about? What looks good? Who is excluded by doing it this way? Who is included by doing it more accessibly? Are there security risks to consider? Will anybody misuse this to harm others? An LLM is not designed to answer any of those. They may be able to answer some of it by accident, because there were enough similar situations in the training data. But ultimately, a language model is for modeling language, not for any of those questions, so you can&#39;t put too much stock in it. An LLM may be a good tool for those who are able to deal with those bigger questions, since they can essentially give you a first draft of code for a problem if you&#39;re able to describe it well enough and then continue with refining it from there. Being able to generate that first draft of code is valuable. I&#39;ve started using that more often. But generating that first draft from a well-defined requirement is not the majority of the job. It simply cannot do the majority of my job in any meaningful way. Of course, Cory Doctorow has said something to this effect: an LLM can&#39;t do your job, but a big tech salesperson might convince your boss that it can. In that case you are still out of a job, at least in the short term until those bosses learn the hard way and then have to return to trying to hire humans again. That sucks, obviously. That&#39;s not really a problem inherent to LLMs, though; that&#39;s just the nature of short-sighted employers always trying to save a little extra money on payroll. What About Junior Jobs? The more common and more nuanced take is to acknowledge my point above, that LLMs may be good at drafting some code but it isn&#39;t good at any other part of the job. But then they will still be concerned that LLMs could effectively take over junior dev jobs. If those junior devs don&#39;t have a chance to put in their time then they won&#39;t have the opportunity to be ready for those other questions. I&#39;m somewhat conflicted on how true this is. If a junior job really is only about regurgitating semantically accurate code without ever thinking about what it means, yeah, maybe it&#39;s true that an LLM can do it instead and that new wannabe developer never gets the skills necessary to get to the more senior jobs. That&#39;s never been my experience. My experience has always been that those junior jobs still want you to be thinking through those other questions. The difference from the senior jobs is simply that there&#39;s more acknowledging that you aren&#39;t as good at them yet so maybe there should be some more experienced voices in the conversation as well to guide you on the way (and those more experienced voices will get paid more). In case, an LLM still can&#39;t do your job. Maybe that&#39;s not the experience for every junior job. Maybe there really are some that only want you to spit out code to a specification defined by somebody else. Maybe they view the junior devs thinking too much about it beyond that as to be a waste of time. That ultimately has the same problem, though: there&#39;s no opportunity to learn thinking through those other questions. If those versions of junior dev jobs do exist, maybe they&#39;re more replaceable by an LLM, but the part about never having an opportunity to learn the bigger ways of thinking is the same either way. Conclusion: Degree or No? That&#39;s where I come back to the university degree question. My alma mater, Queen&#39;s University, explicitly told us in our orientation week that they were not trying to teach us to memorize syntax of a bunch of languages. They explained that a big part of that was because you would immediately hvae to learn a new language on the job anyway, since there are too many languages to learn them all just in case. They were instead trying to teach us how to think about the problems at that more abstract level which could applied to a lot of different contexts and different languages. The advent of LLMs have really underscored that point. If you don&#39;t learn how to deal with those questions, then yes, that&#39;s when you are in some level of danger that you aren&#39;t doing much more than an LLM. But if you can think at that level about what you&#39;re doing, the kinds of things that university degrees give you, you&#39;re not at risk anytime soon of an LLM replacing you. Maybe a university degree isn&#39;t necessary. I am not claiming any inherent superiority here because I got a degree. You certainly can develop those ways of thinking on the job as well. Certainly many employers would have the patience to help you learn that. Some others won&#39;t. If you want the fast-tracked version, when you have the time and money to devote to being as good as you can, without relying on a future employer giving you that time? The university degree can help a lot with that.&quot;
    },{
      "url": &quot;/2025/drupal-docker-deploys-overview/&quot;,
      "title": &quot;Drupal Docker Deploys - An Overview&quot;,
      "content": &quot;My Drupal development environment has gone through a few iterations over the past 4 years, some of which is chronicled here and on my GitLab. Since I last wrote about it, it went through another big update that includes now fully deploying the web service to a Docker swarm, not just pushing out code updates to each server. In this post I&#39;ll provide an overview of some of what changed, then I&#39;ll try to break down more details over some following posts. The Environments Local Environment Conceptually this first stage is similar to previous ideas, but it did need a lot of tweaks to get it to work in a way that also worked for the later plans. I&#39;ll get into those in a future post. In the meantime for those who haven&#39;t followed the previous incarnations, here is the overview of the key features: The VS Code devcontainer functionality handles everything. All I need to do is open the folder in VS Code with the devcontainer extensions enabled, then select the option to open in container. One Docker container for web (Apache and PHP, the main one we connect to), and one for the database. Volumes include the database as well as some key files that you might want to be able to edit from either within the container or directly on the host, mainly the files that help build the devcontainer like the docker-compose files. VS Code extensions are defined in the devcontainer.json so that every developer has the same tools installed and we don&#39;t have to remember what to set up each time. Web container is pulled from the GitLab container registry, saving time on rebuilding. DB container is not pulled from the GitLab container registry. This was considered, and a previous version before we got to Docker deploys was doing it. Now we have a better approach, where the script that starts when web is first loaded has an opportunity to load from an old database dump. Dev and Staging In our setup, we have two middle environments: dev and staging. Dev could also be called &amp;quot;proof of concept.&amp;quot; That is where we put code when we want to collect feedback from others about an idea before finishing it, or in rare cases when we need to test something that can&#39;t be as easily done on the local developer environment, like sending emails. Staging is the last stop before production; it only gets deployed there when we think it is ready for production. After that, it should only need bug fixes or other small and safe tweaks. They are both set up as Docker Swarms, but only one server ode, and only getting 1 replica of the web container as well as 1 replica of the db container deployed. This is because they are not public and don&#39;t get a huge amount of traffic. We do not need redundancy there. They have volumes for Drupal&#39;s public and private folders, on the same server. So dev and staging have almost everything on that one specific server: web containers, db containers, and volumes. The only exception is a volume for database backups, which goes in a network file share that is accessible by all of dev, staging, and production. I&#39;ll try to unpack why that is some more later. There are some differences in dev vs staging in terms of what gets built, e.g. dev has more developer tools enabled, while staging almost exactly replicates the production environment in its settings. For the purposes of this series of posts, they&#39;ll be largely treated as the same thing. Production Unlike dev and staging, production is 3 server nodes in a Docker swarm. The web container is deployed with 3 replicas, which does not necessarily mean one per node, but will overall balance along with any other services that are included. Public and private files are put in network file shares. This allows that no matter which swarm node happened to get the file uploaded to it, the other nodes will immediately have access to it. There&#39;s no risk that somebody uploads a file on server01 and somebody on server02 can&#39;t see it. In our previous setup, we had two servers, but a script had to run every 5 minutes on cron to keep them in sync. The database is on a different dedicated server. This is primarily for performance reasons. While having a database container running alongside on dev and staging is fine, it would likely not be able to keep up with heavy production loads. The Techniques There were a few general techniques that we had to use to get this fully working, which I&#39;ll overview here. One-Time Scripts There are some tasks that you want to be able to run on a manual basis on the containers. To set these up, we added extra services that default to 0 replicas deployed, but then a GitLab CI/CD job can launch it. Some of those will be demonstrated in more detail later. This is the CI/CD job that gets extended for those single-use scripts: The full version looks complicated mainly because of everything needed to show logs back to the GitLab CI/CD interface, but the core part is really just this: docker service update --with-registry-auth --replicas 1 --force ${STACK_NAME}_${SERVICE_NAME}, running the specified service which contains the script we want to run. Dockerfile Branching Dockerfiles can have logic built-in to build a little bit differently depending on the environment. For example, we don&#39;t need npm to support Playwright accessibility testing tools on production. That is only needed on local, where we do the building and the testing. This is done by passing in a variable for environment as part the build: build: extends: .build_dockerfile variables: IMAGE_TAG: web DOCKERFILE_PATH: ./Dockerfile.Drupal BUILD_ARGS: --build-arg BRANCH=$CI_COMMIT_BRANCH --build-arg BASE_URL=$BASE_URL --build-arg TIMESTAMP=$CI_COMMIT_TIMESTAMP That is extending the build_dockerfile job defined elsewhere. That job looks like this: ## General job for building from a Dockerfile. ## .build_dockerfile: stage: build image: docker:dind services: - docker:dind variables: DOCKER_BUILDKIT: 1 IMAGE_TAG: web # Defines image tag to go into the project&#39;s container registry, with a tag of the branch name. # FULL_CONTAINER_PATH: $CI_REGISTRY_IMAGE/$IMAGE_TAG:$CI_COMMIT_REF_SLUG # Include copying git submodules. # GIT_STRATEGY: clone GIT_SUBMODULE_STRATEGY: recursive GIT_SUBMODULE_UPDATE_FLAGS: --init GIT_SUBMODULE_FORCE_HTTPS: &#92;&quot;true&#92;&quot; # Point to the Dockerfile that should be built. # DOCKERFILE_PATH: ./.devcontainer/web.Dockerfile BUILD_ARGS: script: # Login to the GitLab registry using the private unique password provided by GitLab. # - echo $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER --password-stdin $CI_REGISTRY # Build the Docker image. # - docker build --pull -t $FULL_CONTAINER_PATH -f $DOCKERFILE_PATH . $BUILD_ARGS # Push the Docker image to the project&#39;s container registry. # - docker push --quiet $FULL_CONTAINER_PATH - docker logout The relevant sections in the Dockerfile look like: ARG BRANCH RUN if [ &#92;&quot;$BRANCH&#92;&quot; != &#92;&quot;main&#92;&quot; ]; then &#92;&#92; apt-get -y --no-install-recommends install &#92;&#92; nodejs &#92;&#92; npm &#92;&#92; ;fi Multiple Docker-Compose We also found we needed multiple docker-compose files for a couple of unique scenarios, including the distinction of the production network file share volumes as compared to the host machine volumes on dev and staging. These can be merged together in a GitLab CI/CD job. For example, here&#39;s the dev deploy job, with multiple docker-compose: dev_deploy: stage: deploy extends: .deploy_template_swarm allow_failure: false environment: Development variables: STACK_NAME: library COMPOSE_FILES: docker-compose.yml:docker-compose.local.storage.yml LIBWEB_REPLICA_COUNT: 1 LIBDB_REPLICA_COUNT: 1 tags: - dev only: refs: - dev In addition to the general docker-compose, there&#39;s another one for the local storage, telling it save the Drupal public and private files. The deploy job which that extends looks like this, able to handle parsing the multiple compose files: .deploy_template_swarm: stage: deploy retry: max: 2 when: - script_failure variables: STACK_NAME: &#92;&quot;&#92;&quot; COMPOSE_FILES: docker-compose.yml script: - echo &#92;&quot;$CI_REGISTRY_PASSWORD&#92;&quot; | docker login $CI_REGISTRY -u $CI_REGISTRY_USER --password-stdin - if [ -z $COMPOSE_FILES ]; then export COMPOSE_FILES=docker-compose.yml; fi - &gt;- docker --log-level fatal compose $(printf &#39; -f %s &#39; $(echo $COMPOSE_FILES | sed &#39;s/:/ /g&#39;)) -p $STACK_NAME config -o docker-compose.rendered.yml - sed -i &#39;/^name:.*$/d&#39; docker-compose.rendered.yml - docker stack deploy -c docker-compose.rendered.yml --with-registry-auth --detach=false $STACK_NAME - docker logout artifacts: when: always paths: - docker-compose.rendered.yml&quot;
    },{
      "url": &quot;/2025/principles-ai/&quot;,
      "title": &quot;Principles of AI Use&quot;,
      "content": &quot;Let&#39;s set aside a lot of the other ethical questions about &amp;quot;AI&amp;quot; - by which I mean large language models - for a minute. I never really recommend setting aside ethical questions, but I do want to write about something else right now. Are these &amp;quot;AIs&amp;quot; useful? What for? What are they counterproductive for? As I&#39;ve thought about this, I have so far settled on two main principles for the types of tasks that I think are a suitable fit for large language models, commonly referred to as &amp;quot;AI&amp;quot;: Validatable Guideline number one: only use it for tasks where there is a human with adequate knowledge able to validate whether it actually accomplished the desired goal. Otherwise, it might be what you wanted 90% of the time, but you have no way of knowing what the other 10% is. An analogy I saw on Mastodon was that of a boat: a boat that gets you 90% of the way can be useful, but only if you&#39;re a strong enough swimmer to finish the other 10%. Otherwise, now you&#39;re just stuck in the middle of the water with no way to finish your travel. The analogy if you aren&#39;t even able to identify which is the 10% it got wrong is a little rougher, because it would have to be something like not even realizing that you&#39;re drowning. So here&#39;s some uses which do qualify: Generate code in response to a well-defined prompt by a programmer who can then review the code and plug it in to their dev environment to see if it does what they wanted it to do, running it against manual and automated tests. That can be pretty useful for that stage of generating a first draft of code. Write a first draft of a document from some bullet points you provide, and then you review the results before doing anything else with it. Have it analyze your own writing or speaking for you. I&#39;ve heard podcasters mention the idea of feeding in a bunch of their work and asking it to help them identify any ways they could improve. Sometimes LLMs are referred to as &amp;quot;spicy autocorrect&amp;quot; and sure, that&#39;s a derogatory phrase but it does name one thing that it is pretty good at. Conducting a faster natural language search that can better understand your question and combine several sources together for you, with links back to those sources so you can verify anything that isn&#39;t quite right. One of the common examples is planning a vacation, and it might help with a big picture first overview that helps you see how all the pieces can fit together, but you&#39;re still going to need to validate all those pieces, like whether that proposed attraction is even open right now. This was the aspect that I started seeing value in a lot faster with the earlier public models, not in the results but in the ability to understand my question even when I don&#39;t know the exact jargon keywords. There are also some things which definitely do not qualify: Using it in place of a therapist. You might be able to quickly validate that it made you feel a little less lonely, but that&#39;s not the same thing as being made more mentally well. This has been in the news a lot and I am confident in saying that it might make you feel better short term simply because you feel safe expressing yourself (you probably shouldn&#39;t, as a lot of the models will use your input to train the model, but I know if you&#39;re in crisis you aren&#39;t thinking about that). But it has no expertise, no humanity, and no awareness of who you are or your context other than a statical calculation of what words should come next in a response to your query. Summarizing somebody else&#39;s writing that you&#39;ve never read before. You have no way of knowing what it got right and what it got wrong without at least skimming the whole thing yourself. I recently read about a Canadian cabinet minister proudly talking about how he fed a proposed bill into a service that turned it into a podcast he could listen to. That should be terrifying. He has no idea what&#39;s actually in the bill. It&#39;s his job to make major legislative decisions and lead within his portfolio, and he can&#39;t even be bothered to read the proposed legislation? At least have a human expert staffer summarize it for you. Having it completely write and send messages or documents for you without your careful review. Any use in education where the purpose of the work is to learn and develop the skills. Even when an LLM gives you the &amp;quot;right answers&amp;quot; it has completed defeated the real purpose in the same way that copying off your friend does. It&#39;s like having a machine lift weights for you at the gym; sure the weights got moved but that wasn&#39;t the actual goal. The actual goal was to develop your muscles and the weights were just a tool to help you get there. Finally on this point, I&#39;ll add a bit more nuance to the guideline: sometimes it is validatable, but the validation is going to take longer than if it you done it yourself. Some research has shown that yes, it can save a lot of time in the brainstorming and drafting stage, but then adds a lot more time requirement to the editing stage, such that it is about the same amount of time in total. So the question is not simply whether it is validatable, but whether it is validatable in less time than it would have taken to do it all yourself. Know the Model Guideline number two: Don&#39;t use a model that is designed for something different than you are trying to get out of it. The big one currently: don&#39;t use a language model for things other than modelling language. It&#39;s not a fact engine. It&#39;s not a math engine. It&#39;s a language model. If you want to model language, that makes sense. If you want it to answer a factual question, that&#39;s the wrong tool for the job. If you want it to be your friend or therapist, that&#39;s the wrong tool for the job. If you want it to solve math problems or tell you how many r&#39;s there are in the word strawberry, that&#39;s the wrong tool for the job. There might be other models that are built for those kinds of problems, or it might be that machine learning techniques are never the right tool for that kind of job, but you should not assume a language modelling tool to be great at anything other than modelling language. The use of the term &amp;quot;AI&amp;quot; is a big part of what is misleading here, in my opinion, as most people assume that means it can do everything at a superhuman level. It can&#39;t. Certain models are built to do certain things, and the majority of what we are talking about as general &amp;quot;AI&amp;quot; are language models. Unfortunately those who are not highly technically literate hear &amp;quot;AI&amp;quot; and are imagining superintelligent machines, either benevolent or evil, from science fiction. The corporations trying to sell you on relying on it more and more are not in any rush to correct this misconception, and some of them may even genuinely believe they&#39;re only a few months away from achieving the sci-fi version. The Ethical Questions Ok, I know I said we could set those aside for this post, but I&#39;ll just quickly name a few questions you should consider: Is it a shortcut to learning something that increases your skills? Or are you offloading cognitive processes because it saves some time in the short-term? If the latter, are you comfortable losing those skills? What will you do when the enshittification inevitably occurs, with raised prices and/or harmful bias introduced? Do you know if it is training on the data you input to the query? Are you comfortable with the possibility that your query might become an answer to somebody else? Most of the business-class subscriptions will guarantee that it won&#39;t train on your queries, but the personal and free ones usually will be training on it so you really need to be careful what you put in. Are you comfortable with the environmental consequences with the demands for more water and power? As I understand it, this isn&#39;t nearly the biggest part of our climate crisis, but it&#39;s not nothing, so you still need to decide if it is worth it. How will it impact content creators over time? Rhetoric about it being a &amp;quot;plagiarism machine&amp;quot; is overblown, but there is still a power differential with these extremely rich companies getting even more rich based on content written by others who do not get the same benefits. How will it change journalism on the internet, if people don&#39;t feel the need to ever open the actual article and see ads or prompts for a paywall? And will we get so much content generated by language models that it drowns out the better content prepared carefully by humans? There is a lot to think about. While it will be interesting to see how things evolve, we also need to remember that we get some small vote in how it evolves. Nothing is inevitable unless we choose societally to make it inevitable; any claim otherwise is pure marketing hype.&quot;
    },{
      "url": &quot;/2025/ai-assist-vs-code/&quot;,
      "title": &quot;Experiments in Coding Assistants in VS Code&quot;,
      "content": &quot;I have little interest in using Large Language Models for much other than coding, and that only as long as you are still validating that the result is correct and learning from it, not using it to offload all your cognitive processing, but I did decide it was time to give that specific situation a more serious try. I work mostly within VS Code, so whatever I&#39;m going to use I want to be fully integrated to that. This does get complicated, because there are a few overlapping factors: Which underlying model is best for which kinds of problem? What does each model cost, and does it make more sense to pay per-token or monthly? What VS Code extensions combined with what models provide what kind of integration with VS Code? Can it read multiple files to keep in context, offer suggestions within files, run scripts, etc.? How user-friendly are all those things? How secure are those things? Because there are too many overlapping variables, I&#39;m not really going to try to give definitive answers to those. I&#39;m only going to provide some notes on what I have tried and what I thought about it. Copilot Copilot was the first of these coding assistants, before anybody was even calling these kinds of tool &amp;quot;AI&amp;quot;, and at least in theory is the most deeply integrated to VS Code. In terms of modes, it offers a chat mode, an edit mode, and an agent mode. Chat is fairly self-explanatory and if you&#39;ve used an LLM elsewhere, that&#39;s what it is: no touching the files, just a simulated conversation. I assumed edit mode meant that it would suggest edits to my files. The description of that mode sounded that it would. Maybe the task I gave it - writing unit tests for a custom Drupal entity - was deemed too complicated to be allowed to run in that mode, but it didn&#39;t make it clear; it just gave me a response in chat. So I switched to agent mode for the rest of the testing and that&#39;s where the real power seems to come in, able to suggest changes to files and suggest non-destructive terminal commands. It was not very friendly in switching from Chat to Agent. If you start out thinking you just need a chat, then realize it would actually help to see the suggestion being applied to the code, you can&#39;t do that without starting the conversation again. Copilot also can suggest code as an autocomplete function right within your main code editor as you work. I can see why some people like that better. It is maybe faster once you get used to it. I tend to feel like it&#39;s in my way, breaking my own concentration too much, and I&#39;d rather restrict it to acting when I tell it to. It does require logging in with a GitHub account, even for free usage, which might be a nuisance if you don&#39;t want to have a GitHub account, but realistically a large portion of the kinds of developers who would use it have a GitHub account anyway. I could not figure out adding context files to my chats. When I clicked on the button to add context, it gave me a file browser, but if I tried to browse down to the file I wanted, it just made the whole folder the context instead. I did not know how to browse down to more specific context. It does seem to read the entire codebase without the human user specifying what to read or asking for permissions, which means it would be using up more tokens but if you&#39;re paying monthly it doesn&#39;t matter as much. More significantly, you better make sure there&#39;s nothing sensitive in that codebase that it could be reading and feeding back into a free model. It gives access to a few models even in the free tier, including Gemini Flash 2.5, Claude Sonnet 3.5, and GPT 4o, which are the three that I tried to get to write unit tests for a custom entity in a Drupal 10 module. Perhaps this is the best thing to say about Copilot, in that it allows trying all the models without setting up accounts and API keys for all of them, especially since most of them do not have a certain amount available for free. All of those tests would have cost me (a little) money and the hassle of creating accounts otherwise. With those general notes out of the way, how did the different free models do at a task writing Drupal unit tests? With Gemini Flash I first tried to solve the problem with Gemini Flash 2.5 as the model. It did not do well. It made a couple truly nonsensical mistakes, like removing the php declaration at the start of the file or sticking a newline character in the middle of a use statement, both of which obviously become a fatal error right away. These are the kinds of mistake that a basic linter would detect, so it was very strange to see them made here. A couple of times it said it made a change in the file, but didn&#39;t actually do it. That was also confusing. Ultimately, it got stuck pretty quickly, not able to figure out what else to do and not having made any significant progress on the problem. Maybe there was something about my use case where I missed an important step, but based on those tests, it definitely was not helpful. With Claude Sonnet 3.5 I then tried Claude Sonnet 3.5, still in Copilot. That seemed promising at first: fixing issues and applying style standards that I did not even explicitly ask it to do. It then asked for terminal permission to run the tests, then kept improving on its own based on the errors it read from the test output. That was nice, not needing to run the test command myself every time but still confirming permission so I could stop it if it tried to do anything dangerous. It then got stuck, though not nearly as fast as the Gemini model did. It was actually changing things reliably, and the errors were different (whether that was better or worse different I can&#39;t say). But it did eventually also give up, aware that it wasn&#39;t working but having tried everything it could &amp;quot;think&amp;quot; of and stuck in a loop of the same few ideas repeatedly. With GPT 4o With GPT 4o and Copilot the results were similar to Claude Sonnet. One minor difference was that it asked in the chat first if it could run the tests, and I had to type yes, then it gave the prompt asking to run the command line Claude had done and I clicked approve on that. So it made running the tests each time slightly easier than Gemini but less easy than Claude on that point. Then it did read the results and kept iterating from there. It eventually stopped, thinking it got it working, and I ran the tests again myself. It had a couple fatal errors, so I fed those back into Copilot and it ran again for a while. Then it got stuck in a loop, much like Claude&#39;s loop except that it declared itself done without even running the tests and I had to run them myself. Gemini Code Assist I briefly tried the official Gemini Code Assist extension from Google. It&#39;s a very friendly and straightforward interface. It says it has an Agent mode in preview. I tried in once and it couldn&#39;t even find the file that I was asking it to review. I also noticed immediately it did not have a couple of nice features which I had already learned that the extension I&#39;ll get into next offers. It&#39;s just a user-friendly chat integration that&#39;s easy to use if you already have a Google account as most people do and not much more than that. Later update (December 12, 2025): mainly because cline has been somewhat unreliable, I&#39;ve tried Gemini Code Assist again. It has improved a little, but is also still frequently buggy, failing to connect to the model occasionally with no helpful error. The even bigger discovery, though, is that unlike the others, if you use this with a Google account that has a Google AI Pro subscription (including the free trial I got with my Pixel 10 Pro), you get much higher usage limits. I&#39;m not going to use it nearly enough to justify paying $270 CAD/year to keep it after my free trial, but it is definitely worth trying to make full use of it in the few months I have left. Cline That gets me to the extension that I have now tried out the most and like the most so far: Cline. Cline allows connecting to a wide variety of models, using your own API keys. Unlike Copilot, there&#39;s no middle man where you could instead pay them and get a lot more access to a bunch of models. I grabbed an API key for Gemini and put it in. I had hoped that I had unlimited use of it for a year, because with my new Pixel 10 Pro I have Gemini Pro for a year, but that free trial does not include API access for coding, which is really the only thing that I wanted it for. Anyway, the extension makes it as easy as possible to add an API key for the model of your choice. It offers two modes: Plan and Act. Plan won&#39;t write any code for you, just be a chat agent similar to having a chat in the browser, except that it can work with more valuable context of whatever in your codebase you decide to provide as context. Act is the Agent mode, which like Copilot is where it gets really interesting. It will tell you what it is going to do in response to your request, then it will propose the changes. Those proposed changes will show up in the main code window like any other diff viewer that you are probably used to in Code. This gives you an easy way to track exactly what it is changing, one file at a time. There is an option to auto-approve whatever it wants to change, but to me, that violates the rule that I only want to learn from this, not offload my thinking onto it. I want to make sure I am going slow enough to understand exactly what it is doing and why, so I am not going to auto-approve. There are a couple other features of Cline that I really like: You can define a rules file which gives it extra standard instructions. These can be set up per project, but then the front matter in each rule can differentiate even more by file type. For example, I can tell it to always use Drupal style and best practice if I&#39;m editing a PHP file within a Drupal project, and that rules file can be committed as part of the project so everybody working on the project gets the same guiding rules. It shows you the amount of tokens and the cost so far of this query, which is a great way to keep track of how much this is costing you and encourages you to not use it any more than you really need to. Like Copilot, it can also run script commands, like clearing caches or executing the phpunit tests it tried to write. It was a more clear interface about those terminal suggestions, prompting you first to save the changes and then to run the test instead of showing you both buttons at once. It was a really positive experience at first, better in every way than Copilot, other than Copilot giving access to a couple more models for free. Then Cline started failing on me, frequently saying that &amp;quot;the model is overloaded&amp;quot; even though when I tried the same model from the other extensions they were fine. Because I had to keep querying repeatedly when it failed, it used up my daily free quota on Gemini Pro a lot faster. I hope this is a temporary short-term bug, because it wasn&#39;t a problem at first. A Note on Pricing Thanks to the Cline details on what the price would be if I was past the quota for free usage has given me some rough estimates of costs when paying by token or by subscription. Here&#39;s basically what I found, using Gemini Pro as the model. Just loading in the context of all of a moderately-complicated Drupal module cost me about $0.09. As I worked through a moderately-complicated problem that needed a few tweaks, I got up to a total of about $0.40. I gave it a few more problems of similar size that ended up with similar price. I assume that&#39;s USD, but maybe one minor complaint about Cline is that it does not specify that at all; maybe it is CAD knowing it is tied to my Canadian Google account. That&#39;s for it reading my code context, making code changes, then tweaking a little more as I followed up with a few pieces that weren&#39;t quite ideal yet. By comparison, it&#39;s $19 USD a month for the Copilot Business subscription. So assuming about 22 work days in a full month (no vacations or anything), that&#39;s roughly equivalent to 2 of those kinds of problems per day before the subscription starts being cheaper than by token. If you&#39;re handing off all your work to it, a subscription would pay off, but as I already said, I do not recommend that approach. If you&#39;re more like what I am somewhat interested in, using it only occasionally when you&#39;re stuck - the way I also use search engines - and maybe some general code review, then the per-token price is likely to be a better deal. Of course, that&#39;s just using Gemini Pro&#39;s pricing, and with a small sample size of the problems. Other models would be different. How much contextual code you provide might change it dramatically. How big of the problem it is solving would certainly change it dramatically. I offer this only as a rough estimate to give a sense, but I think the general conclusion is likely true: per-token makes sense as long as you&#39;re using it to help you do work, not to do your work for you. Conclusions? As I said in the introduction, I don&#39;t have a lot of strong conclusions to make because there are too many interconnected variables and these tools are all evolving fast enough that any conclusions may be not be valid for that long anyway. What I can say at this point: Google Gemini Code Assist is bad and I see no good reason to use it unless you have a Google AI Pro membership anyway, in which case the worse experience might be an acceptable cost for saving money. Copilot is great for experimenting with models, and might also be a better choice if you are going to use it enough as to justify the monthly subscription. Cline is my favourite to interact with, but those &amp;quot;overloaded models&amp;quot; disconnections got pretty frustrating pretty fast.&quot;
    },{
      "url": &quot;/2025/types-search/&quot;,
      "title": &quot;Types of Search and the Best Tool for Them&quot;,
      "content": &quot;I&#39;ve been thinking a lot recently about which tools are best for answering what kinds of questions, largely prompted by two things: The rise of large language models (LLMs). Hype in tech circles around Kagi as as an alternative search engine where you pay a monthly fee instead of being barraged by increasingly terrible results swayed by advertising dollars. I signed up for the free tier of Kagi to try it out. The free tier has a limited amount of searches, and does not include the &amp;quot;AI&amp;quot; chatbots, but it is still enough to get a good sense of the core search experience. It has made me realize there are essentially three categories of things I use a search engine for, and which tool is best for the job varies by that category. Searching for a Site This is the most &amp;quot;traditional&amp;quot; search engine: you put in some keywords looking for a site, then you click on the site, and you&#39;re happy. This is great if you know a keyword or two that will get you to those one or few sites that get you what you want. I do this kind of search regularly for searching by a person or business&#39; name with the expectation that I am going to visit their website. I also use it when looking for documentation of specific programming functions or tools, where I want to load up the authoritative page and dive deep into it. For this category, Kagi is the winner. In the free version, there&#39;s no AI answer in the way at all; my understanding is on the paid tiers it is there but out of the way. There are no ads. There&#39;s no tracking to sell all your data. There are simply the results, and by all accounts they are at least comparable to if not better than the big search engines. There&#39;s also some freedom to tweak your own search algorithm in some small ways, like if you are searching for programming results you can filter to certain sites. You can also customize your own CSS styles to make it look however you want. Those kinds of customizations are cool and maybe worth it if you are going to be using it a lot to justify the overhead of configuring it. As I realized, though, a lot of my searches really fall in either of the other two categories below, so even if I do end up paying for a Kagi subscription, I might not ever bother with too much extra customizing. Searching for Common Quick Answers The kinds of searches I realized I do a lot are those when I quickly want the status or summary of something. My most common is sports scores or schedule. I type in that I want WNBA scores, and I see WNBA scores. If the games are live, they will update in place. For this kind of search, I don&#39;t need to go into another site; it surfaces everything I need quickly and in a user-friendly way. I tried a couple of these in Kagi and it was not great. Bing, on the other hand, I&#39;ve been happily using to do this for years. The scores even update live in place so I can loosely follow a game while doing something else. Some other examples might include: Definition of a word Movie summary with links to cast members and what other movies they were in Flight information Time in a certain timezone Monetary conversion I tried to look up if there was a technical term for these kinds of searches and results, and there isn&#39;t a clear winner but I saw some things like &amp;quot;direct answer,&amp;quot; &amp;quot;featured snippet,&amp;quot; &amp;quot;knowledge panel,&amp;quot; and &amp;quot;zero-click searches.&amp;quot; Searching for Help to a Complicated Problem Then there is searching for help with complicated problems that don&#39;t have clear mappings between a couple keywords and one site or even a few sites that clearly answer it. In my work, this mostly means large technical problems that I don&#39;t even have a great idea of where to start. With &amp;quot;traditional&amp;quot; search, I would often be doing 15 different searches trying different keyword combinations, sometimes learning from the results of one which jargon can help get me to the next step, and reading multiple websites of results for each query. That&#39;s really time consuming and often hard to even keep track of what else you can try searching for. 3 hours of digging through forums later, many of which were clear dead ends or contradictory to each other, I&#39;m having trouble holding all the pieces together in order to come up with a coherent solution. This is the scenario where large language models (aka &amp;quot;AI&amp;quot;) are better. Most of the hype, both from the salespeople and from the critics, focus on the generating of the response. It is just as important that it can connect you to what you&#39;re looking for even if you don&#39;t get the exact keywords correct. If you&#39;ve got a complicated problem and you don&#39;t know the exact right jargon to search with for each component of the problem, using an LLM is a real improvement because it is able to connect your wording to the jargon in the dataset. It will also help connect all those pieces together, so if you&#39;re investigating some interaction of 12 different factors in your environment, you can lay out all those factors at once and it will keep that information in context for its results. It is a reversal after years of learning to search by succinct keywords to learning to prompt with as much context as is relevant, but it does return better results when you do. No, it isn&#39;t truly searching in the sense of pointing you to existing information on the Internet. It is doing a statistical calculation of what words should come next based on the training data. But for this kind of problem, the statistical calculation is often more useful than simply returning a list of forum pages that may or may not solve one small component of the problem. Often there is no one page that has a clear answer or even a good starting point, but there is a lot of sort of related content out there which the LLM has ingested and will stitch together. And of course you can&#39;t simply assume it is correct - you still need to validate it - but that was also true with grabbing code snippets from a forum. All that&#39;s different is that you can get to that snippet much faster, with a lot less headache and a lot more hope of actually understanding it. It is absolutely true that there is a lot of bad marketing hype about large language models and a lot of use cases that do not live up to that hype which includes many that you probably should just do a normal search. Despite that, I have become fairly convinced that this is the category of scenario where they are legitimately an improvement.&quot;
    },{
      "url": &quot;/2025/drupal-block-heading-level/&quot;,
      "title": &quot;Drupal Block Heading Level&quot;,
      "content": &quot;The Problem Heading levels are important, for accessibility as well for site style consistency. However, it&#39;s not always easy to select the correct heading level in all aspects of Drupal. Specifically, it&#39;s not the easiest to change the heading level on blocks, which could vary depending on where in the site theme the block is going to be placed. There is the twig template system and the heading level is usually hardcoded in there. If you only have one block template file and you want the heading level to always be the same, that&#39;s pretty easy to set it in one template file. Once you lose those two assumptions, though, that gets more tricky. It&#39;s still possible, but if you&#39;re changing a lot of blocks, you could end up with a mess of files that are harder to keep track of and to implement any future changes to blocks because some of the changes need to go to all the template files, some of them to one template file, some of them to some other subset in between. The Solution Instead of having that mess of template files, I decided to make a module that allows changing the heading level in configuration. The whole module is available in my GitHub. Site-Wide Default There&#39;s a setting for a site-wide default. It&#39;s probably true that most blocks on the site are going to need to be an h2 or an h3, and this allows setting that default to make the rest easier from there. Here&#39;s the config schema for that option: block_label_heading.settings: type: config_object label: &#92;&quot;Block Label Headings settings&#92;&quot; mapping: heading_level: type: label label: &#92;&quot;Site&#39;s default heading level&#92;&quot; The form to change that is in src/Form/BlockLabelHeadingSettingsForm.php, which includes these functions to build and submit the form: /** * {@inheritdoc} */ public function buildForm(array $form, FormStateInterface $form_state): array { $config = $this-&gt;config(&#39;block_label_heading.settings&#39;); $form[&#39;heading_level&#39;] = [ &#39;#type&#39; =&gt; &#39;select&#39;, &#39;#title&#39; =&gt; $this-&gt;t(&#39;Default Heading Level&#39;), &#39;#default_value&#39; =&gt; $config-&gt;get(&#39;heading_level&#39;) ?? &#39;h2&#39;, &#39;#description&#39; =&gt; $this-&gt;t(&#92;&quot;This can be overridden per block.&#92;&quot;), &#39;#options&#39; =&gt; [ &#39;h2&#39; =&gt; &#39;H2&#39;, &#39;h3&#39; =&gt; &#39;H3&#39;, &#39;h4&#39; =&gt; &#39;H4&#39;, &#39;h5&#39; =&gt; &#39;H5&#39;, &#39;h6&#39; =&gt; &#39;H6&#39;, ], ]; return parent::buildForm($form, $form_state); } /** * {@inheritdoc} */ public function submitForm(array &amp;amp;$form, FormStateInterface $form_state): void { parent::submitForm($form, $form_state); $this-&gt;config(&#39;block_label_heading.settings&#39;) -&gt;set(&#39;heading_level&#39;, $form_state-&gt;getValue(&#39;heading_level&#39;)) -&gt;save(); // Clear all blocks to reflect any change. Cache::invalidateTags([&#39;block&#39;]); } Per-Block Setting Then there is an option per block to override that default. The schema for that looks like this: block.block.*.third_party.block_label_heading: type: mapping label: &#92;&quot;Block label heading settings&#92;&quot; mapping: heading_level: type: string constraints: Choice: choices: [&#92;&quot;h2&#92;&quot;, &#92;&quot;h3&#92;&quot;, &#92;&quot;h4&#92;&quot;, &#92;&quot;h5&#92;&quot;, &#92;&quot;h6&#92;&quot;] This adds the option in the third_party section for any block. The form is updated with a hook in the .module file: /** * Implements hook_form_block_form_alter(). * * Adds the option to the block form. */ function block_label_heading_form_block_form_alter(array &amp;amp;$form, FormStateInterface $form_state, string $form_id): void { $form_object = $form_state-&gt;getFormObject(); if ($form_object instanceof BlockForm) { /** @var &#92;&#92;Drupal&#92;&#92;block&#92;&#92;Entity&#92;&#92;Block $block */ $block = $form_object-&gt;getEntity(); // Pin the Title fields at the very top with explicit weights, // so the new option will be grouped with it. if (isset($form[&#39;settings&#39;][&#39;label&#39;])) { $form[&#39;settings&#39;][&#39;label&#39;][&#39;#weight&#39;] = -100; } if (isset($form[&#39;settings&#39;][&#39;label_display&#39;])) { $form[&#39;settings&#39;][&#39;label_display&#39;][&#39;#weight&#39;] = -99; } $options = [ &#39;h2&#39; =&gt; &#39;H2&#39;, &#39;h3&#39; =&gt; &#39;H3&#39;, &#39;h4&#39; =&gt; &#39;H4&#39;, &#39;h5&#39; =&gt; &#39;H5&#39;, &#39;h6&#39; =&gt; &#39;H6&#39;, ]; $form[&#39;settings&#39;][&#39;block_label_heading_level&#39;] = [ &#39;#type&#39; =&gt; &#39;select&#39;, &#39;#title&#39; =&gt; t(&#39;Heading level for block title&#39;), &#39;#options&#39; =&gt; $options, &#39;#default_value&#39; =&gt; $block-&gt;getThirdPartySetting(&#39;block_label_heading&#39;, &#39;heading_level&#39;, &#39;h2&#39;), &#39;#description&#39; =&gt; t(&#92;&quot;Choose the semantic heading level used to render this block&#39;s label, as is most appropriate for where it will be placed in the site.&#92;&quot;), &#39;#weight&#39; =&gt; -98, ]; $form[&#39;#entity_builders&#39;][] = &#39;block_label_heading_block_entity_builder&#39;; } } The Block Template The last step is to get those configured headings into the displayed template, without needing to hardcode a separate template for every block. The module will pass a variable through to twig using this hook implementation: /** * Implements hook_preprocess_block(). * * Adds variable for templates to customize block heading level. */ function block_label_heading_preprocess_block(array &amp;amp;$variables): void { $block_id = $variables[&#39;elements&#39;][&#39;#id&#39;] ?? NULL; $variables[&#39;heading_level&#39;] = &#92;&#92;Drupal::service(&#39;block_label_heading.utils&#39;)-&gt;getHeadingLevel($block_id); The last step cannot be done by the module, as it is to change the theme implementation. Now that you have that heading_level variable that you can use in a twig template, the label portion of your block template file may look something like this: {% if label %} {% set tag = heading_level|default(&#39;h2&#39;)|lower %} {# Avoid aria-level conflicts injected elsewhere. #} {% set title_attributes = title_attributes.removeAttribute(&#39;aria-level&#39;) %} &amp;lt;{{ tag }}{{ title_attributes.setAttribute(&#39;id&#39;, heading_id) }}&gt;{{ label }}&amp;lt;/{{ tag }}&gt; {% endif %}&quot;
    },{
      "url": &quot;/2025/expand-collapse-details/&quot;,
      "title": &quot;Expanding and Collapsing Details Elements&quot;,
      "content": &quot;The Problem with Collapsed Details Details elements, commonly referred to as accordions, are a very valuable tool for tucking away some content into sections when not always needed, especially on pages that would be very long with lots of content that would take a while to scroll down if they were all expanded at once. However, they do have some problems as well, namely that it&#39;s easy to miss things that are hidden inside details elements. You can&#39;t use Ctrl+F, or other utilities that search everything on the page, to find them. The Solution A solution is to include buttons on the page, before the details elements, that when pressed will expand all the details elements. One click, and now you can use Ctrl+F or get a better overview of how much content you&#39;re dealing with again! I have a Drupal module for this available in my GitHub. Here are some of the key components of that code: The Block The block itself looks like this, as a class in the src/Plugin/Block folder: &amp;lt;?php namespace Drupal&#92;&#92;expand_details&#92;&#92;Plugin&#92;&#92;Block; use Drupal&#92;&#92;Core&#92;&#92;Block&#92;&#92;BlockBase; use Drupal&#92;&#92;Core&#92;&#92;Cache&#92;&#92;Cache; use Drupal&#92;&#92;Core&#92;&#92;Render&#92;&#92;Markup; /** * Displays button for expanding or collapsing accordions. * * @Block( * id = &#92;&quot;expand_details_block&#92;&quot;, * admin_label = @Translation(&#92;&quot;Expand and Collapse Details Buttons&#92;&quot;), * category = @Translation(&#92;&quot;Expand Details Module&#92;&quot;), * ) */ class ExpandDetailsBlock extends BlockBase { /** * {@inheritdoc} */ public function build(): array { return [ &#39;#markup&#39; =&gt; Markup::create(&#39; &amp;lt;button type=&#92;&quot;button&#92;&quot; name=&#92;&quot;Expand&#92;&quot; aria-label=&#92;&quot;Expand all content on page&#92;&quot; class=&#92;&quot;expand-details-button expand-details-button-expand&#92;&quot;&gt; Expand All + &amp;lt;/button&gt; &amp;lt;button type=&#92;&quot;button&#92;&quot; name=&#92;&quot;Collapse&#92;&quot; aria-label=&#92;&quot;Collapse all content on page&#92;&quot; class=&#92;&quot;expand-details-button expand-details-button-collapse&#92;&quot;&gt; Collapse All - &amp;lt;/button&gt; &#39;), &#39;#attached&#39; =&gt; [ &#39;library&#39; =&gt; [ &#39;expand_details/expand_details&#39;, ], ], &#39;#cache&#39; =&gt; [ &#39;contexts&#39; =&gt; [&#39;theme&#39;], &#39;tags&#39; =&gt; [], &#39;max-age&#39; =&gt; Cache::PERMANENT, ], ]; } } If you haven&#39;t built a custom block before, a couple pieces of this are essential: That location in the file structure, src/Plugin/Block and a filename that matches the class name, so it can be properly recognized as a block plugin. The comment block with those keys, which specify a machine name and how it will show up in the Block Layout interface. Otherwise, the build function determines the markup of that block. Along with the markup itself, it also declares: There is an attached library, which will connect to the next important file. This is how JavaScript and CSS are added to the block. The cache is straightforward, only altering by theme and never clearing automatically. This is because there is nothing here that would change by page or by user or any other contextual variation so it can stay cached and be highly efficient. The Library The module&#39;s .libraries.yml file declares any libraries for the module, which in this context mean a bundle of CSS and/or JavaScript that can be applied to only certain contexts. In this module, there is just one library and we already saw that it would only be added to the context of that block, so this CSS and this JavaScript doesn&#39;t need to be loaded on any other page. expand_details: version: 1.1 js: js/expand_details.js: {} css: theme: css/expand_details.css: {} One more note here is the version number. When you are working with JavaScript, increment the version number after changes. If you don&#39;t, you might not see your changes reflected and you&#39;ll think it isn&#39;t working. Clearing the site&#39;s caches will not resolve this. You need to update the library number so your browser knows it is different JavaScript. That&#39;s not a problem with CSS, where clearing the site&#39;s caches will reflect the update immediately whether or not the version number was increased. The JavaScript This is the core functional part, the JavaScript: (function () { // Get details elements, excluding some special cases. var detailsElements = document.querySelectorAll( &#92;&quot;details:not(.building-hours):not(.captcha):not(.bef--secondary&#92;&quot;, ); // Get blocks, which could be more than once. var expandDetailsBlocks = document.querySelectorAll( &#92;&quot;.block-expand-details-block&#92;&quot;, ); if (detailsElements.length &gt; 0 &amp;amp;&amp;amp; expandDetailsBlocks.length &gt; 0) { expandDetailsBlocks.forEach((expandDetailsBlock) =&gt; { // Display change is set at the block level. expandDetailsBlock.style.display = &#92;&quot;flex&#92;&quot;; // Assuming that there will only ever be one of each per block. var expandDetailsButton = expandDetailsBlock.querySelector( &#92;&quot;.expand-details-button-expand&#92;&quot;, ); var collapseDetailsButton = expandDetailsBlock.querySelector( &#92;&quot;.expand-details-button-collapse&#92;&quot;, ); // Add click actions for each block. expandDetailsButton.addEventListener(&#92;&quot;click&#92;&quot;, () =&gt; { detailsElements.forEach((details) =&gt; { details.open = true; }); }); collapseDetailsButton.addEventListener(&#92;&quot;click&#92;&quot;, () =&gt; { detailsElements.forEach((details) =&gt; { details.open = false; }); }); }); } })(); There are three exclusions added in this example: building hours which is a little details dropdown to show open hours in the header, any appearance of the captcha from the captcha module, and the Better Exposed Filters advanced filter. You may have more or less things that you want to exclude. A potential bit of improvement for this module would be to get those options into configuration so they can be more easily adjusted, but I have not done that here. The Styles Finally, the style sheet: /** Hidden by default, it will only appear when there are details on the page **/ .block-expand-details-block { display: none; flex-direction: row; justify-content: flex-end; margin-bottom: 0.5em; } .expand-details-button { border: 2px solid black; border-radius: 5px; background-color: gold; color: black; } .expand-details-button:is(:hover, :focus) { background-color: purple; color: white; } The first part is the most important here: by default, it does not display. It only appears once the JavaScript detects that there are details elements (other than the exclusions). It would be confusing if it showed up even when there are no details elements on the page. The other rules are just giving it some style.&quot;
    }]

				</div>
				<div class="sidebar">
					
					

	<aside class="sidebar-block" aria-label=About>
		<h2>About</h2>
		<div class="sidebar-container">
  <p>I'm Ryan Robinson. I've been working in technology for my adult life, especially website development. I am now primarily focused on website development, particularly in Drupal. On this site I document some of the work I've done, whether within a full-time job, freelance work, volunteering, or as a personal side project.</p>
</div>

	</aside>


					


					

	<aside class="sidebar-block" aria-labelledby="posts-by-tag-label">
		<h2 id="posts-by-tag-label">Posts by Tag</h2>
		<div class="sidebar-container">
			
<ul class="tags-list">
	
		
		<li><a href="/tags/ai/" class="post-tag">AI</a> (4)</li>
	
		
		<li><a href="/tags/accessibility/" class="post-tag">Accessibility</a> (17)</li>
	
		
		<li><a href="/tags/crm/" class="post-tag">CRM</a> (4)</li>
	
		
		<li><a href="/tags/css/" class="post-tag">CSS</a> (5)</li>
	
		
		<li><a href="/tags/devops/" class="post-tag">DevOps</a> (26)</li>
	
		
		<li><a href="/tags/drupal/" class="post-tag">Drupal</a> (38)</li>
	
		
		<li><a href="/tags/javascript/" class="post-tag">JavaScript</a> (1)</li>
	
		
		<li><a href="/tags/linux/" class="post-tag">Linux</a> (5)</li>
	
		
		<li><a href="/tags/microsoft-365/" class="post-tag">Microsoft 365</a> (49)</li>
	
		
		<li><a href="/tags/php/" class="post-tag">PHP</a> (18)</li>
	
		
		<li><a href="/tags/security/" class="post-tag">Security</a> (16)</li>
	
		
		<li><a href="/tags/static-sites/" class="post-tag">Static Sites</a> (4)</li>
	
		
		<li><a href="/tags/tech-and-society/" class="post-tag">Tech and Society</a> (10)</li>
	
		
		<li><a href="/tags/visual-studio-code/" class="post-tag">Visual Studio Code</a> (18)</li>
	
		
		<li><a href="/tags/wordpress/" class="post-tag">WordPress</a> (7)</li>
	
</ul>

		</div>
	</aside>


					
<aside class="sidebar-block" aria-labelledby="posts-by-year-label">
	<h2 id="posts-by-year-label">Posts by Year</h2>
	<div class="sidebar-container">
		
<ul class="tags-list">
	
		
		<li><a href="/2025/" class="post-tag">2025</a> (10)</li>
	
		
		<li><a href="/2024/" class="post-tag">2024</a> (11)</li>
	
		
		<li><a href="/2023/" class="post-tag">2023</a> (17)</li>
	
		
		<li><a href="/2022/" class="post-tag">2022</a> (35)</li>
	
		
		<li><a href="/2021/" class="post-tag">2021</a> (77)</li>
	
</ul>

	</div>
</aside>

				</div>
			</main>
			<footer class="container" aria-label="Site Footer">
					
<nav aria-labelledby="accounts-elsewhere" id="footer">
	<h2 class="visually-hidden" id="accounts-elsewhere">Ryan's Accounts Elsewhere</h2>
	<ul class="social-links">
		<li>
			<a rel="me" href="https://www.linkedin.com/in/ryanlewisrobinson">
				<span class="visually-hidden">LinkedIn</span>
				<svg xmlns="http://www.w3.org/2000/svg" height="60" width="60" viewBox="0 0 640 640"><!--!Font Awesome Free v7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2025 Fonticons, Inc.--><path d="M160 96C124.7 96 96 124.7 96 160L96 480C96 515.3 124.7 544 160 544L480 544C515.3 544 544 515.3 544 480L544 160C544 124.7 515.3 96 480 96L160 96zM165 266.2L231.5 266.2L231.5 480L165 480L165 266.2zM236.7 198.5C236.7 219.8 219.5 237 198.2 237C176.9 237 159.7 219.8 159.7 198.5C159.7 177.2 176.9 160 198.2 160C219.5 160 236.7 177.2 236.7 198.5zM413.9 480L413.9 376C413.9 351.2 413.4 319.3 379.4 319.3C344.8 319.3 339.5 346.3 339.5 374.2L339.5 480L273.1 480L273.1 266.2L336.8 266.2L336.8 295.4L337.7 295.4C346.6 278.6 368.3 260.9 400.6 260.9C467.8 260.9 480.3 305.2 480.3 362.8L480.3 480L413.9 480z"/></svg>
			</a>
		</li>
		<li>
			<a rel="me" href="https://github.com/ryan-l-robinson">
				<span class="visually-hidden">GitHub</span>
				<svg xmlns="http://www.w3.org/2000/svg" height="60" width="60" viewBox="0 0 640 640"><!--!Font Awesome Free v7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2025 Fonticons, Inc.--><path d="M544 160C544 124.7 515.3 96 480 96L160 96C124.7 96 96 124.7 96 160L96 480C96 515.3 124.7 544 160 544L480 544C515.3 544 544 515.3 544 480L544 160zM361.8 471.7C361.8 469.9 361.8 465.7 361.9 460.1C362 448.7 362 431.3 362 416.4C362 400.8 356.8 390.9 350.7 385.7C387.7 381.6 426.7 376.5 426.7 312.6C426.7 294.4 420.2 285.3 409.6 273.6C411.3 269.3 417 251.6 407.9 228.6C394 224.3 362.2 246.5 362.2 246.5C335.6 239 305.6 239 279 246.5C279 246.5 247.2 224.3 233.3 228.6C224.2 251.5 229.8 269.2 231.6 273.6C221 285.3 216 294.4 216 312.6C216 376.2 253.3 381.6 290.3 385.7C285.5 390 281.2 397.4 279.7 408C270.2 412.3 245.9 419.7 231.4 394.1C222.3 378.3 205.9 377 205.9 377C189.7 376.8 204.8 387.2 204.8 387.2C215.6 392.2 223.2 411.4 223.2 411.4C232.9 441.1 279.3 431.1 279.3 431.1C279.3 440.1 279.4 452.8 279.4 461.7C279.4 466.5 279.5 470.3 279.5 471.7C279.5 476 276.5 481.2 268 479.7C202 457.6 155.8 394.8 155.8 321.4C155.8 229.6 226 159.9 317.8 159.9C409.6 159.9 484 229.6 484 321.4C484.1 394.8 439.3 457.7 373.3 479.7C364.9 481.2 361.8 476 361.8 471.7zM271.3 416.9C271.1 415.4 272.4 414.1 274.3 413.7C276.2 413.5 278 414.3 278.2 415.6C278.5 416.9 277.2 418.2 275.2 418.6C273.3 419 271.5 418.2 271.3 416.9zM262.2 420.1C260 420.3 258.5 419.2 258.5 417.7C258.5 416.4 260 415.3 262 415.3C263.9 415.1 265.7 416.2 265.7 417.7C265.7 419 264.2 420.1 262.2 420.1zM247.9 417.9C246 417.5 244.7 416 245.1 414.7C245.5 413.4 247.5 412.8 249.2 413.2C251.2 413.8 252.5 415.3 252 416.6C251.6 417.9 249.6 418.5 247.9 417.9zM235.4 410.6C233.9 409.3 233.5 407.4 234.5 406.5C235.4 405.4 237.3 405.6 238.8 407.1C240.1 408.4 240.6 410.4 239.7 411.2C238.8 412.3 236.9 412.1 235.4 410.6zM226.9 400.6C225.8 399.1 225.8 397.4 226.9 396.7C228 395.8 229.7 396.5 230.6 398C231.7 399.5 231.7 401.3 230.6 402.1C229.7 402.7 228 402.1 226.9 400.6zM220.6 391.8C219.5 390.5 219.3 389 220.2 388.3C221.1 387.4 222.6 387.9 223.7 388.9C224.8 390.2 225 391.7 224.1 392.4C223.2 393.3 221.7 392.8 220.6 391.8zM214.6 385.4C213.3 384.8 212.7 383.7 213.1 382.8C213.5 382.2 214.6 381.9 215.9 382.4C217.2 383.1 217.8 384.2 217.4 385C217 385.9 215.7 386.1 214.6 385.4z"/></svg>
			</a>
		</li>
		<li>
			<a rel="me" href="https://gitlab.com/ryan-l-robinson">
				<span class="visually-hidden">GitLab</span>
				<svg xmlns="http://www.w3.org/2000/svg" height="60" width="60" viewBox="0 0 640 640"><!--!Font Awesome Free v7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2025 Fonticons, Inc.--><path d="M96 160L96 480C96 515.3 124.7 544 160 544L480 544C515.3 544 544 515.3 544 480L544 160C544 124.7 515.3 96 480 96L160 96C124.7 96 96 124.7 96 160zM433.5 172.5L478.1 288.9L478.5 290.1C484.1 306.9 485.7 325.3 480.8 342.6C475.8 359.8 465.4 375 451 385.9L450.8 386L382.4 437.2L328.3 478.1C327.8 478.3 327.2 478.6 326.6 478.9C324.6 479.9 322.2 480.9 319.9 480.9C316.9 480.9 313.1 479.1 311.6 478.1L257.4 437.2L189.5 386.3L189.1 386L188.9 385.9C174.6 375.1 164.1 359.9 159.2 342.6C154.3 325.3 155 306.9 161.4 290.1L161.9 288.9L206.6 172.5C207.5 170.2 209.1 168.2 211.1 166.9C212.7 165.9 214.5 165.3 216.3 165.1C217.6 164.4 218.4 164.7 219.7 165.2C220.3 165.4 220.9 165.7 221.7 165.9C222.7 166.3 223.3 166.8 224.1 167.4C224.7 167.8 225.3 168.4 226.2 168.9C227.4 170.3 228.4 171.9 228.9 173.7L258.1 265.9L381 265.9L411.2 173.7C411.7 171.9 412.6 170.3 413.8 168.9C415 167.5 416.6 166.5 418.3 165.8C420 165.2 421.9 164.9 423.7 165.1C425.5 165.3 427.3 165.9 428.9 166.9C430.9 168.2 432.6 170.2 433.5 172.5z"/></svg>
			</a>
		</li>
		<li>
			<a rel="me" href="https://dev.to/ryanr">
				<span class="visually-hidden">Dev.To</span>
				<svg xmlns="http://www.w3.org/2000/svg" height="60" width="60" viewBox="0 0 640 640"><!--!Font Awesome Free v7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2025 Fonticons, Inc.--><path d="M216.1 272.3C212.2 269.4 208.3 268 204.5 268L187.1 268L187.1 372.5L204.5 372.5C208.4 372.5 212.3 371.1 216.1 368.2C219.9 365.3 221.9 360.9 221.9 355.1L221.9 285.4C221.9 279.6 219.9 275.2 216.1 272.3zM500.1 96L139.9 96C115.7 96 96.1 115.6 96 139.8L96 500.2C96.1 524.4 115.7 544 139.9 544L500.1 544C524.3 544 543.9 524.4 544 500.2L544 139.8C543.9 115.6 524.3 96 500.1 96zM250.2 355.2C250.2 374 238.6 402.5 201.8 402.5L155.4 402.5L155.4 237L202.8 237C238.2 237 250.2 265.5 250.2 284.3L250.2 355.2zM350.9 266.5L297.6 266.5L297.6 304.9L330.2 304.9L330.2 334.5L297.6 334.5L297.6 372.9L350.9 372.9L350.9 402.5L288.7 402.5C277.5 402.8 268.3 394 268 382.8L268 257.7C267.7 246.6 276.6 237.3 287.7 237L350.9 237L350.9 266.5zM454.5 381.8C441.3 412.5 417.7 406.4 407.1 381.8L368.6 237L401.2 237L430.9 350.7L460.5 237L493.1 237L454.6 381.8z"/></svg>
			</a>
		</li>
		<li>
			<a rel="me" href="https://mstdn.ca/@Ryan">
				<span class="visually-hidden">Mastodon</span>
				<svg xmlns="http://www.w3.org/2000/svg" height="60" width="60" viewBox="0 0 640 640"><!--!Font Awesome Free v7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2025 Fonticons, Inc.--><path d="M529 243.1C529 145.9 465.3 117.4 465.3 117.4C402.8 88.7 236.7 89 174.8 117.4C174.8 117.4 111.1 145.9 111.1 243.1C111.1 358.8 104.5 502.5 216.7 532.2C257.2 542.9 292 545.2 320 543.6C370.8 540.8 399.3 525.5 399.3 525.5L397.6 488.6C397.6 488.6 361.3 500 320.5 498.7C280.1 497.3 237.5 494.3 230.9 444.7C230.3 440.1 230 435.4 230 430.8C315.6 451.7 388.7 439.9 408.7 437.5C464.8 430.8 513.7 396.2 519.9 364.6C529.7 314.8 528.9 243.1 528.9 243.1zM453.9 368.3L407.3 368.3L407.3 254.1C407.3 204.4 343.3 202.5 343.3 261L343.3 323.5L297 323.5L297 261C297 202.5 233 204.4 233 254.1L233 368.3L186.3 368.3C186.3 246.2 181.1 220.4 204.7 193.3C230.6 164.4 284.5 162.5 308.5 199.4L320.1 218.9L331.7 199.4C355.8 162.3 409.8 164.6 435.5 193.3C459.2 220.6 453.9 246.3 453.9 368.3L453.9 368.3z"/></svg>
			</a>
		</li>
		<li>
			<a rel="me" href="https://bsky.app/profile/ryanrobinson.ca">
				<span class="visually-hidden">Bluesky</span>
				<svg xmlns="http://www.w3.org/2000/svg" height="60" width="60" viewBox="0 0 640 640"><!--!Font Awesome Free v7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2025 Fonticons, Inc.--><path d="M160 96C124.7 96 96 124.7 96 160L96 480C96 515.3 124.7 544 160 544L480 544C515.3 544 544 515.3 544 480L544 160C544 124.7 515.3 96 480 96L160 96zM320 311.4C334.5 281.4 374 225.6 410.7 198.1C437.2 178.2 480 162.9 480 211.8C480 221.6 474.4 293.9 471.1 305.6C459.7 346.4 418.1 356.8 381.1 350.5C445.8 361.5 462.3 398 426.7 434.5C359.2 503.8 329.7 417.1 322.1 394.9L321.8 394C320.9 391.4 320.4 389.9 320 389.9C319.6 389.9 319.1 391.4 318.2 394C318.1 394.3 318 394.6 317.9 394.9C310.3 417.1 280.8 503.7 213.3 434.5C177.8 398 194.2 361.5 258.9 350.5C221.9 356.8 180.3 346.4 168.9 305.6C165.6 293.9 160 221.6 160 211.8C160 162.9 202.9 178.3 229.3 198.1C266 225.6 305.5 281.5 320 311.4z"/></svg>
			</a>
		</li>
		<li>
			<a rel="me" href="https://pixelfed.ca/@ryan">
				<span class="visually-hidden">Pixelfed</span>
				<svg xmlns="http://www.w3.org/2000/svg" height="60" width="60" viewBox="0 0 640 640"><!--!Font Awesome Free v7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2025 Fonticons, Inc.--><path d="M320 64C461.4 64 576 178.6 576 320C576 461.4 461.4 576 320 576C178.6 576 64 461.4 64 320C64 178.6 178.6 64 320 64zM299.7 375.9L346.7 375.9C390.9 375.9 426.8 341 426.8 297.9C426.8 254.8 390.9 219.9 346.7 219.9L278.9 219.9C253.4 219.9 232.7 240 232.7 264.9L232.7 440L299.7 375.9z"/></svg>
			</a>
		</li>
		<li>
			<a rel="me" href="https://letterboxd.com/ryanlr/">
				<span class="visually-hidden">Letterboxd</span>
				<svg xmlns="http://www.w3.org/2000/svg" height="60" width="60" viewBox="0 0 640 640"><!--!Font Awesome Free v7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2025 Fonticons, Inc.--><path d="M480 96C515.3 96 544 124.7 544 160L544 480C544 515.3 515.3 544 480 544L160 544C124.7 544 96 515.3 96 480L96 160C96 124.7 124.7 96 160 96L480 96zM201.1 251C162.4 251 131 282.3 131 321C131 359.7 162.4 391 201.1 391C225.9 391 247.6 378.2 260.1 358.8L260.6 358.1L260.2 357.5C253.7 346.9 250 334.4 250 321C250 307.4 253.9 294.7 260.6 283.9C248.2 264.1 226.2 251 201.1 251zM320 251C295.2 251 273.5 263.8 261 283.2L260.5 283.9L260.9 284.5C267.4 295.1 271.1 307.6 271.1 321C271.1 334.6 267.2 347.3 260.5 358.1C272.9 377.8 294.9 391 320 391C344.8 391 366.5 378.2 379 358.8L379.5 358.1L379.1 357.5C372.6 346.9 368.9 334.4 368.9 321C368.9 307.4 372.8 294.7 379.5 283.9C367.1 264.2 345.1 251 320 251zM438.9 251C414.1 251 392.4 263.8 379.9 283.2L379.4 283.9L379.8 284.5C386.3 295.1 390 307.6 390 321C390 334.6 386.1 347.3 379.4 358.1C391.8 377.9 413.8 391 438.9 391C477.6 391 509 359.7 509 321C509 282.3 477.6 251 438.9 251z"/></svg>
			</a>
		</li>
		<li>
			<a rel="me" href="https://xbox.com/en-CA/play/user/ryanlr">
				<span class="visually-hidden">Xbox</span>
				<svg xmlns="http://www.w3.org/2000/svg" height="60" width="60" viewBox="0 0 640 640"><!--!Font Awesome Free v7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2025 Fonticons, Inc.--><path d="M433.9 382.2C478.2 436.5 498.6 481 488.3 500.9C480.4 516 431.6 545.5 395.7 556.8C366.1 566.1 327.3 570.1 295.3 567C257.1 563.3 218.4 549.6 185.2 528C157.3 509.8 151 502.3 151 487.4C151 457.5 183.9 405.1 240.2 345.3C272.2 311.4 316.7 271.6 321.6 272.7C331 274.8 405.9 347.8 433.9 382.2zM252.6 207.8C222.9 180.9 194.5 153.9 166.2 144.4C151 139.3 149.9 139.6 137.5 152.5C108.3 182.9 84 232.2 77.2 274.9C71.8 309.1 71.1 318.7 73 335.4C78.6 385.9 90.3 420.8 113.5 456.3C123 470.9 125.6 473.6 122.8 466.2C118.6 455.2 122.5 428.7 132.3 402.2C146.6 363.2 186.2 289.3 252.6 207.8zM564.2 271.3C547.3 191.3 496.7 141 489.6 141C482.3 141 465.4 147.5 453.6 154.9C430.3 169.4 412.6 186.3 389.3 207.7C431.7 261 491.5 347.1 512.2 410C519 430.7 521.9 451.1 519.6 462.3C517.9 470.8 517.9 470.8 521 466.9C527.1 459.2 540.9 435.6 546.4 423.4C553.8 407.2 561.4 383.2 565 364.7C569.3 342.2 568.9 293.9 564.2 271.3zM205.3 107C253 104.5 315 141.5 319.6 142.4C320.3 142.5 330 138.2 341.2 132.7C405.1 101.6 435.2 106.9 448.6 107.5C384.7 68.2 295.9 57.5 214.7 95.8C191.3 106.9 190.7 107.7 205.3 107z"/></svg>
			</a>
		</li>
		<li>
			<a rel="me" href="/feed/feed.xml" aria-label="rss">
				<span class="visually-hidden">RSS (technically Atom) Feed</span>
				<svg xmlns="http://www.w3.org/2000/svg" height="60" width="60" viewBox="0 0 640 640"><!--!Font Awesome Free v7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2025 Fonticons, Inc.--><path d="M96 128C96 110.3 110.3 96 128 96C357.8 96 544 282.2 544 512C544 529.7 529.7 544 512 544C494.3 544 480 529.7 480 512C480 317.6 322.4 160 128 160C110.3 160 96 145.7 96 128zM96 480C96 444.7 124.7 416 160 416C195.3 416 224 444.7 224 480C224 515.3 195.3 544 160 544C124.7 544 96 515.3 96 480zM128 224C287.1 224 416 352.9 416 512C416 529.7 401.7 544 384 544C366.3 544 352 529.7 352 512C352 388.3 251.7 288 128 288C110.3 288 96 273.7 96 256C96 238.3 110.3 224 128 224z"/></svg>
			</a>
		</li>
	</ul>
</nav>

					<p>&copy; 2025 <a href="https://ryanrobinson.ca">Ryan Robinson</a>. Powered by <a href="https://www.11ty.dev/">Eleventy</a>. The site's font is <a href="https://brailleinstitute.org/freefont">Atkinson Hyperlegible from the Braille Institute</a>.</p>
			</footer>
		</div>
		<script src="/js/lunr.min.js"></script>
		<script src="/js/search.js"></script>
	</body>
</html>
